<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear algebra | Te-Sheng Lin</title>
    <link>https://teshenglin.github.io/categories/linear-algebra/</link>
      <atom:link href="https://teshenglin.github.io/categories/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <description>Linear algebra</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 07 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Linear algebra</title>
      <link>https://teshenglin.github.io/categories/linear-algebra/</link>
    </image>
    
    <item>
      <title>主成分分析 - 2</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們補充一下主成分分析裡的證明部分.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p.
$$
假設想要投影到 $k$ 維, $k\le p$, 數學上來說就是想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;要求極值就是要找微分等於零的解, 由於這式子是 convex, 所以保證找到唯一解而且是最小的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;所以以下做法就是對每個變數做偏微分, 並求出偏微分等於零的解. 這樣就把最佳解找出來了!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;1-首先看-mu&#34;&gt;1. 首先看 $\mu$&lt;/h4&gt;
&lt;p&gt;對 $\mu$ 做偏微分並且利用 $\sum^n_{i=1} \beta_i=\vec{0}$ 我們可以得到
$$
\partial_{\mu} E = -2 \left(\sum_i x_i - n\mu\right)=0.
$$
因此, 最佳的 $\mu$ 是
$$
\mu = \frac{1}{n}\sum_{i=1}^n x_i.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-接著找-beta_i&#34;&gt;2. 接著找 $\beta_i$&lt;/h4&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 我們有
$$
E = \sum_{i=1}^n \|y_i - U\beta_i\|^2.
$$
接著對 $\beta_i$ 微分得到
$$
\partial_{\beta_i} E = 2\left(\beta_i - U^Ty_i\right)=0.
$$
因此可以得到
$$
\beta_i = U^T y_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;,
$$
其中 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-最後找-u&#34;&gt;3. 最後找 $U$&lt;/h4&gt;
&lt;p&gt;代入最佳的 $\beta_i$ 後我們又可將原本的 $E$ 改寫為
$$
E = \sum_{i=1}^n \|y_i - UU^Ty_i\|^2 = \|Y - UU^TY\|^2_F,
$$
其中 $Y=[y_1, \cdots, y_n]$, 而下標 $F$ 代表矩陣的 Frobenius norm.&lt;/p&gt;
&lt;p&gt;我們先定 $P = UU^T$, 則有 $P^T=P$, $P^2=P$. 接著我們改寫 $E$ 為
$$
E = \|Y - PY\|^2_F = trace\left[(Y-PY)^T(Y-PY)\right] = trace\left(Y^TY-Y^TPY\right).
$$
由於 $Y$ 不會變, 因此求 $E$ 的最小值變成求 $trace\left(-Y^TPY\right)$ 的最小值, 也就是求 $trace\left(Y^TPY\right)$ 的最大值, 換回來得到是求 $trace\left(Y^TUU^TY\right)$ 的最大值.&lt;/p&gt;
&lt;p&gt;接著我們用線性代數裡一個定理, $trace(AB)=trace(BA)$, 將原式轉換成求 $trace\left(U^TYY^TU\right)$ 的最大值, 最後得到
$$
\arg\min_U E = \arg\max_U trace\left(U^T\Sigma U\right),
$$
其中 $\Sigma=YY^T$ 也就是原資料的共變異數矩陣. 上式是個非常重要的式子, 它告訴我們以下兩件事是等價的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使得原始資料與投影後的資料之間的&lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;使得資料有&lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最後, 線性代數告訴我們等號右邊這問題的解就是 $\Sigma$ 最大的 $k$ 個 eigenvalues 其相對應的 eigenvectors, 收集起來得到 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以上使用的線性代數結果, 其證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/252272/is-trace-invariant-under-cyclic-permutation-with-rectangular-matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tr(AB)=tr(BA) proof&lt;/a&gt;, 以及 
&lt;a href=&#34;https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;maximize v^T Av&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;總結一下最佳解如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 是原始資料的平均
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將平移後的資料投影到子空間中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Multidimensional scaling</title>
      <link>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Multidimensional scaling, 簡稱 MDS, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 MDS 的原理及做法, 更精確的說, 這裡講的是 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p
$$
那我們可以據此構造出一個距離平方矩陣 $D$, Euclidian distance matrix, 簡稱 EDM, 其中 $D_{ij} = \|x_i-x_j\|^2$, 也就是 $(x_i)$ 及 $x_j$ 這兩個點距離的平方.&lt;/p&gt;
&lt;hr&gt;
&lt;h5 id=&#34;舉例來說&#34;&gt;舉例來說&lt;/h5&gt;
&lt;p&gt;以下我們用 Matlab 隨機生出 $R^2$ 空間中的 $5$ 個點, 並求出他的 EDM:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
X = rand(2,5);                                  % R^2 中的 5 個點
D = squareform(pdist(X&#39;, &#39;squaredeuclidean&#39;));  % 求出其 EDM
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;D =
     0    0.3839    0.2333    0.2675    0.6373
0.3839         0    0.1571    0.1272    0.0470
0.2333    0.1571         0    0.0028    0.3725
0.2675    0.1272    0.0028         0    0.3222
0.6373    0.0470    0.3725    0.3222         0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可看出 EDM 的幾個性質&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一定是個對稱矩陣&lt;/li&gt;
&lt;li&gt;所有元素都非負&lt;/li&gt;
&lt;li&gt;對角線元素必為零&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;而 MDS 要做的事是以上的反問題:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設我們拿到一個 EDM 矩陣, 我想要把它原始生成的點 $x_i$ 找出來.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;這樣子的問題稱為 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;classical MDS 處理這些距離用 Euclidean distance 來量出來的矩陣.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;要講 MDS 做法之前我們先定義一個矩陣稱為&lt;strong&gt;置中矩陣&lt;/strong&gt; $H$, centering matrix, 也就是把一組資料平移使得其中心為坐標原點:
$$
H = I_n - \frac{1}{n}{\bf 1}{\bf 1}^T,
$$
其中 $I_n$ 是 $n\times n$ 的單位矩陣, ${\bf 1}$ 則是元素全為 $1$ 的 $n\times 1$ 向量.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;可以輕易看出來 $H$ 是一個對稱矩陣, $H^T=H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;若我們將原始資料收集一起成一個 $p\times n$ 矩陣 $X = [x_1, \cdots, x_n]$, 則有
$$
X H = X (I_n - \frac{1}{n}{\bf 1}{\bf 1}^T) = X - \frac{1}{n}\left(\sum_i x_i\right) {\bf 1}^T = X - \mu {\bf 1}^T = Y,
$$
其中 $\mu=\frac{1}{n}\sum_i x_i$ 就是原始資料的平均, 而 $Y=[y_1, \cdots, y_n]$, $y_i = x_i-\mu$. 所以的確 $XH$ 就是把資料平移使得其中心為坐標原點.&lt;/p&gt;
&lt;p&gt;推導一下後也可以發現 $H\hat{X}$ 則是將 $\hat{X}$ 的 rows 做資料平移到原點.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們有以下這個定理&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;給定原始資料 $X = [x_1, \cdots, x_n]$, 且其相對應的 EDM 為 $D$, 則我們有
$$Y^T Y = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個定理證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;h3 id=&#34;sketch-not-complete-please-full-in-the-details-by-yourself&#34;&gt;(Sketch, not complete, please full-in the details by yourself)&lt;/h3&gt;
&lt;p&gt;我們先重新整理一下這個 EDM 矩陣:
$$
\begin{aligned}
D_{ij} &amp;amp;= \|x_i-x_j\|^2 = &amp;lt;x_i-x_j, x_i-x_j&amp;gt; \\ &amp;amp;= &amp;lt;x_i, x_i&amp;gt; + &amp;lt;x_j, x_j&amp;gt; - 2&amp;lt;x_i, x_j&amp;gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;接著我們定義一個 $n\times 1$ 向量 ${\bf k}$, 其中 ${\bf k}_i = &amp;lt;x_i, x_i&amp;gt;$, 則可以將 $D$ 改寫為
$$D = {\bf k}{\bf 1}^T + {\bf 1}{\bf k}^T - 2 X^TX.$$
接著兩邊乘上 $H$, 並利用 ${\bf 1}^T H = 0$ 以及 $H {\bf 1} = 0$ (分別將一個常數列向量以及常數行向量置中都會得到零向量), 我們可得
$$HDH = -2HX^TXH = -2(XH)^T(XH) = -2Y^TY.$$
故得證.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;這個定理告訴我們的是, 如果我們只知道一組資料的 EDM 矩陣 $D$, 那要怎樣把資料給還原回來.&lt;/p&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark&lt;/h4&gt;
&lt;p&gt;當然, 不可能把原始資料完全還原回來! 如同定理中所述我們所能算出的等號左邊是 $Y$, 也就是置中後的原始資料. 事實上我們也很容易想像, 若將一組資料平移或是旋轉後其 EDM 應該是完全不會變的. 也就是說我們若只知道 EDM, 則其原始資料應該有無限多解. 這裡所謂的還原回來是找到其中一組解, 其他所有可能的解則都可以將之&lt;strong&gt;平移&lt;/strong&gt;或&lt;strong&gt;旋轉&lt;/strong&gt;後得到.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;定理等號右邊由於是個對稱矩陣, 所以可以對角化為
$$
-\frac{1}{2}HDH = V\Lambda V^T = V\sqrt{\Lambda}\sqrt{\Lambda} V^T,
$$
其中 $\Lambda$ 是個對角矩陣包含所有特徵值, 而 $\sqrt{\Lambda}$ 則是將 $\Lambda$ 對角線元素都開根號.&lt;/p&gt;
&lt;p&gt;跟定理對照一下可以輕易地看出來, 平移後的原始資料點可以被還原出來: $Y = \sqrt{\Lambda}V^T$.&lt;/p&gt;
&lt;p&gt;這就是 classical MDS 的作法!! 非常簡單.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若原始資料 $x_i\in R^p$, 則 $Y^TY$ 的 rank 為 $p$, 必有 $n-p$ 個為零的特徵值. 所以, 若我們將 $-\frac{1}{2}HDH$ 對角化後發現有 $m$ 個為零的特徵值, 表示原資料的 $p=n-m$, 那我們就把這些零特徵值都拿掉, 使 $\sqrt{\Lambda}$ 為一個 $p\times n$ 的矩陣, 這樣我們就有 $Y = \sqrt{\Lambda}V^T \in R^{p\times n}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若將 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$. 則 $Y^TY = \hat{V}\hat{\Sigma}^T\hat{\Sigma}\hat{V}^T$. 對照一下定理可以看出, 如果 $D$ 是個 EDM 矩陣, 那他的 eigenvalues 一定都是非負.&lt;/p&gt;
&lt;p&gt;依照上面兩個 remark 稍微整理一下 MDS 的做法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;將 EDM 矩陣 $D$ 做 double centering 並乘以 $-1/2$ 求出 $-\frac{1}{2}HDH$.&lt;/li&gt;
&lt;li&gt;做對角化得到 $\Lambda$ 以及 $V$&lt;/li&gt;
&lt;li&gt;拿掉所有零特徵值及其相對應的特徵向量, 求出 $Y = \sqrt{\Lambda}V^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般來說我們會將 MDS 當成一個降維的工具, 希望在低維度(二或三, 或 $k$)找到一組資料使得其 EDM 與原始資料的 EDM 最像. 而作法就是保留前 $k$ 個特徵值及其特徵向量.&lt;/p&gt;
&lt;p&gt;給定一個 $n\times n$ 的 EDM 以及欲投影的維度 $k$, MDS 簡單的程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function Y = multidimensional_scaling(D, k)

%   Input: D, n*n EDM 矩陣, 元素為距離平方
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: Y: k*n data matrix, k 個 features 以及 n 個 samples

    n = size(D, 1);                         % n 個 samples
    mu = sum(D, 1)/n; D = D - mu;           %
    mu = sum(D, 2)/n; B = D - mu;           %
    B = -0.5*B;                             % B = -0.5*H*D*H
    [V, D] = eig(B, &#39;vector&#39;);              % 求出特徵值及特徵向量
    [sqD, ind] = sort(sqrt(D), &#39;descend&#39;);  % 將特徵值按大小排列
    sqD = sqD(1:k);                         % 取前 k 個
    V = V(:, ind(1:k));                     % 取相對應的特徵向量
    Y = V&#39;.*(sqD*ones(1,n));                % 求出 k 維資料點
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;廣義一點的 MDS 可以想像是, 我們拿到的距離也許不是用歐式距離量的. 比如說手裡有許多照片, 照片與照片兩兩之間的距離就有非常多種測量的方式. 而不管是用什麼方式量的, 只要他們是 &lt;strong&gt;距離(metric)&lt;/strong&gt; , 我們就可以定出一個 $D$ 矩陣, 其中 $d_{ij}$ 就是 sample points $x_i$ 與 $x_j$ 之間的距離.&lt;/p&gt;
&lt;p&gt;接著我們可以定義一個 cost function, 稱之為 stress:
$$
Stress_D(x_1, \cdots, x_n) = \left(\sum_i\sum_j\left(d_{ij} - \|x_i-x_j\|\right)^2\right)^{1/2}.
$$
Metric Multidimensional scaling (mMDS) 要做的事就是要找到一組資料點 $\{x_1, \cdots, x_n\}$ 使得上式 stress 有最小值.&lt;/p&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若 $D$ 是用廣義距離造出來的, 就無法保證 $-\frac{1}{2}HDH$ 這矩陣的特徵值都是正的. 不過我們依然可以用 classical MDS 的做法來做, 只是這時候我們會將所有負的特徵值全都丟掉.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-1&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;原始點資料若是在某個 weighted inner product space 裡, 內積定義為
$$
&amp;lt;x, y&amp;gt;_Q = x^TQy.
$$
則我們可以將原本的定理推廣如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem-1&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;$$Y^T QY = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-classical-mds-vs-pca&#34;&gt;Extension: classical MDS v.s. PCA&lt;/h2&gt;
&lt;p&gt;給定 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p,
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們可以造出其 EDM, 再用 classical MDS 投影到 $k$ 維.&lt;/li&gt;
&lt;li&gt;我們也可以直接用 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis&#34;&gt;PCA&lt;/a&gt; 投影到 $k$ 維.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有趣的是以上兩個做法得到完全相同的結果.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不管 PCA 或 classical MDS 最後都是考慮置中後的資料, 所以我們只需看 $Y$ 即可, $y_i = x_i-\mu$, 而 $\mu$ 是平均數.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;PCA 是對 $\Sigma = YY^T$ 做 spectral decomposition&lt;/li&gt;
&lt;li&gt;classical MDS 是對 $Y^TY$ 做 spectral decomposition&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;若將 $Y$ 做 SVD 得到 $Y = \hat{U}\hat{\Sigma}\hat{V}^T$, 則&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PCA 我們知道最後投影的結果為 $B = \hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;li&gt;classical MDS 我們也是投影到 $\hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以雖然出發點不同, 不過結果真的一模一樣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PCA = classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/Yt0o8ukIOKU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GeostatsGuy Lectures - Multidimensional Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;主成分分析, Principal component analysis, 簡稱 PCA, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;資料降維簡單來說, 假設我們有一些資料, 這資料中的每一筆維度都很高, 導致我們很難 &amp;ldquo;看出&amp;rdquo; 或 &amp;ldquo;分析&amp;rdquo; 這資料集的特性, 這時候我們就會想要在低維度空間裡(通常三維以下)建構一組點資料, 並且想辦法使新的這組點資料在某些程度上能表示出原本高維度的點資料, 或保有某種特性. 這種將高維度資料在低維度表現出來的方式就稱為資料降維. 而 PCA 就是其中一種線性降維的方式.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 PCA 的原理及做法.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p
$$
那我們想要做以下等價的兩件事:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到一組低維度的投影並且使得資料有 &lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;找到一組低維度的投影並且使得原始資料與投影後的資料之間的 &lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這裡所謂低維度的投影講精確一點, 事實上是在原本的 $R^p$ 空間中找到一個低維度的仿射子空間(affine subspace), 可以想像成一個不穿過原點的點或直線或平面, 找到之後再把原始資料點投影上去.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-0-維&#34;&gt;Example: 投影到 $0$ 維&lt;/h3&gt;
&lt;p&gt;先舉一個最簡單的例子, 假設我們想要投影到 $0$ 維, 也就是投影到一個點.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也就是說我們想要找一個點來代表整組資料.&lt;/p&gt;
&lt;p&gt;直覺上來想, 如果我要幫一組資料找一個最具代表性的點, 應該就會用 &amp;ldquo;平均數&amp;rdquo; 或 &amp;ldquo;中位數&amp;rdquo; 來當這個點.&lt;/p&gt;
&lt;p&gt;不過這邊我們需要講清楚的是究竟是以何種機制來做選擇的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PCA 的做法就是要找一個點 $\mu\in R^p$ 使得所有資料點到這個點的距離平方和最小, 也就是要讓下式有最小值
$$
\sum_{i=1}^n \|x_i - \mu\|^2.
$$
簡單的微分求極值我們可以得到其最佳解為
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i,
$$
也就是原始資料點的平均值.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果我們想要的是&amp;quot;距離&amp;quot;和最小而不是&amp;quot;距離平方&amp;quot;和, 也就是要使下式最小,
$$
\sum_{i=1}^n \|x_i - \mu\|,
$$
則最佳解為資料的中位數. 證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm#:~:text=111-,The%20Median%20Minimizes%20the%20Sum,Deviations%20%28The%20L1%20Norm%29&amp;amp;text=is%20minimal%20if%20x%20is%20equal%20to%20the%20median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Median minimizes sum of absolute deviations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-1-維&#34;&gt;Example: 投影到 $1$ 維&lt;/h3&gt;
&lt;p&gt;投影到 $0$ 維也許過於簡化, 接著我們來投影到 $1$ 維試試, 我們直接用圖形來說明:















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

藍色小圈就是原始資料點, 共10筆資料. 而 PCA 要做的事就是找到一個一維的 affine subspace, 就是中間那條斜線. 這樣我們就能把原始資料都投影到這個子空間去, 投影後的資料就是紅色點.&lt;/p&gt;
&lt;p&gt;而這 affine subspace 怎麼找到的呢, 事實上這個子空間就是使得所有虛線段(原始資料到投影點連線)距離平方加起來最小的那個.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我們來定義更一般的投影, 假設想要投影到 $k$ 維, $k\le p$, 也就是我們想要在 $R^p$ 空間中找一個 $k$ 維的 affine subspace, 使得說投影後資料與原始資料的距離平方和最小. 數學上來說就是要讓下式有最小值
$$
\sum_{i=1}^n \| x_i - (\mu + U\beta_i)\|^2,
$$
其中未知數有 $\mu\in R^p$, $U\in R^{p\times k}$, 以及 $\beta_i\in R^{k\times 1}$.&lt;/p&gt;
&lt;p&gt;稍微解釋一下以上這些變數:&lt;/p&gt;
&lt;p&gt;事實上這個 affine subspace 可以表示成
$$
\mu + V, \quad V = span\{u_1, \cdots, u_k\},
$$
其中 $\mu$ 是 $R^p$ 空間中的一個點, $\{u_1, u_2, \cdots, u_k\}$ 是這 affine subspace 的基底, 收集在一起構成 $U$, 也就是 $U=[u_1, u_2, \cdots, u_k]$. 而 $\mu + U\beta_i$ 就是 $x_i$ 這個資料點在這 affine subspace 上的投影, 所以 $\beta_i$ 可以想像是原始資料第 $i$ 筆投影到 affine subspace 之後的座標, 或是係數.&lt;/p&gt;
&lt;p&gt;此外, 我們可以特別要求這基底要正交, 也就是 $U^TU = I_k$, 其中 $I_k$ 表示 $k\times k$ 的 identity matrix.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;再稍微整理一下, 我們想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;第二個條件 $\sum_i \beta_i=\vec{0}$ 看起來似乎是憑空冒出來的, 不過這可以從兩方面來說.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們希望投影之後的解他們的平均是 $0$, 所以新資料點的中心就在新座標原點的位置.&lt;/li&gt;
&lt;li&gt;數學上來說, 其實 $\mu$ 並沒有唯一性. 也就是雖然 affine subspace 表示成 $\mu +V$, 但是這個 $\mu$ 是這子空間裡的任何一個點都可以. 為了讓解有唯一性我們加了這個條件.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;這問題可以被完全解出來, 也就是說給定 $x_i$, 我們可以決定出最佳的 $\mu$, $U$ 以及 $\beta_i$ 使得上式的值為最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推導過程我們在這先省略, 有興趣的請見 references, 或是 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其最佳解整理如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 就剛好是投影到 $0$ 維的解, 也就是資料點的平均:
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$ 是個 $p\times n$ 的矩陣, 而 $\Sigma$ 則是 $p\times p$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原資料投影到此 affine subspace 中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, (x_i-\mu)&amp;gt;
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般最基礎的 PCA 做法就是從 $x_i$, 得到 $\mu$. 將資料平移求出 $y_i$, 接著求共變異數矩陣 $\Sigma$. 接著對 $\Sigma$ 做 eigen-decomposition 求出其特徵值及特徵向量. 拿出最大的幾個就可以定出 affine subspace. 然後將 $y_i$ 投影下去就得到 $\beta_i$, 也就是投影之後的座標了.&lt;/p&gt;
&lt;p&gt;以上這做法的 &lt;code&gt;Matlab&lt;/code&gt; 程式如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$X$ 是 $p\times n$ 的 data matrix 含有 $p$ 個 features 以及 $n$ 個 samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;投影到 $k$ 維子空間, 投影後得到 $B$, 為 $k\times n$ 的 data matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均 mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    S = Y*Y&#39;;                       % 求出共變異數矩陣 S
    [U, D] = eig(S, &#39;vector&#39;);      % 求出特徵值 D 及特徵向量 U
    [D, ind] = sort(D, &#39;descend&#39;);  % 將特徵值由大到小排列
    U = U(:, ind);                  % 將特徵向量照樣排列
    B = U(:,1:k)&#39;*Y;                % 投影到前 k 個組成的空間中並求出係數 B
end
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這做法會造出一個 $p\times p$ 的共變異數矩陣 $\Sigma$, 並且算出全部的特徵值及特徵向量. 不過其實我們只需要前 $k$ 個. 所以多出來的部分會丟掉, 有點浪費.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們再來看一下這個共變異數矩陣 $\Sigma=YY^T$, 可以很輕易地看出來這是個對稱矩陣, 所以特徵值一定是實數, 也一定存在 orthonormal 的實數特徵向量組. 因此我們以上的要求(將特徵值按大小排列, 特徵向量要彼此正交)都一定做得到. 而分解之後可以得到
$$
\Sigma = U\Lambda U^T,
$$
其中 $U$ 是個 $p\times p$ 矩陣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;事實上由 $\Sigma=YY^T$ 可以知道 $\Sigma$ 一定是個半正定矩陣, 所以它的 eigenvalues 一定都非負.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們事實上可以將矩陣 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}$ 及 $\hat{V}$ 都是 orthogonal matrix, 也就是 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$.&lt;/p&gt;
&lt;p&gt;既然我們有 $\Sigma=YY^T$, 很容易可以看出來其實
$$
U = \hat{U}, \quad \Lambda = \hat{\Sigma}\hat{\Sigma}^T\in R^{p\times p}.
$$
也就是說將 $Y$ 的 singular values 平方就可以得到 $\Sigma$ 的 eigenvalues. 而 $Y$ 的 left singular vectors 事實上就是 $\Sigma$ 的 eigenvectors.&lt;/p&gt;
&lt;p&gt;而投影後的係數我們可以算一下:
$$
B=U^TY = \hat{U}^T\hat{U}\hat{\Sigma}\hat{V}^T = \hat{\Sigma}\hat{V}^T.
$$
所以如果我們只需要投影後的係數, 將 $Y$ 做 SVD 並且我們需要的是 $Y$ 的 right singular vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;以上這式子很有趣, 告訴我們投影之後的座標 $B$ 與投影之前的座標 $Y$ 的 SVD 之間的關係.
另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt; 在某些情況下會有一模一樣的關係. 這樣就把兩種方法 PCA 跟 MDS 連在一起了. 雖然出發點不一樣, 竟然(在某些情況下)結果是一樣的!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;利用以上關係我們可以將程式改寫如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis2(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均, mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    [~, S, V] = svds(Y, k);         % 將 Y 做奇異值分解並取前 k 個 eigenvectors
    B = S*V&#39;;                       % 求出投影後的係數
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;而事實上, matlab 已經內建 PCA 程式了, 所以其實完全不用自己寫. 只是要注意一下 matlab 裡 PCA 的輸入 sample points 是 $n\times p$, 由於我們以上都是將 $X$ 設成 $p\times n$ 的矩陣, 所以要轉置一下, 程式只有兩行, 如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
[U, B] = pca(X&#39;);
B = B(:,1:k)&#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;working-example&#34;&gt;Working example&lt;/h1&gt;
&lt;p&gt;這裡我們舉一個例子, 我們先構造 sample points, 是一個類似螺旋狀結構, 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
n = 1000;                                   % 取 n 個點
t = 3*pi*(1:n)/n;                           % 利用參數化來構造, 取 t\in[0, 3*pi]
X = [t.*cos(t); 5*rand(1,n); t.*sin(t)];    % 先利用 random 構造三圍中的一個面
M = makehgtform(&#39;axisrotate&#39;,[1 1 1], 30);  % 構造旋轉矩陣
X = M(1:3, 1:3)*X + 10*rand(3,1)*ones(1,n); % 將 sample 旋轉並且隨機平移
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scatter3(X(1,:), X(2,:), X(3,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;接著我們用 PCA 投影到二維, 圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[~, B] = pca(X&#39;);
B=B&#39;;
scatter(B(1,:), B(2,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;可以發現 PCA 找到一個正確的軸來做投影, 使得原本的螺旋線可以看得很清楚.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PCA 推導過程及證明: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>擲硬幣問題</title>
      <link>https://teshenglin.github.io/post/2020_coin_problem/</link>
      <pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_coin_problem/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;重複擲一枚硬幣, 當出現連續兩次為正時停止, 平均要扔幾次?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;第一種做法-窮舉法&#34;&gt;第一種做法: 窮舉法&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們以正負號 (+ -) 代表正面反面.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;觀察&#34;&gt;觀察&lt;/h3&gt;
&lt;p&gt;假設花 $n$ 次才擲到連續兩次為正, $n \geq 2$.&lt;/p&gt;
&lt;p&gt;我們發現, 可以從第 $n-1$ 次的結果進行窮舉法得到第 $n$ 次的結果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當 $n = 2$, 只會有 1 種情況 : &lt;code&gt;++&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;要窮舉 $n = 3$, 要知道三次中的尾巴兩次必須是 $n=2$ 的所有情況. 由於 $n=2$ 的第一個為正, 因此要在前加一項只能為負, 因此 $n=3$ 只有 &lt;code&gt;-++&lt;/code&gt; 這情況&lt;/li&gt;
&lt;li&gt;對 $n=4$, 尾巴三次必須是 $n=3$ 的所有情況. 由於 $n=3$ 的第一個為負, 因此要在前加正或負皆可, 因此 $n=4$ 有兩種情況: &lt;code&gt;+-++&lt;/code&gt;,  &lt;code&gt;--++&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;小結論&#34;&gt;小結論&lt;/h4&gt;
&lt;p&gt;對 $n=k+1$ 而言, 我們只需要考慮 $n=k$ 所有情況的第一個是正或負即可. 若為正則只能在前面再加個負, 若為負則可以在前面加正或負.&lt;/p&gt;
&lt;p&gt;也就是說, 假設 $n=k$ 中有 $x_k$ 個情況第一個為正, 有 $y_k$ 個情況第一個為負, 那 $n=k+1$ 的所有情況中必有 $y_k$ 個情況第一個為正, $x_k+y_k$ 個情況第一個為負.
寫成矩陣型式就是
$$
\begin{bmatrix} x_{k+1} \\ y_{k+1}\end{bmatrix}
= A
\begin{bmatrix} x_{k} \\ y_{k}\end{bmatrix}, \quad
A =
\begin{bmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 1 \end{bmatrix}
$$
則我們有
$$
\begin{bmatrix} x_{k} \\ y_{k}\end{bmatrix}
= A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}, \quad k\ge 2.
$$
並且我們知道花 $k$ 次才達到連續兩次為正共會有 $x_k+y_k$ 種情況, 意即
$$
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}.
$$
另外我們還知道丟硬幣 $k$ 次之任一情況的機率為 $\frac{1}{2^k}$.&lt;/p&gt;
&lt;h3 id=&#34;計算期望值&#34;&gt;計算期望值&lt;/h3&gt;
&lt;p&gt;利用以上結果我們可以知道期望值為
$$
E = \sum_{k=2}^\infty
\left(
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
\right)
\cdot \frac{k}{2^{k}}
=\sum_{k=0}^\infty
\left(
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
\right)
\cdot \frac{k+2}{2^{k+2}}.
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;對角化
$$
A = \begin{bmatrix}0 &amp;amp; 1 \\ 1 &amp;amp; 1\end{bmatrix}
= \frac{1}{\sqrt{5}}
\begin{bmatrix}1 &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\frac{1+\sqrt{5}}{2} &amp;amp; 0 \\ 0 &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\frac{\sqrt{5}-1}{2} &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; -1\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可得
$$
A^k
= \frac{1}{\sqrt{5}}
\begin{bmatrix}1 &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\left(\frac{1+\sqrt{5}}{2}\right)^k &amp;amp; 0 \\ 0 &amp;amp; \left(\frac{1-\sqrt{5}}{2}\right)^k\end{bmatrix}
\begin{bmatrix}\frac{\sqrt{5}-1}{2} &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; -1\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接著我們有
$$
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
=\frac{1}{\sqrt{5}}\left[(\frac{1+\sqrt{5}}{2})^{k-1}\cdot\frac{3+\sqrt{5}}{2} - (\frac{1-\sqrt{5}}{2})^{k-1}\cdot\frac{3-\sqrt{5}}{2}\right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此
$$
E = \frac{3+\sqrt{5}}{16\sqrt{5}} \underbrace{\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1+\sqrt{5}}{4})^{k-1}}_{(a)} - \frac{3-\sqrt{5}}{16\sqrt{5}} \underbrace{\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1-\sqrt{5}}{4})^{k-1}}_{(b)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;recall-calculus&#34;&gt;Recall Calculus&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;要算 (a) 與 (b) 我們先 recall 微積分裡的一個結果&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$
\sum_{n=0}^{\infty} x^n = \frac{1}{1-x}, \quad |x| &amp;lt; 1.
$$
兩邊微分得到
$$
\sum_{n=0}^{\infty} n x^{n-1} = \frac{1}{(1-x)^2}, \quad |x| &amp;lt; 1.
$$&lt;/p&gt;
&lt;p&gt;利用這個我們可以算 (a)
$$
\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1+\sqrt{5}}{4})^{k-1} = 18 + 10\sqrt{5}
$$
也可以算 (b)
$$
\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1-\sqrt{5}}{4})^{k-1} = 18 - 10\sqrt{5}
$$&lt;/p&gt;
&lt;p&gt;最後,
$$
E = \frac{3+\sqrt{5}}{16\sqrt{5}}\cdot (18+10\sqrt{5}) - \frac{3-\sqrt{5}}{16\sqrt{5}}\cdot (18-10\sqrt{5}) = 6
$$&lt;/p&gt;
&lt;h3 id=&#34;結論&#34;&gt;結論&lt;/h3&gt;
&lt;p&gt;重複擲一枚硬幣, 當出現連續兩次為正時停止, 平均要扔&lt;strong&gt;六&lt;/strong&gt;次.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;第二種做法&#34;&gt;第二種做法:&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們可以用 Markov chain 來想這問題&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;觀察-1&#34;&gt;觀察&lt;/h3&gt;
&lt;p&gt;假設目前連續投擲出 0 個正, 那下一投有 $0.5$ 機率是正以及 $0.5$ 機率是負.
也就是 $0.5$ 機率會是到 &lt;code&gt;1正&lt;/code&gt; 這狀態以及 $0.5$ 機率回到 &lt;code&gt;0正&lt;/code&gt; 這狀態.&lt;/p&gt;
&lt;p&gt;假設目前連續投擲出 1 個正, 那下一投則 $0.5$ 機率會是到 &lt;code&gt;2正&lt;/code&gt; 這狀態以及 $0.5$ 機率回到 &lt;code&gt;0正&lt;/code&gt; 這狀態.&lt;/p&gt;
&lt;p&gt;根據這些觀察我們可以做出以下的 diagram:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34; data-lang=&#34;mermaid&#34;&gt;graph LR;
A[0 正] -- 0.5 --&amp;gt;B(1 正)
  B -- 0.5 --&amp;gt; C(2 正)
  A -- 0.5 --&amp;gt; A
  B -- 0.5 --&amp;gt; A
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接著我們假設從 &lt;code&gt;0正&lt;/code&gt; 這狀態需要投 $k_0$ 次才會到 &lt;code&gt;2正&lt;/code&gt;, 而從 &lt;code&gt;1正&lt;/code&gt; 這狀態需要投 $k_1$ 次才會到 &lt;code&gt;2正&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;對 &lt;code&gt;0正&lt;/code&gt; 狀態而言, 投一次有 $0.5$ 機會到&lt;code&gt;1正&lt;/code&gt;狀態, 然後再需要 $k_1$ 次到&lt;code&gt;2正&lt;/code&gt;, 所以這樣總共投 $1+k_1$ 次.
而 &lt;code&gt;0正&lt;/code&gt; 投一次也有 $0.5$ 機會會回到自己原始 &lt;code&gt;0正&lt;/code&gt; 狀態, 然後再需要 $k_0$ 次到&lt;code&gt;2正&lt;/code&gt;, 所以這樣總共投 $1+k_0$ 次.
如此可以列式:
$$
k_0 = 0.5(1+k_1) + 0.5(1+k_0)
$$&lt;/p&gt;
&lt;p&gt;同理, 對 &lt;code&gt;1正&lt;/code&gt; 狀態而言我們有以下式子
$$
k_1 = 0.5(1) + 0.5(1+k_0)
$$&lt;/p&gt;
&lt;p&gt;這樣我們有兩個未知數兩個方程式, 解出來得到
$$
k_0=6, \quad k_1=4.
$$&lt;/p&gt;
&lt;p&gt;所以, 從 &lt;code&gt;0正&lt;/code&gt; 狀態到出現連續兩次為正平均要投 $6$ 次.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
