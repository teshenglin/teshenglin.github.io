<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear algebra | Te-Sheng Lin</title>
    <link>https://teshenglin.github.io/categories/linear-algebra/</link>
      <atom:link href="https://teshenglin.github.io/categories/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <description>Linear algebra</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 19 Dec 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Linear algebra</title>
      <link>https://teshenglin.github.io/categories/linear-algebra/</link>
    </image>
    
    <item>
      <title>Least square method 2</title>
      <link>https://teshenglin.github.io/post/2023_la_least_square_2/</link>
      <pubDate>Tue, 19 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_la_least_square_2/</guid>
      <description>&lt;h1 id=&#34;最小平方法-2&#34;&gt;最小平方法 2&lt;/h1&gt;
&lt;p&gt;給定一個矩陣 $A$ 以及一個向量 $b$, 我們想要找到一個向量 $x$ 使得 $\|Ax - b\|^2+\|x\|^2$ 最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
A\in M_{m\times n}, \quad b \in M_{m\times 1}, \quad x\in M_{n\times 1}.
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;h3 id=&#34;11-non-uniqueness&#34;&gt;1.1 Non-uniqueness&lt;/h3&gt;
&lt;p&gt;最小平方法 $\|Ax-b\|^2$ 問題有時候解並不唯一, 常見的例子例如深度學習裡的神經網路, 他的參數數量通常都遠比資料點數量多非常多. 若我們把 $x$ 看成所有要找的參數的集合, 因此 $n$ 就是參數個數. 然後 $b$ 就是 target 資料集, 所以 $m$ 就是資料個數. 所以常常會有 $m \ll n$ 的情形. 這樣的話 $A$ 的 null space 不為零, 最小平方法的解空間變成了一個 affine subspace, 有無窮多組解.&lt;/p&gt;
&lt;h4 id=&#34;111-sensitivity-in-prediction&#34;&gt;1.1.1 Sensitivity in prediction&lt;/h4&gt;
&lt;p&gt;由於有無窮多組解, 因此選的解變異就非常大. 而最怕的情形就是某個參數非常的大. 這樣訓練出來的模型會很敏感, 一點點小擾動預測就差非常多.&lt;/p&gt;
&lt;p&gt;舉個極端的例子, 比如說我們要找一個模型 $f(x,y) = ax+by$, 其中 $a, b$ 是參數. 假設我們只有一筆資料 $f(1,1) = 0$. 這樣的話我們就只有一個方程式, 也就是, 模型裡的參數必須滿足&lt;/p&gt;
&lt;p&gt;$$
a + b = 0,
$$&lt;/p&gt;
&lt;p&gt;有無窮多解!&lt;/p&gt;
&lt;p&gt;如果我們選 $(a, b) = (10000, -10000)$. 那這樣我們的模型就是&lt;/p&gt;
&lt;p&gt;$$
f_1(x,y) = 10000x-10000y.
$$&lt;/p&gt;
&lt;p&gt;然後算一下 $f_1(0, 0)=0$ 以及 $f_1(1,0)=10000$, 初始值差 $1$ 不過預測值差 $10000$.&lt;/p&gt;
&lt;p&gt;如果我們選 $(a, b) = (1, -1)$. 那這樣我們的模型就是&lt;/p&gt;
&lt;p&gt;$$
f_2(x,y) = x-y.
$$&lt;/p&gt;
&lt;p&gt;因此 $f_2(0, 0)=0$ 以及 $f_2(1,0)=1$, 初始值差 $1$ 預測值也差 $1$.&lt;/p&gt;
&lt;p&gt;所以模型參數的大小會直接影響到模型預測的敏感度. 通常我們希望模型不要太敏感, 因此輸入的資料難免有誤差, 不要因為一點點的誤差就在預測差了十萬八千里. 而一個簡單的做法就是我們不僅要求 $\|Ax-b\|^2$ 要小, 我們也要求 $\|x\|^2$ 要小. 這樣子參數就不會太大了.&lt;/p&gt;
&lt;h3 id=&#34;12-ill-conditioning&#34;&gt;1.2 Ill-conditioning&lt;/h3&gt;
&lt;p&gt;在最小平方法的計算裡需要解 $A^TAx = A^Tb$ 這個系統. 不過 $A^TA$ 這矩陣我們只能保證半正定, 所以不一定可以解. 另外, 解這個矩陣也有可能會有很大的誤差 (在數值分析裡我們稱之為 ill-conditioned matrix). 簡單的說如果一個矩陣的 eigenvalue 離 $0$ 很靠近的話, 這個矩陣就會很像 singular matrix, 解起來就會有很大的誤差. 因此我們希望矩陣的 eigenvalue 遠離 $0$.&lt;/p&gt;
&lt;p&gt;一個簡單的觀察是, $A^TA + I$ 這個矩陣是個正定矩陣, 並且他的 eigenvalues 全都大於等於 $1$. 所以 $(A^TA+I)x = A^Tb$ 這個系統就會 well-condition, 解起來誤差不會太大.&lt;/p&gt;
&lt;h2 id=&#34;2-ridge-regression-and-its-dual-problem&#34;&gt;2. Ridge regression and its dual problem&lt;/h2&gt;
&lt;p&gt;首先我們定義 $\hat{x}$ 為找到的那個解, 也就是說, 我們要解以下這個問題&lt;/p&gt;
&lt;p&gt;$$
\newcommand{\argmin}{\arg\min}
\tag{1}
\hat{x} = \argmin_{x\in\mathbb{R}^n}\left(\|Ax - b\|^2+\|x\|^2\right).
$$&lt;/p&gt;
&lt;p&gt;首先觀察可以發現&lt;/p&gt;
&lt;p&gt;$$
\tag{2}
\|Ax - b\|^2+\|x\|^2 =
\left\|\begin{bmatrix}A\\ I
\end{bmatrix}x -
\begin{bmatrix}b\\ 0
\end{bmatrix}\right\|^2.
$$&lt;/p&gt;
&lt;p&gt;因此, (1) 其實就是個最小平方問題, 只是這個問題的系統變成加大的一個系統而已. 因此我們知道這個問題的解會滿足&lt;/p&gt;
&lt;p&gt;$$
\tag{3}
\begin{bmatrix}A^T &amp;amp; I
\end{bmatrix}
\begin{bmatrix}A\\ I
\end{bmatrix}\hat{x} =
\begin{bmatrix}A^T &amp;amp; I
\end{bmatrix}
\begin{bmatrix}b\\ 0
\end{bmatrix},
$$&lt;/p&gt;
&lt;p&gt;也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{4}
(A^TA + I)\hat{x} = A^Tb.
$$&lt;/p&gt;
&lt;p&gt;接著我們將 (4) 改寫成&lt;/p&gt;
&lt;p&gt;$$
\tag{5}
\hat{x} = A^T(b-A\hat{x}),
$$&lt;/p&gt;
&lt;p&gt;並且我們定義一個新變數 $\alpha$ 為&lt;/p&gt;
&lt;p&gt;$$
\tag{6}
\alpha = b-A\hat{x},
$$&lt;/p&gt;
&lt;p&gt;因此我們有&lt;/p&gt;
&lt;p&gt;$$
\tag{7}
\hat{x} = A^T\alpha.
$$&lt;/p&gt;
&lt;p&gt;接著從 (6) 跟 (7) 我們可以得到&lt;/p&gt;
&lt;p&gt;$$
\tag{8}
\alpha = b-A\hat{x} = b-AA^T\alpha,
$$&lt;/p&gt;
&lt;p&gt;整理一下得到 $\alpha$ 要滿足的方程式為&lt;/p&gt;
&lt;p&gt;$$
\tag{9}
(AA^T+ I)\alpha = b.
$$&lt;/p&gt;
&lt;p&gt;最後, 由於 $\hat{x}=A^T\alpha$ 我們可以得到&lt;/p&gt;
&lt;p&gt;$$
\tag{10}
\hat{x}  = A^T(AA^T + I)^{-1}b.
$$&lt;/p&gt;
&lt;h3 id=&#34;21-qr-decomposition&#34;&gt;2.1 QR decomposition&lt;/h3&gt;
&lt;p&gt;我們對 $A$ 做 (reduced) QR 分解得到&lt;/p&gt;
&lt;p&gt;$$
\tag{11}
A = QR,
$$
where $Q^TQ= I_{r\times r}$, $Q\in M_{m\times r}$ and $R\in M_{r\times n}$.&lt;/p&gt;
&lt;p&gt;那 least square 問題的解 (4) 可以改寫成&lt;/p&gt;
&lt;p&gt;$$
\tag{12}
(R^TR+I)\hat{x} = R^TQ^Tb.
$$&lt;/p&gt;
&lt;p&gt;而 (10) 則是寫成&lt;/p&gt;
&lt;p&gt;$$
\tag{13}
\hat{x} = R^T(RR^T+I)^{-1}Q^Tb,
$$&lt;/p&gt;
&lt;h2 id=&#34;3-conclusion&#34;&gt;3. Conclusion&lt;/h2&gt;
&lt;p&gt;我們考慮以下最小平方法問題&lt;/p&gt;
&lt;p&gt;$$
\min_{x\in\mathbb{R}^n}\left(\|Ax - b\|^2+\|x\|^2\right),
$$&lt;/p&gt;
&lt;p&gt;並且我們令最佳解為 $\hat{x}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $m&amp;gt;n$, 我們以下列式子來計算
$$
\hat{x} = (A^TA+I)^{-1}A^Tb.
$$
&lt;ul&gt;
&lt;li&gt;如果對 $A$ 做 (reduced) QR, $A=QR$, 並且 $Q^TQ=I_{n\times n}$,
$$
\hat{x} = (R^TR+I)^{-1}R^TQ^Tb.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果 $m&amp;lt;n$, 我們以下列式子來計算
$$
\hat{x} = A^T(AA^T+I)^{-1}b.
$$
&lt;ul&gt;
&lt;li&gt;如果對 $A$ 做 (reduced) QR, $A=QR$, 並且 $Q^TQ=I_{n\times n}$,
$$
\hat{x} = R^T(RR^T+I)^{-1}Q^Tb.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Gibbs_phenomenon - code</title>
      <link>https://teshenglin.github.io/post/2023_gibbs_phenomenon/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_gibbs_phenomenon/</guid>
      <description>&lt;h3&gt;&lt;a href=&#34;https://teshenglin.github.io/pybooks/Gibbs_phenomenon.html&#34;&gt;Click here to view this notebook in full screen&lt;/a&gt;&lt;/h3&gt;
 &lt;iframe src=&#34;https://teshenglin.github.io/pybooks/Gibbs_phenomenon.html&#34;
        style=&#34;height:2750px;width:100%;border:none;overflow:hidden;&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析 - 0</title>
      <link>https://teshenglin.github.io/post/2023_principal_component_analysis_0/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_principal_component_analysis_0/</guid>
      <description>&lt;h1 id=&#34;principle-component-analysis---0&#34;&gt;Principle component analysis - 0&lt;/h1&gt;
&lt;h2 id=&#34;1-一維資料的統計學&#34;&gt;1. 一維資料的統計學&lt;/h2&gt;
&lt;p&gt;假設我們有 $n$ 筆資料, 每筆資料都是一個數字 (例如 $n$ 個學生的成績). 這 $n$ 筆資料我們設為 $x_1, \cdots, x_n$, 並且定義一個矩陣&lt;/p&gt;
&lt;p&gt;$$
\tag{1}
A =
\begin{bmatrix}
x_1 &amp;amp; \cdots &amp;amp; x_n
\end{bmatrix}\in M_{1\times n}.
$$&lt;/p&gt;
&lt;p&gt;那這些資料的平均數為&lt;/p&gt;
&lt;p&gt;$$
\tag{2}
\mu = \frac{1}{n}(x_1+\cdots+x_n) = \frac{1}{n}\begin{bmatrix}
1 &amp;amp; \cdots &amp;amp; 1
\end{bmatrix}\begin{bmatrix}
x_1\\
\vdots\\&lt;br&gt;
x_n
\end{bmatrix} = \frac{1}{n}\mathbb{1}^TA^T,
$$&lt;/p&gt;
&lt;p&gt;其中 $\mathbb{1}^T = [1, \cdots, 1]$ 是個全為 $1$ 的向量.&lt;/p&gt;
&lt;p&gt;資料的變異數 (variance) 則定義為&lt;/p&gt;
&lt;p&gt;$$
\tag{3}
\text{Var}(A) = \sigma^2 = \frac{1}{n}\sum^n_{k=1} (x_i - \mu)^2,
$$&lt;/p&gt;
&lt;p&gt;而 $\sigma$ 則是標準差 (standard deviation, std).&lt;/p&gt;
&lt;p&gt;要將之寫成矩陣形式首先我們定義置中矩陣 (centering matrix)&lt;/p&gt;
&lt;p&gt;$$
\tag{4}
H = I - \frac{1}{n}\mathbb{1}\mathbb{1}^T.
$$&lt;/p&gt;
&lt;p&gt;計算一下可以發現&lt;/p&gt;
&lt;p&gt;$$
\tag{5}
HA^T = (I - \frac{1}{n}\mathbb{1}\mathbb{1}^T)A^T = A^T - \mu\mathbb{1} = \begin{bmatrix}
x_1 -\mu\\&lt;br&gt;
\vdots\\&lt;br&gt;
x_n -\mu
\end{bmatrix} = Y^T,
$$&lt;/p&gt;
&lt;p&gt;其中我們將這些置中後的資料存為 $Y$ 矩陣. 接著我們就可以知道&lt;/p&gt;
&lt;p&gt;$$
\tag{6}
\sigma^2 = \frac{1}{n}YY^T.
$$&lt;/p&gt;
&lt;h2 id=&#34;2-二維資料的統計學&#34;&gt;2. 二維資料的統計學&lt;/h2&gt;
&lt;p&gt;假設我們有 $n$ 筆資料, 每筆資料都是 $2$ 個數字 (例如 $n$ 個學生在 $2$ 次考試的成績), 這兩個數字我們稱之為這筆資料的 features. 這 $n$ 筆資料我們設為 $(x_1, y_1), \cdots, (x_n, y_n)$, 並且定義一個矩陣&lt;/p&gt;
&lt;p&gt;$$
\tag{7}
A =
\begin{bmatrix}
x_1 &amp;amp; \cdots &amp;amp; x_n\\&lt;br&gt;
y_1 &amp;amp; \cdots &amp;amp; y_n
\end{bmatrix}\in M_{2\times n}.
$$&lt;/p&gt;
&lt;p&gt;我們把每種資料都平移, 使其平均為 $0$, 並令平移後的資料為 $Y$. 簡單計算可以發現我們一樣可以用置中矩陣來做, $HA^T = Y^T$,&lt;/p&gt;
&lt;p&gt;$$
\tag{8}
Y^T = \begin{bmatrix}
x_1 -\mu_x &amp;amp; y_1 -\mu_y\\
\vdots &amp;amp; \vdots\\&lt;br&gt;
x_n -\mu_x &amp;amp; y_n -\mu_y
\end{bmatrix}, \quad
\mu_x = \frac{1}{n}\sum^n_{k=1} x_k, \quad
\mu_y = \frac{1}{n}\sum^n_{k=1} y_k.
$$&lt;/p&gt;
&lt;p&gt;接著我們可以定義兩個變數的共變異數 (covariance),&lt;/p&gt;
&lt;p&gt;$$
\tag{9}
\text{cov}(x, y) = \frac{1}{n}\sum^n_{k=1}(x_k - \mu_x)(y_k-\mu_y).
$$&lt;/p&gt;
&lt;p&gt;接著計算一下就可以發現&lt;/p&gt;
&lt;p&gt;$$
\tag{10}
\frac{1}{n}YY^T = \begin{bmatrix}
\text{cov}(x,x) &amp;amp; \text{cov}(x,y)\\
\text{cov}(x,y) &amp;amp; \text{cov}(y,y)
\end{bmatrix},
$$&lt;/p&gt;
&lt;p&gt;也就是所謂的共變異數矩陣. 這個矩陣的對角線元素代表每個 feature 自己的變異數, 而非對角線則是共變異數, 代表兩個 features 的相關程度.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;/em&gt;: 不過要真正算相關程度會更近一步的去計算相關係數 (correlation coefficients), 這邊就不再深入探討.&lt;/p&gt;
&lt;h2 id=&#34;3-pca-maximize-variance&#34;&gt;3. PCA: maximize variance&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要找到一個方向, 使得資料投影上去之後, 新資料的變異數會最大.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設這個方向為 $v\in\mathbb{R}^2$, 並且 $\|v\|=1$, 那資料投影到 $v$ 所得的新資料就是 $v^TY \in M_{1\times n}$. 接著我們就來算這筆新資料的變異數.&lt;/p&gt;
&lt;p&gt;第一步一樣先置中. 也就是計算 $HY^Tv$. 不過由於 $Y$ 是置中過的資料, 因此 $HY^T = Y^T$, 所以 $HY^Tv = Y^Tv$, 也就是說, 新資料 $v^TY$ 的平均一定是 $0$.&lt;/p&gt;
&lt;p&gt;接著, 新資料的變異數就會是&lt;/p&gt;
&lt;p&gt;$$
\tag{11}
\sigma^2 =\frac{1}{n}(v^TY)(v^TY)^T =\frac{1}{n}v^TYY^Tv.
$$&lt;/p&gt;
&lt;p&gt;所以統整一下, 若我們將資料投影到 $v$ 上, 那新資料的變異數就是 (11). 而PCA 所要找的方向就是使變異數最大的方向, 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{12}
\hat{v} = \arg\max_{v\in\mathbb{R}^2, \|v\|=1} \left(v^TYY^Tv\right).
$$&lt;/p&gt;
&lt;p&gt;最後, 我們知道這個解可以由 $Y^T$ 這個矩陣的 SVD 得出.&lt;/p&gt;
&lt;p&gt;假設 $Y^T = U\Sigma V^T$, 那  $\hat{v} = v_1$, 也就是第一個 singular vector.&lt;/p&gt;
&lt;h2 id=&#34;4-pca-minimize-square-distance&#34;&gt;4. PCA: minimize square distance&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要找到一個方向, 使得資料投影上去之後, 新舊資料的距離平方和最小.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設這個方向為 $v\in\mathbb{R}^2$, 並且 $\|v\|=1$, 那資料投影到 $v$ 的投影點就是 $vv^TY \in M_{2\times n}$. 接著我們就來算新舊資料的距離平方和:&lt;/p&gt;
&lt;p&gt;$$
\tag{13}
\begin{align}
\sum^n_{k=1} d_k^2 &amp;amp;= \sum^n_{k=1}\|Y_k - vv^TY_k\|^2  \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}&amp;lt;Y_k-vv^TY_k, Y_k-vv^TY_k&amp;gt; \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}Y_k^TY_k -Y_k^Tvv^TY_k \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}Y_k^TY_k -v^TY_kY_k^Tv \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}(Y_k^TY_k) -v^TYY^Tv,
\end{align}
$$&lt;/p&gt;
&lt;p&gt;其中我們用到了 $v^TY_k$ 是個數字以及 $\sum Y_kY_k^T = YY^T$ 這兩件事.&lt;/p&gt;
&lt;p&gt;我們希望找到一個方向使得距離平方和最小, 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{14}
\hat{v}=\arg\min_{v\in\mathbb{R}^2, \|v\|=1} \left(\sum^n_{k=1}(Y_k^TY_k) -v^TYY^Tv\right) = \arg\max_{v\in\mathbb{R}^2, \|v\|=1} \left(v^TYY^Tv\right).
$$&lt;/p&gt;
&lt;p&gt;觀察 (12) 與 (14) 發現他們一模一樣. 也就是這兩個問題的解會是一樣的, 最佳的方向都是第一個 singular vector.&lt;/p&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PCA 想做的事就是找到一個仿射子空間 (affine subspace), 使得
&lt;ul&gt;
&lt;li&gt;投影下去之後的資料有最大的變異數&lt;/li&gt;
&lt;li&gt;投影前後的資料距離平方合最小.
&lt;blockquote&gt;
&lt;p&gt;而這兩件事情是等價的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PCA 也是一種資料降維的工具, 而將資料投影到一維所出來的新資料就是 $v^TY$.&lt;/li&gt;
&lt;li&gt;以上雖然是以二維資料為例, 不過若有 $m$ 維資料整個推導是一樣的.&lt;/li&gt;
&lt;li&gt;以上是以投影到一維為例, 若投影到更高維度就是依序找第二, 三, 等等的 singular vectors. 不過推導會利用到矩陣 trace 的一些性質, 一些細節這裡就先跳過.&lt;/li&gt;
&lt;li&gt;PCA 要找的是個仿射子空間, $V = \mu + \text{span}\{v\}$,  這裏我們都直接說 $\mu$ 就是資料的平均. 不過其實這是可以算出來的. 假設我們想要找一個點 $\mu$ 使得所有資料到這個點的距離和為最小, 也就是
$$
\tag{15}
\mu = \arg\min \sum^n_{k=1}(x_k - \mu)^2.
$$
我們先定義 $f(\mu) = \sum^n_{k=1}(x_k - \mu)^2$. 這是個單變數函數, 而且其實就是個 $\mu$ 的二次多項式, 首項係數等於 $1$, 有一個最小值. 接著微分求極值得到
$$
\tag{16}
\frac{d}{d\mu} f =  \sum^n_{k=1}(-2)(x_k - \mu) = (-2)\left[\sum^n_{k=1}x_k - n\mu\right]
$$
因此極值發生在 $\frac{d}{d\mu} f=0$, 也就是
$$
\tag{17}
\mu = \frac{1}{n}\sum^n_{k=1}x_k.
$$
所以到所有資料點距離和最小的就是平均數.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析 - code</title>
      <link>https://teshenglin.github.io/post/2023_principal_component_analysis_01/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_principal_component_analysis_01/</guid>
      <description>&lt;h3&gt;&lt;a href=&#34;https://teshenglin.github.io/pybooks/pca.html&#34;&gt;Click here to view this notebook in full screen&lt;/a&gt;&lt;/h3&gt;
 &lt;iframe src=&#34;https://teshenglin.github.io/pybooks/pca.html&#34;
        style=&#34;height:2750px;width:100%;border:none;overflow:hidden;&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Least square method 1</title>
      <link>https://teshenglin.github.io/post/2023_la_least_square_1/</link>
      <pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_la_least_square_1/</guid>
      <description>&lt;h1 id=&#34;最小平方法-1&#34;&gt;最小平方法 1&lt;/h1&gt;
&lt;p&gt;給定一個矩陣 $A$ 以及一個向量 $b$, 我們想要找到一個向量 $x$ 使得 $\|Ax - b\|^2$ 最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$
A\in M_{m\times n}, \quad b \in M_{m\times 1}, \quad x\in M_{m\times 1}.
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先我們定義 $\hat{x}$ 為找到的那個解, 也就是說, 我們要解以下這個問題&lt;/p&gt;
&lt;p&gt;$$
\newcommand{\argmin}{\arg\min}
\tag{1}
\hat{x} = \argmin_{x\in\mathbb{R}^n}\|Ax - b\|^2.
$$&lt;/p&gt;
&lt;h2 id=&#34;1-notations&#34;&gt;1. Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$C(A)$: column space of $A$.
$$
C(A) = \{Ax  |  x\in\mathbb{R}^n\}\subseteq\mathbb{R}^m.
$$&lt;/li&gt;
&lt;li&gt;$N(A)$: null space of $A$.
$$
N(A) = \{x\in\mathbb{R}^n | Ax = 0\}\subseteq\mathbb{R}^n.
$$&lt;/li&gt;
&lt;li&gt;Reduced QR for $A$.
$$
A = QR, \quad Q^TQ = I,
$$
但是 $Q$ 不是個方陣, $N(R^T)=\{0\}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-最小平方解與投影子空間&#34;&gt;2. 最小平方解與投影子空間&lt;/h2&gt;
&lt;p&gt;要找到最小平方解首先我們做個重要的觀察.&lt;/p&gt;
&lt;p&gt;事實上, 以下三件事是等價敘述:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;給定 $A$ 與 $b$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;找到一個向量 $\hat{x}\in\mathbb{R}^n$, 使得 $\|A\hat{x}-b\|^2$ 最小.&lt;/li&gt;
&lt;li&gt;找到一個向量 $\hat{b}\in C(A)\subseteq\mathbb{R}^m$, 使得 $\|\hat{b}-b\|^2$ 最小.&lt;/li&gt;
&lt;li&gt;將 $b$ 投影到 $C(A)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;因為 $A\hat{x}$ 可以視為 $A$ 的 columns 的線性組合. 而任何在 column space $C(A)$ 裡的向量也都可以被寫成 $Ax$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以若我們知道怎樣解其中一個, 另外兩個問題就同時解出來了. 我們先證明以下這個 lemma.&lt;/p&gt;
&lt;h3 id=&#34;lemma&#34;&gt;Lemma&lt;/h3&gt;
&lt;p&gt;任意給定一個向量 $b\in\mathbb{R}^m$, 如果我們能夠找到一個向量 $\hat{b}\in\mathbb{R}^m$ 滿足 $(\hat{b}-b)\perp C(A)$, 那這個向量就會是 $b$ 在 $C(A)$ 的投影向量,
$$
\tag{2}
\hat{b} = \arg\min_{\hat{b}\in\mathbb{R}^m}\|\hat{b}-b\|^2.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pf:&lt;/p&gt;
&lt;p&gt;Let $\hat{b}\in C(A)\subseteq\mathbb{R}^m$ such that $(\hat{b}-b)\perp C(A)$.&lt;/p&gt;
&lt;p&gt;Given $p\in C(A)$, we define $e = p - \hat{b}$ and we have $e\in C(A)$.&lt;/p&gt;
&lt;p&gt;$$
\tag{3}
\begin{align}
\|p - b\|^2 &amp;amp;= &amp;lt;p-b, p-b&amp;gt; \\&lt;br&gt;
&amp;amp;= &amp;lt;\hat{b} + e - b, \hat{b} + e - b&amp;gt;\\&lt;br&gt;
&amp;amp;= \|\hat{b} - b\|^2 + 2&amp;lt;\hat{b} - b, e&amp;gt; + \|e\|^2\\&lt;br&gt;
&amp;amp;= \|\hat{b} - b\|^2  + \|e\|^2,
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where we have used the fact that $(\hat{b}-b)\perp C(A)$ and $e\in C(A)$, so that $&amp;lt;\hat{b} - b, e&amp;gt;=0$.&lt;/p&gt;
&lt;p&gt;Therefore, for any $p\in C(A)$, $\|p - b\|^2 \ge \|\hat{b} - b\|^2$, and the minimal of $\|p - b\|^2$ occurs when $\|e\|^2=0$, that is when $p=\hat{b}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;由這 lemma 我們知道 $\hat{b}$ 可以從 $(\hat{b}-b)\perp C(A)$ 這個條件下手, 也就是要找一個 $\hat{b}$ 滿足&lt;/p&gt;
&lt;p&gt;$$
\tag{4}
A^T(\hat{b}-b)=0.
$$&lt;/p&gt;
&lt;p&gt;那因為 $\hat{b}\in C(A)$, 一定存在某個 $\hat{x}\in\mathbb{R}^n$ 使得 $A\hat{x}=\hat{b}$. 因此 (4) 就變成&lt;/p&gt;
&lt;p&gt;$$
\tag{5}
A^T(A\hat{x}-b)=0.
$$&lt;/p&gt;
&lt;p&gt;展開就知道 $\hat{x}$ 要滿足&lt;/p&gt;
&lt;p&gt;$$
\tag{6}
A^TA\hat{x}=A^Tb.
$$&lt;/p&gt;
&lt;p&gt;因此, 我們剛剛說的事其解就是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找到一個向量 $\hat{x}\subseteq\mathbb{R}^n$, 使得 $\|A\hat{x}-b\|^2$ 最小.
&lt;ul&gt;
&lt;li&gt;$\hat{x}$ 滿足 $A^TA\hat{x}=A^Tb$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;找到一個向量 $\hat{b}\in C(A)\subseteq\mathbb{R}^m$, 使得 $\|\hat{b}-b\|^2$ 最小.
&lt;ul&gt;
&lt;li&gt;$\hat{b}$ 滿足 $(\hat{b}-b)\perp C(A)$, 或是 $A^T(\hat{b}-b)=0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;以上推導都是充分條件, 就是如果我們解出 (4) 或 (6), 那他們就一定是 (1) 的解.&lt;/li&gt;
&lt;li&gt;以上推導跟 $A$ 的 column 有沒有 linearly independent 無關.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-independent-columns&#34;&gt;3. Independent columns&lt;/h2&gt;
&lt;p&gt;如果 $A$ 的 column 是 linearly independent, 那 $A^TA$ 就可逆, 然後我們就可以把 $\hat{x}$ 顯式的寫下來, 就得到
$$
\tag{7}
\hat{x} = (A^TA)^{-1}A^Tb.
$$
在這情況下, $b$ 在 $C(A)$ 的投影也可以寫下來, 就是
$$
\tag{8}
\hat{b} = A(A^TA)^{-1}A^Tb.
$$
或是我們可以更近一步定義投影到 $C(A)$ 的投影矩陣, 就是
$$
\tag{9}
P = A(A^TA)^{-1}A^T.
$$&lt;/p&gt;
&lt;h2 id=&#34;4-dependent-columns&#34;&gt;4. Dependent columns&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;$A^TA$ 不可逆&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;case-1&#34;&gt;Case 1&lt;/h3&gt;
&lt;p&gt;假設 $m&amp;lt;n$ 並且 $\text{rank}(A)=n$.&lt;/p&gt;
&lt;p&gt;在這情況下, $A$ 的 columns 不是 linearly independent, $A^TA$ 不可逆, 並且 $N(A)\ne{0}$. 所以若有一個最小平方解, 那就還會有無窮多個. 不過我們至少先找到一個再說.&lt;/p&gt;
&lt;p&gt;因為 $A$ 的 rows 會線性獨立, 因此我們對 $A^T$ 做 (reduced) QR 分解得到&lt;/p&gt;
&lt;p&gt;$$
\tag{10}
A^T = QR,
$$
where $Q^TQ= I$, $Q\in M_{n\times m}$ and $R\in M_{m\times m}$. 並且我們知道 $R$ 是可逆矩陣.&lt;/p&gt;
&lt;p&gt;接著我們知道最小平方解必須滿足 (6), 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{11}
QRR^TQ^T \hat{x} = QRb.
$$&lt;/p&gt;
&lt;p&gt;兩邊同乘 $(R^T)^{-1}R^{-1}Q^T$ 我們得到&lt;/p&gt;
&lt;p&gt;$$
\tag{12}
Q^T \hat{x} = (R^T)^{-1}b.
$$&lt;/p&gt;
&lt;p&gt;最後, 如果我們選擇&lt;/p&gt;
&lt;p&gt;$$
\tag{13}
\hat{x} = Q(R^T)^{-1}b,
$$&lt;/p&gt;
&lt;p&gt;那可以很容易驗證 (12) 是滿足的. 也就是說 (13) 會是這個問題的一個解.&lt;/p&gt;
&lt;h3 id=&#34;case-2&#34;&gt;Case 2&lt;/h3&gt;
&lt;p&gt;假設 $\text{rank}(A)=r$, 並且 $r&amp;lt;m$, $r&amp;lt;n$.&lt;/p&gt;
&lt;p&gt;我們對 $A$ 做 (reduced) QR 分解得到&lt;/p&gt;
&lt;p&gt;$$
\tag{14}
A = QR,
$$
where $Q^TQ= I$, $Q\in M_{m\times r}$ and $R\in M_{r\times n}$.&lt;/p&gt;
&lt;p&gt;接著我們知道最小平方解必須滿足 (6), 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{15}
R^TR \hat{x} = R^TQ^Tb.
$$&lt;/p&gt;
&lt;p&gt;不過要注意的是這裡 $R^TR\in M_{n\times n}$ 不是一個可逆矩陣, 所以操作上無法兩邊同乘其反矩陣. 但是好消息是 $R^T$ 的 columns 是線性獨立的, 所以我們會有&lt;/p&gt;
&lt;p&gt;$$
\tag{16}
R \hat{x} = Q^Tb.
$$&lt;/p&gt;
&lt;p&gt;接著我們試著在 row space 裡找解. 假設 $\hat{x} = R^T \hat{y}$, $\hat{y}\in\mathbb{R}^r$, 那 (16) 可以改寫為&lt;/p&gt;
&lt;p&gt;$$
\tag{17}
RR^T \hat{y} = Q^Tb.
$$&lt;/p&gt;
&lt;p&gt;這樣, 由於 $RR^T\in M_{r\times r}$ 並且可逆, 我們就有 $\hat{y} = (RR^T)^{-1}Q^Tb$. 最後&lt;/p&gt;
&lt;p&gt;$$
\tag{18}
\hat{x} = R^T(RR^T)^{-1}Q^Tb.
$$&lt;/p&gt;
&lt;p&gt;一樣可以很容易驗證 (16) 是滿足的. 也就是說 (18) 會是這個問題的一個解.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; QR 的這整套做法也適用於 $A$ 的 columns 線性獨立的情形. 而且在這情形之下的 $R$ 矩陣會是個可逆方陣, 因此有 $(RR^T)^{-1} = (R^T)^{-1}R^{-1}$. 代入之後得到&lt;/p&gt;
&lt;p&gt;$$
\tag{19}
\hat{x} = R^{-1}Q^Tb.
$$&lt;/p&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;p&gt;我們考慮以下最小平方法問題&lt;/p&gt;
&lt;p&gt;$$
\min_{x\in\mathbb{R}^n}|Ax - b|^2.
$$&lt;/p&gt;
&lt;p&gt;並且我們令最佳解為 $\hat{x}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $A$ 的 columns 線性獨立
$$
\hat{x} = (A^TA)^{-1}A^Tb.
$$
&lt;ul&gt;
&lt;li&gt;如果對 $A$ 做 (reduced) QR, $A=QR$, 並且 $Q^TQ=I_{n\times n}$,
$$
\hat{x} = R^{-1}Q^Tb.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果 $A$ 的 columns 線性相依
&lt;blockquote&gt;
&lt;p&gt;則有無窮多解, 以下是一特解 (並且是所有的解裡面長度最短的).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;對 $A$ 做 (reduced) QR, $A=QR$, 並且 $Q^TQ=I_{r\times r}$,
$$
\hat{x} = R^T(RR^T)^{-1}Q^Tb.
$$&lt;/li&gt;
&lt;li&gt;或是若 $\text{rank}(A)=n$, 則可對 $A^T$ 做 (reduced) QR, $A^T=QR$, 並且 $Q^TQ=I_{m\times m}$,
$$
\hat{x} = Q(R^T)^{-1}b.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Trace, determinant 與 eigenvalue 的關係</title>
      <link>https://teshenglin.github.io/post/2023_la_det_trace_zh/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_la_det_trace_zh/</guid>
      <description>&lt;h2 id=&#34;方陣-a-的行列式等於其特徵值相乘&#34;&gt;方陣 A 的行列式等於其特徵值相乘&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Determinant of A equals to the product of its eigenvalues&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;給定一個 $n\times n$ 的方陣 $A$, 我們定義一個變數為 $\lambda$ 的函數 $P(\lambda)$ 如下:
$$
\tag{1}
P(\lambda) = det(\lambda I-A).
$$
根據行列式的算法我們馬上知道 $P$ 其實就是個 $\lambda$ 的 $n$ 次多項式, 可以寫成
$$
\tag{2}
P(\lambda) = \lambda^n + c_{n-1}\lambda^{n-1}\cdots + c_1\lambda + c_0.
$$
因此這個多項式必有 $n$ 個根 $\lambda_1, \cdots, \lambda_n$ (有可能是實根, 虛根, 或重根, 不過必有 $n$ 個), 所以我們可以將 $P$ 改寫為
$$
\tag{3}
P(\lambda) = (\lambda - \lambda_1)\cdots (\lambda - \lambda_n).
$$&lt;/p&gt;
&lt;p&gt;最後, 我們算一下 $P(0)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由 (1)
$$
\tag{4}
P(0) = det(-A) = (-1)^n det(A).
$$&lt;/li&gt;
&lt;li&gt;由 (3)
$$
\tag{5}
P(0) = (- \lambda_1)\cdots (- \lambda_n) = (-1)^n\lambda_1\cdots\lambda_n.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此, 由 (4) 以及 (5),
$$
det(A) = \lambda_1\cdots\lambda_n.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;方陣-a-的跡httpszhwikipediaorgzh-tw跡等於其特徵值相加&#34;&gt;方陣 A 的
&lt;a href=&#34;https://zh.wikipedia.org/zh-tw/%e8%b7%a1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;跡&lt;/a&gt;等於其特徵值相加&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Trace of A equals to the sum of its eigenvalues&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;給定一個 $n\times n$ 的方陣 $A$:
$$
\tag{6}
A = \begin{bmatrix}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n}\\&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots\\&lt;br&gt;
a_{n1} &amp;amp; \cdots &amp;amp; a_{nn}
\end{bmatrix}.
$$&lt;/p&gt;
&lt;p&gt;因此我們有
$$
\tag{7}
\lambda I - A =
\begin{bmatrix}
\lambda - a_{11} &amp;amp; -a_{12} &amp;amp; \cdots &amp;amp; -a_{1n} \\&lt;br&gt;
-a_{21} &amp;amp; \lambda-a_{22} &amp;amp; \cdots &amp;amp; -a_{2n} \\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\&lt;br&gt;
-a_{n1} &amp;amp; -a_{n2} &amp;amp;  \cdots &amp;amp; \lambda - a_{nn}
\end{bmatrix}.
$$&lt;/p&gt;
&lt;p&gt;接著我們來算 $det(\lambda I - A)$. 我們直接沿著第一行使用降階法 (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Laplace_expansion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cofactor expansion&lt;/a&gt;):
$$
\tag{8}
det(\lambda I - A) =
(\lambda - a_{11})C_{11} +(- a_{21}) C_{21}+ \cdots,
$$
其中
$$
\tag{9}
C_{11} =
\begin{bmatrix}
\lambda-a_{22} &amp;amp; \cdots &amp;amp; -a_{2n}\\&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots\\&lt;br&gt;
-a_{n2} &amp;amp;  \cdots &amp;amp; \lambda - a_{nn}
\end{bmatrix}_{(n-1)\times(n-1)},
$$
以及
$$
\tag{10}
C_{21} =
\begin{bmatrix}
-a_{12} &amp;amp; -a_{13} &amp;amp; \cdots &amp;amp; -a_{1n}\\&lt;br&gt;
-a_{32} &amp;amp; \lambda-a_{33} &amp;amp; \cdots &amp;amp; -a_{3n}\\&lt;br&gt;
\vdots &amp;amp;  \vdots &amp;amp; \ddots &amp;amp; \vdots\\&lt;br&gt;
-a_{n2} &amp;amp; \cdots &amp;amp;  \cdots &amp;amp; \lambda - a_{nn}
\end{bmatrix}_{(n-1)\times(n-1)}.
$$&lt;/p&gt;
&lt;p&gt;簡單觀察可以發現 $C_{21}$ 是個最多 $n-2$ 次的多項式. 並且對所有 $k&amp;gt;1$ 的 $C_{k1}$, 他們全都是最多 $n-2$ 次的多項式. 因此我們就有
$$
\tag{11}
det(\lambda I - A) = (\lambda - a_{11})C_{11} + \hat{Q}_{n-2}(\lambda),
$$
其中 $\hat{Q}_{n-2}(\lambda)$ 是個最多 $n-2$ 次的多項式.&lt;/p&gt;
&lt;p&gt;接著我們對 $C_{11}$ 做展開, 並且用以上的論述一直做下去. 最終我們就會得到
$$
\tag{12}
det(\lambda I - A) = (\lambda - a_{11})(\lambda - a_{22})\cdots (\lambda - a_{nn}) + \tilde{Q}_{n-2}(\lambda),
$$
其中 $\tilde{Q}_{n-2}(\lambda)$ 是個最多 $n-2$ 次的多項式.&lt;/p&gt;
&lt;p&gt;我們接著把 (12) 展開, 得到
$$
\tag{13}
det(\lambda I - A) = \lambda^n - (a_{11} + a_{22} + \cdots + a_{nn})\lambda^{n-1}  + Q_{n-2}(\lambda),
$$
其中 $Q_{n-2}(\lambda)$ 是個最多 $n-2$ 次的多項式.&lt;/p&gt;
&lt;p&gt;另一方面, 我們也可以同樣把 (3) 展開, 得到
$$
\tag{14}
P(\lambda) = \lambda^n - (\lambda_1 + \lambda_2 + \cdots + \lambda_n)\lambda^{n-1}  + \cdots.
$$
因此, 由 (13) 以及 (14),
$$
a_{11} + a_{22} + \cdots + a_{nn} = \lambda_1 + \lambda_2 + \cdots + \lambda_n.
$$&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Power method with Rayleigh Quotient</title>
      <link>https://teshenglin.github.io/post/2023_power_method_3/</link>
      <pubDate>Sat, 29 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_power_method_3/</guid>
      <description>&lt;p&gt;Power 迭代法目錄:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_1&#34;&gt;基本概念&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power iteration; inverse power method; shifted inver power method&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_2&#34;&gt;找第二大的 eigenvalue&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;deflation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_3&#34;&gt;Rayleigh Quotient 迭代及其收斂性&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power method with Rayleigh Quotient&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;假設&#34;&gt;假設&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;$A$ 是一個對稱矩陣&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;演算法-power-method-with-rayleigh-quotient&#34;&gt;演算法: Power method with Rayleigh Quotient&lt;/h2&gt;
&lt;p&gt;Iterate until convergence:
$$
\tag{1}
\begin{align}
\hat{x}^{(k+1)} &amp;amp;= Ax^{(k)}\\&lt;br&gt;
\lambda^{(k+1)} &amp;amp;= x^{(k)T}\hat{x}^{(k+1)}\\&lt;br&gt;
x^{(k+1)} &amp;amp;= \hat{x}^{(k+1)}/|\hat{x}^{(k+1)}|_2
\end{align}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;收斂性證明&#34;&gt;收斂性證明&lt;/h2&gt;
&lt;p&gt;由於 $A$ 是個對稱矩陣, 因此存在一組 orthonormal 的 eigenvectors $\{v_1, \cdots, v_n\}$, 使得 $v_i^Tv_j=0$ if $i\ne j$, and $v_i^Tv_i=0$.&lt;/p&gt;
&lt;p&gt;任意給定初始值 $x^{(0)}\ne 0$, 他可以被 eigenvectors 組出來, 因此我們有
$$
x^{(0)} = \alpha_1 v_1 + \sum^n_{i=2} \alpha_i v_i,
$$
並且
$$
\tag{2}A^kx^{(0)} = \lambda_1^k\left[\alpha_1 v_1 + \sum^n_{i=2} \alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right].
$$
另一方面, 從迭代式 (1) 可以看出我們一直不斷地把得到的向量 normalize, 使其為單位長, 因此我們必定有
$$
x^{(k)} = \frac{A^kx^{(0)}}{|A^kx^{(0)}|_2}.
$$
將 (2) 代入得到
$$
\tag{3}
\begin{align}
x^{(k)}
&amp;amp;= \frac{\lambda_1^k\left[\alpha_1v_1 + \sum^n_{i=2}\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]}{\sqrt{\lambda_1^{2k}\left[\alpha_1v_1 + \sum^n_{i=2}\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]^T\left[\alpha_1v_1 + \sum^n_{i=2}\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]}} \\&lt;br&gt;
&amp;amp;= \frac{\left[\alpha_1v_1 + \sum^n_{i=2}\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]}{\sqrt{\alpha^2_1 + \sum^n_{i=2}\alpha^2_i \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}}}\\&lt;br&gt;
&amp;amp;=\frac{\left[v_1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right) \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]}{\sqrt{1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right)^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}}},
\end{align}
$$
其中我們用到了 $\{v_1, \cdots, v_n\}$ 是 orthonormal basis 這個事實.&lt;/p&gt;
&lt;p&gt;接著我們可以算 eigenvalue, 將 (3) 代入:
$$
\begin{align}
\lambda^{(k+1)} &amp;amp;= x^{(k)T}\hat{x}^{(k+1)} = x^{(k)T}(Ax^{(k)}) \\&lt;br&gt;
&amp;amp;= \frac{\left[v_1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right) \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]^T\left[\lambda_1v_1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right) \lambda_i\left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]}{1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right)^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}} \\&lt;br&gt;
&amp;amp;= \lambda_1
\frac{\left[v_1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right) \left(\frac{\lambda_i}{\lambda_1}\right)^kv_i\right]^T\left[v_1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right) \left(\frac{\lambda_i}{\lambda_1}\right)^{k+1}v_i\right]}{1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right)^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}} \\&lt;br&gt;
&amp;amp;= \lambda_1
\frac{\left[1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right)^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k+1}\right]}{1 + \sum^n_{i=2}\left(\frac{\alpha_i}{\alpha_1}\right)^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}} \\&lt;br&gt;
&amp;amp;= \lambda_1
\frac{\left[1 + O \left(\epsilon^{2k+1}\right)\right]}{1 + O \left(\epsilon^{2k}\right)} \\&lt;br&gt;
&amp;amp;= \lambda_1 + O\left(\epsilon^{2k}\right),
\end{align}
$$
where $\epsilon=|\lambda_2/\lambda_1|$.&lt;/p&gt;
&lt;p&gt;因此這個迭代式, &lt;strong&gt;power method with Rayleigh quotient for symmetric matrix&lt;/strong&gt;, 會是線性收斂, 並且其 rate of convergence 是 $\left(\frac{\lambda_2}{\lambda_1}\right)^2$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-remark&#34;&gt;Final remark&lt;/h2&gt;
&lt;p&gt;若使用基本的 power 迭代, 就是每次將得到得向量單位化, 如以下方式:&lt;/p&gt;
&lt;p&gt;Pseudo code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while diff &amp;gt; Tol
    v = A*u
    lambda = norm(v)
    u = v/lambda
end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果 $A$ 是個對稱矩陣, 會發現這個迭代式其收斂性為 $\left(\frac{\lambda_2}{\lambda_1}\right)^2$.&lt;/p&gt;
&lt;p&gt;這是因爲, 若 $Au=\lambda u$,
$$
\|v\|_2 = \|Au\|_2 = \sqrt{(Au)^T(Au)} = |\lambda|.
$$
利用跟上面類似的推導手法即可證明收斂性為 $\left(\frac{\lambda_2}{\lambda_1}\right)^2$.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Power method - deflation</title>
      <link>https://teshenglin.github.io/post/2023_power_method_2/</link>
      <pubDate>Fri, 28 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_power_method_2/</guid>
      <description>&lt;p&gt;Power 迭代法目錄:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_1&#34;&gt;基本概念&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power iteration; inverse power method; shifted inver power method&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_2&#34;&gt;找第二大的 eigenvalue&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;deflation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_3&#34;&gt;Rayleigh Quotient 迭代及其收斂性&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power method with Rayleigh Quotient&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;deflation&#34;&gt;Deflation&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;對一方陣 $A$, 假設我們以 power iteration 找到了一組 eigenvalue/eigenvector, $\lambda$ and $v$, 使得 $Av = \lambda v$. 那要怎麼找下一組呢?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;1-matrix-tranformation&#34;&gt;1. Matrix tranformation&lt;/h2&gt;
&lt;p&gt;假設能找到一個向量 $x$ 使 $x^T v = 1$, 則我們定義
$$
\tag{1} B = A - \lambda v x^T,
$$
並且以 $B$ 來找 eigenvalue/eigenvector. 那 $A$ 與 $B$ 之間有什麼關係呢?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$B$ 的一組 eigenvalue/eigenvector 為 $0$ 與 $v$, 滿足 $Bv = 0v$.&lt;/li&gt;
&lt;li&gt;$B$ 其他所有的 eigenvalues 都跟 $A$ 一樣.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pf-of-1&#34;&gt;pf of 1:&lt;/h3&gt;
&lt;p&gt;$$
Bv = (A - \lambda v x^T)v = Av - \lambda v (x^Tv) = Av - \lambda v=0.
$$&lt;/p&gt;
&lt;h3 id=&#34;pf-of-2&#34;&gt;pf of 2:&lt;/h3&gt;
&lt;p&gt;假設 $w$ 是 $A$ 的 left-eigenvector, 其 eigenvalue 為 $\lambda_2\ne \lambda$, 滿足 $w^TA = \lambda_2w^T$, 則我們知道 $w^Tv=0$, 並且
$$
w^TB = w^T(A - \lambda v x^T) = w^TA - \lambda (w^Tv)x^T = w^TA = \lambda_2w^T.
$$
因此 $\lambda_2$ 亦為 $B$ 的 eigenvalue.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;所以我們會利用找 $B$ 的 eigenvalue 來找 $A$ 的 eigenvalue.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;那假設我們找到 $B$ 的 eigenvalue 跟 eigenvector 了, 稱之為 $\lambda_2$ 以及 $u_2$, 那我們知道 $\lambda_2$ 也是 $A$ 的 eigenvalue, 不過其相對應的 eigenvector 是誰呢?&lt;/p&gt;
&lt;p&gt;若 $B$ 有 eigenvalues $0,\lambda_2, \cdots, \lambda_n$ 以及 eigenvectors $v, u_2, u_3, \cdots, u_n$, 則
$$
\tag{2} v_i = (\lambda_i-\lambda) u_i + \lambda (x^T u_i) v, \quad i=2,\cdots, n,
$$
並且 $Av_i = \lambda_i v_i$.&lt;/p&gt;
&lt;h3 id=&#34;pf&#34;&gt;pf&lt;/h3&gt;
&lt;p&gt;因為 $u_i$ 是 $B$ 的 eigenvector, 所以
$$
\lambda_i u_i = Bu_i = (A - \lambda v x^T)u_i = Au_i - \lambda(x^Tu_i)v.
$$
因此
$$
Au_i = \lambda_i u_i +\lambda(x^Tu_i)v.
$$
那麼
$$
\begin{align}
Av_i &amp;amp;= (\lambda_i-\lambda) Au_i + \lambda (x^T u_i) Av \\&lt;br&gt;
&amp;amp;= (\lambda_i-\lambda)(\lambda_i u_i +\lambda(x^Tu_i)v) + \lambda^2 (x^T u_i) v\\&lt;br&gt;
&amp;amp;= \lambda_i((\lambda_i-\lambda) u_i + \lambda (x^T u_i) v) \\&lt;br&gt;
&amp;amp;=\lambda_i v_i.
\end{align}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此, 對 (1) 做 power iteration 可以找出下一個 eigenvalue, 並且利用 (2) 可以得到其 eigenvector.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;接下來我們介紹兩種 &lt;em&gt;&lt;strong&gt;看起來似乎可以使用&lt;/strong&gt;&lt;/em&gt; 的 deflation 作法.
不過實際使用起來有一些問題!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-eigen-decomposition&#34;&gt;2. Eigen-decomposition&lt;/h2&gt;
&lt;p&gt;對一個方陣, 如果我們可以找到所有的 eigenvalues 跟 eigenvectors, 並且有以下關係
$$
A V = V\Lambda  \quad \longleftrightarrow \quad A = V\Lambda V^{-1}
$$
並且我們將 $V$, $\Lambda$, $V^{-1}$ 記為
$$
V = [v_1, v_2, \cdots, v_n], \quad \Lambda = \text{diag}\{\lambda_1, \lambda_2, \cdots, \lambda_n\},\quad
V^{-1} = \begin{bmatrix}
w^T_1 \\ w^T_2 \\ \vdots \\ w^T_n\end{bmatrix}.
$$
則
$$
A = \lambda_1 v^1w^T_1 + \lambda_2 v^2w^T_2 + \cdots + \lambda_n v^nw^T_n.
$$&lt;/p&gt;
&lt;p&gt;有趣的是, 若我們定義
$$
\tag{3} B = A-\lambda_1 v^1w^T_1,
$$
則可以很輕易地看出 $B$ 的 eigenvectors 與 $A$ 的完全一樣, 而 eigenvalues 也幾乎完全一樣, 只有 $\lambda_1$ 被移到 $0$ 去了.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此, 若我們知道一個 eigenvalue, 以及其相對應的 left and right eigenvectors, 則可以用 (3) 來做 power iteration 找出下一個 eigenvalue, 並且它的 eigenvector 就會是 $A$ 的 eigenvector.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; 不過實務上 left eigenvector 比較難找, 需要先找出 linear operator 的 adjoint operator (也就是 $A^T$) 才有辦法做矩陣-向量乘法, 因此這個方法除了對稱的情況外幾乎不會被使用.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; 當 $A$ 是 symmetric, 則可以選擇 $v_i$ 使得 $w_i = v_i$, 在這時候我們就可以輕易地使用 (3) 來找下一組了.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-annihilation&#34;&gt;3. Annihilation&lt;/h2&gt;
&lt;p&gt;給定 $A$ 為一個 $n\times n$ 方陣. 我們做以下兩個假設&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;它的 eigenvalues 滿足
$$
|\lambda_1| &amp;gt; |\lambda_2| &amp;gt; |\lambda_3| \cdots |\lambda_n|.
$$&lt;/li&gt;
&lt;li&gt;相對應的 eigenvectors, $\{v_1, v_2, \cdots, v_n\}$, 會構成 $R^n$ 的一組基底.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;observation&#34;&gt;Observation&lt;/h4&gt;
&lt;p&gt;如果初始向量我們選擇 $y$ 使得
$$
\tag{4} y = \sum_{i=2}^n c_iv_i,
$$
也就是沒有 $v_1$ 的分量, 則 power iteration 會找到 $\lambda_2$ 以及 $v_2$.&lt;/p&gt;
&lt;p&gt;當然, 任意給一個向量並無法滿足 (4) 這個條件, 不過若我們定義
$$
\tag{5} y = A x - \lambda_1 x,
$$
則對任意一個向量 $x$, 這個 $y$ 就會滿足 (4).&lt;/p&gt;
&lt;h3 id=&#34;pf-1&#34;&gt;pf&lt;/h3&gt;
&lt;p&gt;Let $x = \sum^{n}_{i=1} c_iv_i$, then&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
y &amp;amp;= A x - \lambda_1 x \\&lt;br&gt;
&amp;amp;= \sum^n_{i=1} (c_iAv_i - c_i\lambda_1v_i) \\&lt;br&gt;
&amp;amp;= \sum^n_{i=2} c_i(\lambda_i-\lambda_1)v_i.
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此, 若我們知道一個 eigenvalue, 則可以任取一個向量 $x$ 再用 (5) 來做出 power iteration 的初始值, 這樣就可以找出下一個 eigenvalue, 並且它的 eigenvector 就會是 $A$ 的 eigenvector.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; 不過實際在做的時候, 由於 $\lambda_1$ 有數值誤差, 因此 (5) 並不會完全滿足 (4), 在設計 power iteration 的時候要小心這件事.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Conjugate gradient method - iterative method</title>
      <link>https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/</guid>
      <description>&lt;p&gt;共軛梯度法 (CG method,  conjugate gradient method) 目錄:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_conjugate_gradient_method_1&#34;&gt;CG method - Direct mehtod&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;直接法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_conjugate_gradient_method_2&#34;&gt;CG method - iterate mehtod&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;迭代法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions:&lt;/h2&gt;
&lt;p&gt;$A\in M_{n\times n}$ is a symmetric positive definite matrix.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A-orthogonal (A-conjugate)
假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \neq u_2$，若這兩個向量滿足
$$
\langle{u_1},A{u_2}\rangle = {u_1}^TA{u_2} = 0,
$$
則稱之為 A-orthogonal (或 A-conjugate).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note :&lt;/strong&gt; We can define
$$
\langle{u_1}, {u_2}\rangle_A = \langle{u_1},A{u_2}\rangle= \langle A{u_1},{u_2}\rangle,
$$
then $\langle \cdot\rangle_A$ is an inner product.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cg-as-an-iterative-method-first-attempt&#34;&gt;CG as an iterative method: First attempt&lt;/h2&gt;
&lt;h3 id=&#34;theorem-1&#34;&gt;Theorem 1:&lt;/h3&gt;
&lt;p&gt;假設對一個矩陣 A，我們可以找到一組 A-orthogonal set ${u_0, \ldots, u_{n-1}}$. 則給定一個初始值 $x_0$, 我們可定義一個迭代法: For $0\le i\le n-1$,
$$
\tag{1} x_{i+1}=x_i+t_{i}u_{i}, \quad
t_{i}=\frac{\langle b-Ax_i,u_{i}\rangle}{\langle u_i,Au_{i}\rangle}.
$$
並且我們可以證明以下兩件事&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義第 $k$ 步的 residual, $r_k = b-Ax_k$, 我們有 $\langle r_k, u_{j}\rangle=0$, for $0\le j&amp;lt; k\le n$.&lt;/li&gt;
&lt;li&gt;$Ax_n=b$, 也就是第 $n$ 次迭代保證得到解.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;pf-of-1&#34;&gt;pf of 1:&lt;/h4&gt;
&lt;p&gt;首先我們可以推 residual 的迭代式
$$
\tag{2} \begin{align}
r_{k+1} &amp;amp;= b - Ax_{k+1} \\
&amp;amp;= b - A(x_k + t_ku_k) \\&lt;br&gt;
&amp;amp;= (b-Ax_k)  - t_kAu_k \\&lt;br&gt;
&amp;amp;= r_k  - t_kAu_k.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;接著, for $0\le j&amp;lt;k-1$, 由於 $u_{k-1}$ 與 $u_j$ 是 A-orthogonal, 因此
$$
\begin{align}
\langle r_{k}, u_{j}\rangle
&amp;amp;= \langle r_{k-1}, u_{j}\rangle-t_{k-1}\langle Au_{k-1}, u_{j}\rangle \\&lt;br&gt;
&amp;amp;= \langle r_{k-1}, u_{j}\rangle.
\end{align}
$$
同樣的理由可以一直使用得到
$$
\langle r_{k}, u_{j}\rangle
= \langle r_{k-1}, u_{j}\rangle
=\cdots
= \langle r_{j+1}, u_{j}\rangle.
$$&lt;/p&gt;
&lt;p&gt;此外, 若 $j=k-1$, 則我們直接有 $\langle r_{k}, u_{j}\rangle  = \langle r_{j+1}, u_j\rangle$.&lt;/p&gt;
&lt;p&gt;接著再用 (2) 一次, 並且用 (1) 裡 $t_j$ 的定義我們得到&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\langle r_{j+1}, u_j\rangle
&amp;amp;= \langle r_j, u_j\rangle-t_j\langle Au_j, u_j\rangle \\
&amp;amp;= \langle r_j, u_j\rangle-\frac{\langle r_j,u_j\rangle}{\langle u_j,Au_j\rangle}\langle Au_j, u_j\rangle\\&lt;br&gt;
&amp;amp;=0.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;因此, for $0\le j&amp;lt;k$, 我們有
$$
\langle r_{k}, u_{j}\rangle =0.
$$&lt;/p&gt;
&lt;h4 id=&#34;pf-of-2&#34;&gt;pf of 2:&lt;/h4&gt;
&lt;p&gt;根據 1 我們有 $\langle r_n, u_{j}\rangle=0$ for $0\le j&amp;lt; n-1$, 也就是
$$
\langle b - Ax_n, u_{j}\rangle=0, \quad 0\le j&amp;lt; n-1.
$$
因為 $\{u_i\}^{n-1}_{i=0}$ span $\mathbb{R}^n$, 所以 $b - Ax_n$ 必為一個零向量, 也就是說 $Ax_n=b$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gram-schimidt-process-to-find-a-orthogonal-set&#34;&gt;Gram-Schimidt process to find A-orthogonal set&lt;/h2&gt;
&lt;p&gt;不過這方法建立在一個重要假設之下, 就是一開始我們可以找到一組 A-orthogonal set $\{u_0, \ldots, u_{n-1}\}$. 不過在實際解問題是這顯然是不容易做到的, 因此我們會需要一步一步地將 $u_i$ 求出來.&lt;/p&gt;
&lt;p&gt;這裡我們可以利用兩件事&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The residual vector $r_k = b - Ax_k$.
&lt;ul&gt;
&lt;li&gt;每次迭代都會產生出一組新的 residual vector, 並且我們知道 residual 是下降最快的方向, 因此我們可以將下個搜尋方向設為 $r_k$, 只不過這並不滿足 A-orthogonal 的條件.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gram-Schimidt process
&lt;ul&gt;
&lt;li&gt;線性代數裡告訴我們, 可以用 Gram-Schimidt process 將一組向量正交化. 因此我們就利用這方法來造出一組 A-orthogonal 的 basis.&lt;/li&gt;
&lt;li&gt;若已經有 $\{u_0, \cdots, u_k\}$, 並且我們得到 $r_{k+1}$, 則 Gram-Schimidt process 告訴我們可以定義 $u_{k+1}$ 為
$$
u_{k+1} = r_{k+1} - \frac{\langle r_{k+1}, Au_0\rangle}{\langle u_0, Au_0\rangle}u_0 - \cdots - \frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle}u_k,
$$
並且 $u_{k+1}$ 會跟 $u_0, \cdots, u_k$ 是 A-orthogonal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cg-as-iterative-method-second-attempt&#34;&gt;CG as iterative method: Second attempt&lt;/h2&gt;
&lt;p&gt;給一個初始猜測 $x_0$, 我們可以算 $r_0 = b-Ax_0$, 並且我們定義第一個方向為 $u_0 = r_0$.&lt;/p&gt;
&lt;p&gt;接著我們可以往下做, for $k\ge 0$,
$$
\begin{align}
t_{k} &amp;amp;=\frac{\langle b-Ax_k,u_{k}\rangle}{\langle u_k,Au_{k}\rangle}, \\&lt;br&gt;
x_{k+1} &amp;amp;= x_k + t_k u_k, \\&lt;br&gt;
r_{k+1} &amp;amp;= r_k - t_kAu_k, \\&lt;br&gt;
u_{k+1} &amp;amp;= r_{k+1} - \frac{\langle r_{k+1}, Au_0\rangle}{\langle u_0, Au_0\rangle}u_0 - \cdots - \frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle}u_k.
\end{align}
$$
這樣保證在第 $n$ 步得到解.&lt;/p&gt;
&lt;p&gt;不過一般而言這是個迭代法, 我們在過程中會看 residual 的大小, $\|r_{k+1}\|^2_2$, 如果 residual 夠小就會停止整個迭代, 不需要真的做到第 $n$ 步.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark :&lt;/strong&gt;
這樣子做有個很明顯的缺點, 就是為了求出 A-orthogonal set 我們需要把所有方向 $\{u_i\}$ 都一直記著. 這樣當問題維度非常大時會需要耗費大量記憶體在存這些方向.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;不過我們可以再觀察一下, 若使用 Gram-Schimidt process, 則這些 $\{u\}$ 都是從 $\{r\}$ 得來的, 因此這兩個所組出來的空間應該是一樣的, 也就是
$$
\text{span}\{u_0,\cdots, u_k\} = \text{span}\{r_0,\cdots, r_k\}.
$$
我們若定義 $V_k = \text{span}\{u_0,\cdots, u_k\}$, 則由 Theorem 1 我們知道,
$$
\langle r_{k+1}, u_{j}\rangle=0, \quad 0\le j\le k,
$$&lt;/p&gt;
&lt;p&gt;也就是 $r_{k+1}$ 這個向量會垂直於 $V_k$ 這個空間, 因此我們也有&lt;/p&gt;
&lt;p&gt;$$
\tag{3} \langle r_{k+1}, r_{j}\rangle=0, \quad 0\le j\le k.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;事實上, 實作 second attempt 後會發現, 要算 $u_{k+1}$ 其實只需要他的前一項, $u_{k}$, 即可, 也就是
$$
\tag{4} u_{k+1} = r_{k+1}  + \beta_k u_k, \quad \beta_k =  -\frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle},
$$
因為其他項都自動為零了! 接著我們就來證明這件事.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;lemma-&#34;&gt;&lt;em&gt;&lt;strong&gt;Lemma :&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;$$
\langle r_{k+1}, Au_j\rangle = 0, \quad 0\le j\le k-1.
$$&lt;/p&gt;
&lt;h4 id=&#34;pf&#34;&gt;pf:&lt;/h4&gt;
&lt;p&gt;根據 (2), 我們可以得到 $Au_j = \frac{r_j - r_{j+1}}{t_j}$. 因此
$$
\tag{5} \langle r_{k+1}, Au_j\rangle = \frac{1}{t_j}\langle r_{k+1}, r_j - r_{j+1}\rangle= \frac{\langle r_{k+1}, r_j \rangle - \langle r_{k+1}, r_{j+1} \rangle}{t_j}.
$$&lt;/p&gt;
&lt;p&gt;接著我們利用 (3), 代入 (5) 我們就得到 $\langle r_{k+1}, Au_j\rangle=0$ for $0\le j\le k-1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark :&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;將 $j=k$ 代入 (5) 得到
$$
\tag{6}\langle r_{k+1}, Au_k\rangle =  \frac{\langle r_{k+1}, r_k \rangle - \langle r_{k+1}, r_{k+1} \rangle}{t_k} = \frac{ - \langle r_{k+1}, r_{k+1} \rangle}{t_k}.
$$&lt;/p&gt;
&lt;p&gt;利用 (1) 與 (4) $t_k$ 及 $u_k$ 的定義我們可以推得
$$
\tag{7}
\begin{align}
t_k &amp;amp;= \frac{\langle r_{k}, u_{k} \rangle}{\langle u_{k}, Au_{k} \rangle} = \frac{\langle r_{k}, r_{k} + \beta_{k-1}u_{k-1} \rangle}{\langle u_{k}, Au_{k} \rangle} \\&lt;br&gt;
&amp;amp;= \frac{\langle r_{k}, r_{k}  \rangle}{\langle u_{k}, Au_{k} \rangle} + \frac{\langle r_{k}, \beta_{k-1}u_{k-1} \rangle}{\langle u_{k}, Au_{k} \rangle} \\&lt;br&gt;
&amp;amp;= \frac{\langle r_{k}, r_{k}  \rangle}{\langle u_{k}, Au_{k} \rangle}.
\end{align}
$$
最後一個等號我們用到了 Theorem 1 的結果. 因此 $t_k$ 可改為以 (7) 來做.&lt;/p&gt;
&lt;p&gt;將 (7) 改寫一下我們得到
$$
\tag{8}\langle u_{k}, Au_{k} \rangle = \frac{\langle r_{k}, r_{k}  \rangle}{t_k}.
$$&lt;/p&gt;
&lt;p&gt;最後, 我們可以將 $\beta_k$ 重新計算一下, 利用 (6) 跟 (8) 得到
$$
\tag{9} \beta_k = -\frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle} = \frac{\langle r_{k+1}, r_{k+1}\rangle}{\langle r_{k}, r_{k}\rangle}.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cg-as-iterative-method-the-algorithm&#34;&gt;CG as iterative method: The algorithm&lt;/h2&gt;
&lt;p&gt;給一個初始猜測 $x_0$, 我們可以算 $r_b = b-Ax_0$, 並且我們定義第一個方向為 $u_0 = r_0$.&lt;/p&gt;
&lt;p&gt;接著我們可以往下做, for $k\ge 0$,
$$
\begin{align}
t_{k} &amp;amp;=\frac{\langle r_k,r_{k}\rangle}{\langle u_k,Au_{k}\rangle},\\&lt;br&gt;
x_{k+1} &amp;amp;= x_k + t_k u_k, \\&lt;br&gt;
r_{k+1} &amp;amp;= r_k  - t_kAu_k, \\&lt;br&gt;
\beta_k &amp;amp;= \frac{\langle r_{k+1}, r_{k+1}\rangle}{\langle r_{k}, r_{k}\rangle},\\&lt;br&gt;
u_{k+1} &amp;amp;= r_{k+1} + \beta_k u_k.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;這樣保證在第 $n$ 步得到解, 而且每次迭代都只需要 &lt;em&gt;&lt;strong&gt;一個&lt;/strong&gt;&lt;/em&gt; 矩陣向量乘法.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不過一般而言這是個迭代法, 我們在過程中會看 residual 的大小, $\|r_{k+1}\|^2_2=\langle r_{k+1}, r_{k+1}\rangle$, 如果 residual 夠小就會停止整個迭代, 不需要真的做到第 $n$ 步. 而且這個量是在過程中會被計算出來的, 所以不需花費額外力氣.&lt;/li&gt;
&lt;li&gt;Residual $r_{k+1}=r_k-t_kAu_k$ 這個式子可能會在迭代的過程由於誤差進來而越來越不準, 所以每隔一段時間要以原本公式重新更新一次, $r_{k+1} = b - Ax_{k+1}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Power method</title>
      <link>https://teshenglin.github.io/post/2023_power_method_1/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_power_method_1/</guid>
      <description>&lt;p&gt;Power 迭代法目錄:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_1&#34;&gt;基本概念&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power iteration; inverse power method; shifted inver power method&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_2&#34;&gt;找第二大的 eigenvalue&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;deflation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_3&#34;&gt;Rayleigh Quotient 迭代及其收斂性&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Power method with Rayleigh Quotient&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;基本-power-method&#34;&gt;基本 Power method&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;求一個方陣最大(in magnitude) 的 eigenvalue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;給定 $A$ 為一個 $n\times n$ 方陣. 我們做以下兩個假設&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;它的 eigenvalues 滿足
$$
|\lambda_1| &amp;gt; |\lambda_2| \ge |\lambda_3| \cdots |\lambda_n|.
$$&lt;/li&gt;
&lt;li&gt;相對應的 eigenvectors, $\{v_1, , v_2, , \cdots, v_n\}$, 會構成 $R^n$ 的一組基底.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;則若我們&lt;em&gt;&lt;strong&gt;任意取&lt;/strong&gt;&lt;/em&gt;一個 $n\times 1$ 向量 $u$, 它能被寫成 (根據假設 2)
$$
u =\sum^{n}_{i=1} c_iv_i.
$$&lt;/p&gt;
&lt;p&gt;我們將兩邊乘以 $A$ 得到&lt;/p&gt;
&lt;p&gt;$$
Au = \sum^{n}_{i=1} c_i\lambda_i v_i,
$$&lt;/p&gt;
&lt;p&gt;並且如果我們一直乘以 $A$, 共乘 $k$ 次, 則有
$$
\tag{1} A^ku =\sum^{n}_{i=1} c_i\lambda^k_i v_i.
$$&lt;/p&gt;
&lt;p&gt;因為 $|\lambda_1|$ 比其他的都大 (根據假設 1), 我們可以得到當 $k$ 夠大時
$$
\tag{2} \frac{1}{\lambda^k_1}A^ku = c_1 v_1 + \sum^{n}_{i=2} c_i\left(\frac{\lambda_i}{\lambda_1}\right)^k v_i \to c_1 v_1.
$$&lt;/p&gt;
&lt;p&gt;因此我們知道, 如果我們一開始隨機選取一個向量, 並且將之以 $A$ 矩陣一直乘它, 乘出來的這個向量應該會越來越接近第一個 eigenvector, 藉此我們也可以得到最大的 eigenvalue.&lt;/p&gt;
&lt;p&gt;另外我們也知道, 收斂速度 (rate of convergence) 會是 $\frac{\lambda_2}{\lambda_1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;可以看到 (2) 式有個特別的地方是等號左邊我們把他除以 $\lambda^k_1$, 這樣等號右邊就會趨近於 $c_1v_1$.&lt;/li&gt;
&lt;li&gt;如果真的照 (1) 式的做法, 任意取一個 $u$ 然後矩陣一直乘會發生什麼事呢?
&lt;blockquote&gt;
&lt;p&gt;如果 $\lambda&amp;gt;1$ 那 $\lambda^k\to\infty$, 如果 $\lambda&amp;lt;1$ 那 $\lambda^k\to 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;因此, 實際執行 (1) 是不可行的. 而 (2) 也不可行, 因為 $\lambda_1$ 就是我們要找的東西, 所以無法除以 $\lambda_1^k$.&lt;/li&gt;
&lt;li&gt;實際執行 power iteration 時, 會將每次迭代所得到的向量 normalize, 這樣就可以避免爆掉或趨近於零的狀況. 而最簡單的 normalization 就是讓每次得到的向量為單位長.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;作法&#34;&gt;作法:&lt;/h4&gt;
&lt;p&gt;Input: A
Output: (取絕對值後)最大的 eigenvalue $\lambda_1$ 以及 $v_1$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;隨機選取一個向量 $u$, 並使 $\|u\|=1$&lt;/li&gt;
&lt;li&gt;計算 $v = Au$, 並令 $\lambda=\|v\|$&lt;/li&gt;
&lt;li&gt;另 $u=v/\lambda$ 並重複步驟 2 直到 $\lambda$ 收斂為止&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;Pseudo code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: matrix A
Output: lambda1, u

u = random(size(A,1),1)
u = u/norm(u)

Tol=10**(-10)
max_iter = 1000

lambda0=1
diff=1

iter=0

while diff &amp;gt; Tol
    v = A*u
    lambda1 = norm(v)
    u = v/lambda1
    
    diff = abs(lambda1-lambda0)    
    lambda0 = lambda1
    
    iter =iter+1
    if iter &amp;gt; max_iter || diff &amp;lt; Tol
        break
    end
end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; 不過這樣得到的 &lt;code&gt;lambda1&lt;/code&gt; 會是 $|\lambda_1|$, 要拿到 $\lambda_1$ 需要再乘一次矩陣.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;事實上我們的目標只是不要讓向量爆掉或趨近於零, 所以可以簡單的把某個位置的值固定住, 比如說我們強迫 $v_1$ 的第一個位置等於 $1$, 這樣就沒問題了.&lt;/p&gt;
&lt;p&gt;Pseudo code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while diff &amp;gt; Tol
    v = A*u
    lambda1 = v(1)
    u = v/lambda1
end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;當然, 如果 $v_1$ 這個 eigenvector 本來他第一個位置就是 $0$ 那這招就不行了. 不過我們可以固定其他位置啊!&lt;/li&gt;
&lt;li&gt;理論上, 由於 $v_1$ 不是零向量, 因此一定有個位置非零, 可以被固定住. 不過通常就隨機選個位置固定住就好.&lt;/li&gt;
&lt;li&gt;這樣做的好處有兩個:
a. 取向量某一個位置的值遠比算向量的 norm 要快很多.
b. 這樣拿到的 eigenvalue 就是 $\lambda_1$, 沒有絕對值.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;若要找第二大的 eigenvalue 可以利用 deflation: 
&lt;a href=&#34;https://teshenglin.github.io/post/2023_power_method_2&#34;&gt;Power method with deflation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;inverse-power-method&#34;&gt;Inverse power method&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;求一個方陣最小(in magnitude) 的 eigenvalue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;observation&#34;&gt;Observation:&lt;/h4&gt;
&lt;p&gt;如果 $\lambda$ 滿足 $Av = \lambda v$, 則 $A^{-1}v = \frac{1}{\lambda} v$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此, 如果我們想要找一個矩陣 $A$ 它(絕對值)最小的 eigenvalue, $\lambda_n$, 那我們就需要對 $A^{-1}$ 做 power method. 也就是計算 $(A^{-1})^k u$. 這樣我們可以找到 $\frac{1}{\lambda_n}$, 將之反過來就得到最小的 eigenvalue.&lt;/p&gt;
&lt;p&gt;這裡需注意兩件事:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;必須先確定最小的 eigenvalue 不是 $0$, 也就是這矩陣 non-singular, 不然 inverse power method 也無法做.&lt;/li&gt;
&lt;li&gt;$A^{-1}u$ 是解線性系統, 不是乘以 $A^{-1}$ 矩陣.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Inverse power method 的 rate of convergence 是 $\frac{\lambda_{n}}{\lambda_{n-1}}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;shift-inverse-power-method&#34;&gt;Shift-inverse power method&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;求一個方陣最靠近某個數字 $c$ 的 eigenvalue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;observation-1&#34;&gt;Observation:&lt;/h4&gt;
&lt;p&gt;如果 $\lambda$ 滿足 $Av = \lambda v$, 則 $(A-cI)v = (\lambda-c) v$. 因此
$$
(A-cI)^{-1}v = \frac{1}{\lambda-c} v.
$$
也就是說, $(A-cI)^{-1}$ 這矩陣最大的 eigenvalue, 會是使得 $\frac{1}{\lambda-c}$ 最大的那個, 我們可以據此反推 $A$ 矩陣其最靠近 $c$ 的那個 eigenvalue.&lt;/p&gt;
&lt;p&gt;因此, 若我們想要找矩陣 $A$ 它最靠近某個數字 $c$ 的 eigenvalue, 那我們就需要對 $(A-cI)^{-1}$ 做 power method. 也就是計算 $((A-cI)^{-1})^k u$.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Conjugate gradient method - direct method</title>
      <link>https://teshenglin.github.io/post/2023_conjugate_gradient_method_1/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_conjugate_gradient_method_1/</guid>
      <description>&lt;p&gt;共軛梯度法 (CG method,  conjugate gradient method) 目錄:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_conjugate_gradient_method_1&#34;&gt;CG method - Direct mehtod&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;直接法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_conjugate_gradient_method_2&#34;&gt;CG method - iterate mehtod&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;迭代法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions:&lt;/h2&gt;
&lt;p&gt;$A\in M_{n\times n}$ is a symmetric positive definite matrix.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A-orthogonal (A-conjugate)
假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \neq u_2$，若這兩個向量滿足
$$
\langle{u_1},A{u_2}\rangle = {u_1}^TA{u_2} = 0,
$$
則稱之為 A-orthogonal (或 A-conjugate).&lt;/li&gt;
&lt;li&gt;A-orthonormal
假設有兩個向量 $u_1$ 跟 $u_2$ 為 A-orthogonal, 並且 $\langle{u_i}, {u_i}\rangle_A = 1$ for $1\le i\le 2$, 則稱之為 A-orthonormal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recall :&lt;/strong&gt; ${u_1}$ and ${u_2}$ are orthogonal if ${u_1}^T{u_2} = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note :&lt;/strong&gt; We can define
$$
\langle{u_1}, {u_2}\rangle_A = \langle{u_1},A{u_2}\rangle= \langle A{u_1},{u_2}\rangle,
$$
then $\langle \cdot\rangle_A$ is an inner product.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;lemma&#34;&gt;Lemma&lt;/h3&gt;
&lt;p&gt;如果 ${u_0, \cdots, u_k}$ 是個 A-orthogonal set, 則 ${u_0, \cdots, u_k}$ 也是一個 linearly independent set.&lt;/p&gt;
&lt;h4 id=&#34;pf&#34;&gt;pf:&lt;/h4&gt;
&lt;p&gt;假設 $c_0 u_0 + \cdots c_ku_k =0$.&lt;/p&gt;
&lt;p&gt;將此方程兩邊同時乘以 $u^T_j A$, 由於 $u_j^TAu_i=0$ for $i\ne j$, 因此得到 $c_j u^T_j Au_j = 0$. 此外, 由於 $A$ 是正定矩陣, $u^T_j Au_j &amp;gt; 0$, 因此 $c_j=0$.&lt;/p&gt;
&lt;p&gt;由於以上論述對所有 $j$ 都對, 因此得到 $c_i=0$ $\forall i$, 因此 ${u_0, \cdots, u_k}$ 為線性獨立.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note :&lt;/strong&gt;
如果 ${u_0, \cdots, u_{n-1}}\subset\mathbb{R}^n$ 是個 A-orthogonal set, 則他們也是 $\mathbb{R}^n$ 的一組 basis.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cg-as-a-direct-method&#34;&gt;CG as a direct method&lt;/h2&gt;
&lt;h3 id=&#34;theorem&#34;&gt;Theorem:&lt;/h3&gt;
&lt;p&gt;假設對一個矩陣 A，我們可以找到一組 A-orthogonal set ${u_0, \ldots, u_{n-1}}$, 則線性系統 $Ax=b$ 的解為
$$
x = \sum^{n-1}_{j=0}\frac{u^T_j b}{u^T_jAu_j} u_j.
$$&lt;/p&gt;
&lt;h4 id=&#34;pf-1&#34;&gt;pf:&lt;/h4&gt;
&lt;p&gt;假設 $x = \sum^{n-1}_{i=0} c_i u_i$, 則&lt;/p&gt;
&lt;p&gt;$$
Ax = \sum^{n-1}_{i=0} c_i Au_i.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;將之代入 $b = Ax$ 並將兩側同時乘以 $u^T_j$ 得到
$$
u^T_j b = \sum^{n-1}_{i=0} c_i u^T_jAu_i = c_ju^T_jAu_j.
$$
因此
$$
c_j = \frac{u^T_j b}{u^T_jAu_j}.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;實作上我們需要以迭代來尋找 $A$-orthogonal set, 並以迭代法來解線性系統.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Conjugate_gradient_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conjugate gradient method - wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;迭代法推導請見
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://teshenglin.github.io/post/2023_conjugate_gradient_method_2&#34;&gt;Conjugate gradient method - iterative method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Diffusion maps</title>
      <link>https://teshenglin.github.io/post/2021_diffusion_maps/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2021_diffusion_maps/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;擴散映射, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Diffusion_map&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion maps&lt;/a&gt; (以下簡稱 DM), 是個資料分析, 流型學習或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要介紹以 &lt;code&gt;julia&lt;/code&gt; 來做 diffusion maps 降維.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;algorithm---diffusion-maps-embeding&#34;&gt;Algorithm - diffusion maps embeding&lt;/h2&gt;
&lt;p&gt;先簡單介紹一下作法.&lt;/p&gt;
&lt;p&gt;假設我們有 $n$ 筆 $d$ 維的資料,
$$
\{x_1, x_2, \cdots, x_n\} \in R^d.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;1-affinity-matrix-k&#34;&gt;1. Affinity matrix $K$&lt;/h4&gt;
&lt;p&gt;我們需要先定一個 $K$ 矩陣, $K_{ij}=k(x_{i},x_{j})$, 一般而言我們使用 Guassian kernel
$$
k(x,y) = e^{-\frac{\lVert x-y \rVert^2}{\sigma^2}},
$$
其中 $\sigma$ 是個常數. 這樣造出來的 $K$ 矩陣會是個 $n\times n$ 對稱半正定的矩陣.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-normalized-affinity-matrix-q&#34;&gt;2. Normalized affinity matrix $Q$&lt;/h4&gt;
&lt;p&gt;接著我們要定 diffusion matrix $P$, 其定義為
$$
P=D^{-1}K,
$$
其中 $D$ 是個只有對角線有值的矩陣, 其元素為相對應 $K$ 矩陣的 rowsum, $D_{ii} = \sum^n_{j=1} K_{ij}$.
因此可以知道 $P$ 矩陣其實就是將 $K$ 矩陣的每個 row 做 normalize 的動作, 使其 rowsum 等於 $1$.&lt;/p&gt;
&lt;p&gt;$P$ 這矩陣可看成是個機率矩陣, 其第 $i$ 個 row 表示從 $x_{i}$ 這個點跳到其他點的機率分佈.&lt;/p&gt;
&lt;p&gt;接著我們考慮 $Q$ 矩陣, 定義為
$$
Q=D^{-\frac{1}{2}}KD^{-\frac{1}{2}}.
$$
我們可以很輕易發現 $P$ 以及 $Q$ 兩個矩陣有完全相同的 eigenvalues, 而 $P$ 的 eigenvectors 是
$$
v = D^{-1/2}v_Q,
$$
其中 $v_Q$ 是 $Q$ 的 eigenvector.&lt;/p&gt;
&lt;p&gt;另一個重要觀察是 $Q$ 矩陣跟 $K$ 一樣是對稱半正定的矩陣, 因此它的 eigenvalues 都非負, 而且他的 eigenvalues 跟 singular values 會完全一模一樣.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-求出-p-矩陣的-eigenvalues-跟-eigenvectors&#34;&gt;3. 求出 $P$ 矩陣的 eigenvalues 跟 eigenvectors&lt;/h4&gt;
&lt;p&gt;先將 $Q$ 的 eigenvalues 跟 eigenvectors 都找出來, 接著 $P$ 的 eigenvalues 跟 eigenvectors 就依照上面的公式可以輕易得到.&lt;/p&gt;
&lt;p&gt;我們令 $P$ 的 eigenvalues 跟 eigenvectors 分別為 $\lambda_i$ 跟 $\psi_i$, $1\le i\le n$.&lt;/p&gt;
&lt;p&gt;需要注意的是, $\lambda_1=1$ 並且 $\psi_1$ 是一個常數向量, 因為我們要拿的是從第二個開始.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-define-diffusion-map-y&#34;&gt;4. Define diffusion map $Y$&lt;/h4&gt;
&lt;p&gt;接著我們定義 $Y$ 矩陣為
$$
Y = \left[\lambda_2\psi_2, \lambda_3\psi_3, \cdots, \lambda_n\psi_n\right],
$$
這是個 $n\times (n-1)$ 的矩陣.&lt;/p&gt;
&lt;p&gt;如果我們想要將原始資料投射到 $k$ 維, $k \le (n-1)$, 那我們就只要到第 $k+1$ 個 eigenvector 就好. 比如說要投影到三維, 我們只需要取
$$
Y = \left[\lambda_2\psi_2, \lambda_3\psi_3, \lambda_4\psi_4\right].
$$
而 diffusion maps embedding 的每個點就是這個 $Y$ 矩陣的 row, 也就是 $y_i = Y(i,:)$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in &lt;code&gt;julia&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;接著我們就可以來看要怎樣以 &lt;code&gt;julia&lt;/code&gt; 來做 diffusion maps.&lt;/p&gt;
&lt;p&gt;我們需要以下幾個 packages:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Plots
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Distances
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; LinearAlgebra
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;以下例子為一個 spiral curve, 我們想要看經過 diffusion maps 的投射到二或三維後會長什麼樣子.&lt;/p&gt;
&lt;p&gt;我們先把 spiral curve 上面的點造出來 (取 $300$ 個點)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# generating the data - a spiral curve&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# n: number of sampling&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;;

theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,stop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi,length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n);
r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,stop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n);
&lt;span style=&#34;color:#75715e&#34;&gt;# x and y-coordinates&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;cos&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(theta);
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;sin&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(theta);
&lt;span style=&#34;color:#75715e&#34;&gt;# X is a n-by-2 data matrix&lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [x y];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;畫出來看看原始 data 長怎樣. 我們照順序將點標為藍色, 紅色以及綠色.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;plot(reshape(X[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(X[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), aspect_ratio&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:equal&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_01.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h4 id=&#34;1-affinity-matrix-k-1&#34;&gt;1. Affinity matrix $K$&lt;/h4&gt;
&lt;p&gt;先來造出 $K$ 矩陣:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;E &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pairwise(Euclidean(), X, dims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
sigma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;;
K &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; exp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(E&lt;span style=&#34;color:#f92672&#34;&gt;.^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;./&lt;/span&gt;(sigma&lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h4 id=&#34;2-normalized-affinity-matrix-q-1&#34;&gt;2. Normalized affinity matrix $Q$&lt;/h4&gt;
&lt;p&gt;接著造出 $Q$ 矩陣, 並求出其 eigenvalues 跟 eigenvectors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q = zeros(n,n);
d_sq = zeros(n);
for ii = 1:n
    d_sq[ii] = sqrt(sum(K[ii,:]));
end
for ii = 1:n
    for jj = 1:n
        Q[ii,jj] = K[ii,jj]/(d_sq[ii]*d_sq[jj]);
    end
end
U,S,V = svd(Q);
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h4 id=&#34;3-求出-p-矩陣的-eigenvalues-跟-eigenvectors-1&#34;&gt;3. 求出 $P$ 矩陣的 eigenvalues 跟 eigenvectors&lt;/h4&gt;
&lt;p&gt;$P$ 矩陣的 eigenvalues 跟 $Q$ 一樣, eigenvectors 也可以依公式得到:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ii &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; n
    V[ii,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V[ii,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;d_sq[ii];
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;將 eigenvalues 畫出來看看. 左圖是第 $2$ 到第 $11$ 個, 右圖則是以 semilogy 畫出全部的 eigenvalues. 可以看出 eigenvalues 遞減的非常快, 所以其實 embedding 不需要取到全部 $(n-1)$ 維.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;p1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; scatter(S[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;], title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;eigenvalues 2:11&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false);
p2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(log&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(S), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;eigenvalues in log&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false);
plot(p1, p2, layout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:png&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_02.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h4 id=&#34;4-define-diffusion-map-y-1&#34;&gt;4. Define diffusion map $Y$&lt;/h4&gt;
&lt;p&gt;最後我們定義 embedding $Y$:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;Y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zeros(n,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;);
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ii &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
    Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,ii] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,ii&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;S[ii&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我們知道 $Y$ 的每個 row 就是這個 embedding 的座標.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們將 embedding 畫出來看看. 左圖是 embed 到二維, 右圖則是 embed 到三維.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;p1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2D&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
p2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3D&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
plot(p1, p2, layout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:png&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我們一樣照順序將點標為藍色, 紅色以及綠色. 有趣的是, 我們發現這個 embedding 將整個 spiral curve &lt;strong&gt;打開了&lt;/strong&gt;. 因此, 經由 diffusion maps 投射之後我們比較榮以可以看出點資料真正在整個曲線上彼此間距離的遠近.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_03.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;Diffusion maps 也常被拿來搭配 
&lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k-means&lt;/a&gt; 做成分群演算法, 算是 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Spectral_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spectral clustering&lt;/a&gt; 的其中一種.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diffusion maps 的 &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt; 以及 &lt;code&gt;matlab&lt;/code&gt; 程式都可以在這裡找到: 
&lt;a href=&#34;https://github.com/teshenglin/diffusion_maps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github - teshenglin/diffusion_maps&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析 - 2</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們補充一下主成分分析裡的證明部分.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p.
$$
假設想要投影到 $k$ 維, $k\le p$, 數學上來說就是想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;要求極值就是要找微分等於零的解, 由於這式子是 convex, 所以保證找到唯一解而且是最小的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;所以以下做法就是對每個變數做偏微分, 並求出偏微分等於零的解. 這樣就把最佳解找出來了!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;1-首先看-mu&#34;&gt;1. 首先看 $\mu$&lt;/h4&gt;
&lt;p&gt;對 $\mu$ 做偏微分並且利用 $\sum^n_{i=1} \beta_i=\vec{0}$ 我們可以得到
$$
\partial_{\mu} E = -2 \left(\sum_i x_i - n\mu\right)=0.
$$
因此, 最佳的 $\mu$ 是
$$
\mu = \frac{1}{n}\sum_{i=1}^n x_i.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-接著找-beta_i&#34;&gt;2. 接著找 $\beta_i$&lt;/h4&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 我們有
$$
E = \sum_{i=1}^n \|y_i - U\beta_i\|^2.
$$
接著對 $\beta_i$ 微分得到
$$
\partial_{\beta_i} E = 2\left(\beta_i - U^Ty_i\right)=0.
$$
因此可以得到
$$
\beta_i = U^T y_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;,
$$
其中 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-最後找-u&#34;&gt;3. 最後找 $U$&lt;/h4&gt;
&lt;p&gt;代入最佳的 $\beta_i$ 後我們又可將原本的 $E$ 改寫為
$$
E = \sum_{i=1}^n \|y_i - UU^Ty_i\|^2 = \|Y - UU^TY\|^2_F,
$$
其中 $Y=[y_1, \cdots, y_n]$, 而下標 $F$ 代表矩陣的 Frobenius norm.&lt;/p&gt;
&lt;p&gt;我們先定 $P = UU^T$, 則有 $P^T=P$, $P^2=P$. 接著我們改寫 $E$ 為
$$
E = \|Y - PY\|^2_F = trace\left[(Y-PY)^T(Y-PY)\right] = trace\left(Y^TY-Y^TPY\right).
$$
由於 $Y$ 不會變, 因此求 $E$ 的最小值變成求 $trace\left(-Y^TPY\right)$ 的最小值, 也就是求 $trace\left(Y^TPY\right)$ 的最大值, 換回來得到是求 $trace\left(Y^TUU^TY\right)$ 的最大值.&lt;/p&gt;
&lt;p&gt;接著我們用線性代數裡一個定理, $trace(AB)=trace(BA)$, 將原式轉換成求 $trace\left(U^TYY^TU\right)$ 的最大值, 最後得到
$$
\arg\min_U E = \arg\max_U trace\left(U^T\Sigma U\right),
$$
其中 $\Sigma=YY^T$ 也就是原資料的共變異數矩陣. 上式是個非常重要的式子, 它告訴我們以下兩件事是等價的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使得原始資料與投影後的資料之間的&lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;使得資料有&lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最後, 線性代數告訴我們等號右邊這問題的解就是 $\Sigma$ 最大的 $k$ 個 eigenvalues 其相對應的 eigenvectors, 收集起來得到 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以上使用的線性代數結果, 其證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/252272/is-trace-invariant-under-cyclic-permutation-with-rectangular-matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tr(AB)=tr(BA) proof&lt;/a&gt;, 以及 
&lt;a href=&#34;https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;maximize v^T Av&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;總結一下最佳解如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 是原始資料的平均
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將平移後的資料投影到子空間中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Multidimensional scaling</title>
      <link>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Multidimensional scaling, 簡稱 MDS, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 MDS 的原理及做法, 更精確的說, 這裡講的是 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in \mathbb{R}^p
$$
那我們可以據此構造出一個距離平方矩陣 $D$, Euclidian distance matrix, 簡稱 EDM, 其中 $D_{ij} = \|x_i-x_j\|^2$, 也就是 $(x_i)$ 及 $x_j$ 這兩個點距離的平方.&lt;/p&gt;
&lt;hr&gt;
&lt;h5 id=&#34;舉例來說&#34;&gt;舉例來說&lt;/h5&gt;
&lt;p&gt;以下我們用 Matlab 隨機生出 $R^2$ 空間中的 $5$ 個點, 並求出他的 EDM:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
X = rand(2,5);                                  % R^2 中的 5 個點
D = squareform(pdist(X&#39;, &#39;squaredeuclidean&#39;));  % 求出其 EDM
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;D =
     0    0.3839    0.2333    0.2675    0.6373
0.3839         0    0.1571    0.1272    0.0470
0.2333    0.1571         0    0.0028    0.3725
0.2675    0.1272    0.0028         0    0.3222
0.6373    0.0470    0.3725    0.3222         0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可看出 EDM 的幾個性質&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一定是個對稱矩陣&lt;/li&gt;
&lt;li&gt;所有元素都非負&lt;/li&gt;
&lt;li&gt;對角線元素必為零&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;而 MDS 要做的事是以上的反問題:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設我們拿到一個 EDM 矩陣, 我想要把它原始生成的點 $x_i$ 找出來.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;這樣子的問題稱為 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;classical MDS 處理這些距離用 Euclidean distance 來量出來的矩陣.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;要講 MDS 做法之前我們先定義一個矩陣稱為&lt;strong&gt;置中矩陣&lt;/strong&gt; $H$, centering matrix, 也就是把一組資料平移使得其中心為坐標原點:
$$
H = I_n - \frac{1}{n}{\bf 1}{\bf 1}^T,
$$
其中 $I_n$ 是 $n\times n$ 的單位矩陣, ${\bf 1}$ 則是元素全為 $1$ 的 $n\times 1$ 向量.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;可以輕易看出來 $H$ 是一個對稱矩陣, $H^T=H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;若我們將原始資料收集一起成一個 $p\times n$ 矩陣 $X = [x_1, \cdots, x_n]$, 也就是把原始資料每一筆當成一個 column 排隊排好, 則有
$$
X H = X (I_n - \frac{1}{n}{\bf 1}{\bf 1}^T) = X - \frac{1}{n}\left(\sum_i x_i\right) {\bf 1}^T = X - \mu {\bf 1}^T = Y,
$$
其中 $\mu=\frac{1}{n}\sum_i x_i \in \mathbb{R}^p$ 就是原始資料的平均,
而 $Y=[y_1, \cdots, y_n]$, $y_i = x_i-\mu$, 也就是把每筆資料減去平均之後記成 $y_i$ 再排排站好.
所以的確 $XH$ 就是把資料平移使得其中心為坐標原點.&lt;/p&gt;
&lt;p&gt;推導一下後也可以發現, 若將每筆資料當成一個 row 擺好變成 $\hat{X}$,
則要用 $H\hat{X}$ 將資料做平移使資料中心為原點.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們有以下這個定理&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;給定原始資料 $X = [x_1, \cdots, x_n]$, 且其相對應的 EDM 為 $D$, 則我們有
$$Y^T Y = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個定理證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;h3 id=&#34;sketch-not-complete-please-full-in-the-details-by-yourself&#34;&gt;(Sketch, not complete, please full-in the details by yourself)&lt;/h3&gt;
&lt;p&gt;我們先重新整理一下這個 EDM 矩陣:
$$
\begin{aligned}
D_{ij} &amp;amp;= \|x_i-x_j\|^2 = &amp;lt;x_i-x_j, x_i-x_j&amp;gt; \\ &amp;amp;= &amp;lt;x_i, x_i&amp;gt; + &amp;lt;x_j, x_j&amp;gt; - 2&amp;lt;x_i, x_j&amp;gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;接著我們定義一個 $n\times 1$ 向量 ${\bf k}$, 其中 ${\bf k}_i = &amp;lt;x_i, x_i&amp;gt;$, 則可以將 $D$ 改寫為
$$D = {\bf k}{\bf 1}^T + {\bf 1}{\bf k}^T - 2 X^TX.$$
接著兩邊乘上 $H$, 並利用 ${\bf 1}^T H = 0$ 以及 $H {\bf 1} = 0$ (分別將一個常數列向量以及常數行向量置中都會得到零向量), 我們可得
$$HDH = -2HX^TXH = -2(XH)^T(XH) = -2Y^TY.$$
故得證.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;這個定理告訴我們的是, 如果我們只知道一組資料的 EDM 矩陣 $D$, 那要怎樣把資料給還原回來.&lt;/p&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark&lt;/h4&gt;
&lt;p&gt;當然, 不可能把原始資料完全還原回來! 如同定理中所述我們所能算出的等號左邊是 $Y$, 也就是置中後的原始資料. 事實上我們也很容易想像, 若將一組資料平移或是旋轉後其 EDM 應該是完全不會變的. 也就是說我們若只知道 EDM, 則其原始資料應該有無限多解. 這裡所謂的還原回來是找到其中一組解, 其他所有可能的解則都可以將之&lt;strong&gt;平移&lt;/strong&gt;或&lt;strong&gt;旋轉&lt;/strong&gt;後得到.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;定理等號右邊由於是個對稱矩陣, 所以可以對角化為
$$
-\frac{1}{2}HDH = V\Lambda V^T = V\sqrt{\Lambda}\sqrt{\Lambda} V^T,
$$
其中 $\Lambda$ 是個對角矩陣包含所有特徵值, 而 $\sqrt{\Lambda}$ 則是將 $\Lambda$ 對角線元素都開根號.&lt;/p&gt;
&lt;p&gt;跟定理對照一下可以輕易地看出來, 平移後的原始資料點可以被還原出來: $Y = \sqrt{\Lambda}V^T$.&lt;/p&gt;
&lt;p&gt;這就是 classical MDS 的作法!! 非常簡單.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若原始資料 $x_i\in R^p$ (並假設排排站之後的 $X$ 其 rank 為 $p$), 則 $Y^TY$ 的 rank 為 $p$, 必有至少 $n-p$ 個為零的特徵值. 所以, 若我們將 $-\frac{1}{2}HDH$ 對角化後發現有 $m$ 個為零的特徵值, 表示原資料的 $p=n-m$, 那我們就把這些零特徵值都拿掉, 使 $\sqrt{\Lambda}$ 為一個 $p\times n$ 的矩陣, 這樣我們就有 $Y = \sqrt{\Lambda}V^T \in R^{p\times n}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若將 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$. 則 $Y^TY = \hat{V}\hat{\Sigma}^T\hat{\Sigma}\hat{V}^T$. 對照一下定理可以看出, 如果 $D$ 是個 EDM 矩陣, 那他的 eigenvalues 一定都是非負.&lt;/p&gt;
&lt;p&gt;依照上面兩個 remark 稍微整理一下 MDS 的做法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;將 EDM 矩陣 $D$ 做 double centering 並乘以 $-1/2$ 求出 $-\frac{1}{2}HDH$.&lt;/li&gt;
&lt;li&gt;做對角化得到 $\Lambda$ 以及 $V$&lt;/li&gt;
&lt;li&gt;拿掉所有零特徵值及其相對應的特徵向量, 求出 $Y = \sqrt{\Lambda}V^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般來說我們會將 MDS 當成一個降維的工具, 希望在低維度(二或三, 或 $k$)找到一組資料使得其 EDM 與原始資料的 EDM 最像. 而作法就是保留前 $k$ 個特徵值及其特徵向量.&lt;/p&gt;
&lt;p&gt;給定一個 $n\times n$ 的 EDM 以及欲投影的維度 $k$, MDS 簡單的程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function Y = multidimensional_scaling(D, k)

%   Input: D, n*n EDM 矩陣, 元素為距離平方
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: Y: k*n data matrix, k 個 features 以及 n 個 samples

    n = size(D, 1);                         % n 個 samples
    mu = sum(D, 1)/n; D = D - mu;           %
    mu = sum(D, 2)/n; B = D - mu;           %
    B = -0.5*B;                             % B = -0.5*H*D*H
    [V, D] = eig(B, &#39;vector&#39;);              % 求出特徵值及特徵向量
    [sqD, ind] = sort(sqrt(D), &#39;descend&#39;);  % 將特徵值按大小排列
    sqD = sqD(1:k);                         % 取前 k 個
    V = V(:, ind(1:k));                     % 取相對應的特徵向量
    Y = V&#39;.*(sqD*ones(1,n));                % 求出 k 維資料點
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;廣義一點的 MDS 可以想像是, 我們拿到的距離也許不是用歐式距離量的. 比如說手裡有許多照片, 照片與照片兩兩之間的距離就有非常多種測量的方式. 而不管是用什麼方式量的, 只要他們是 &lt;strong&gt;距離(metric)&lt;/strong&gt; , 我們就可以定出一個 $D$ 矩陣, 其中 $d_{ij}$ 就是 sample points $x_i$ 與 $x_j$ 之間的距離.&lt;/p&gt;
&lt;p&gt;接著我們可以定義一個 cost function, 稱之為 stress:
$$
Stress_D(x_1, \cdots, x_n) = \left(\sum_i\sum_j\left(d_{ij} - \|x_i-x_j\|\right)^2\right)^{1/2}.
$$
Metric Multidimensional scaling (mMDS) 要做的事就是要找到一組資料點 $\{x_1, \cdots, x_n\}$ 使得上式 stress 有最小值.&lt;/p&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若 $D$ 是用廣義距離造出來的, 就無法保證 $-\frac{1}{2}HDH$ 這矩陣的特徵值都是正的. 不過我們依然可以用 classical MDS 的做法來做, 只是這時候我們會將所有負的特徵值全都丟掉.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-1&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;原始點資料若是在某個 weighted inner product space 裡, 內積定義為
$$
&amp;lt;x, y&amp;gt;_Q = x^TQy.
$$
則我們可以將原本的定理推廣如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem-1&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;$$Y^T QY = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-classical-mds-vs-pca&#34;&gt;Extension: classical MDS v.s. PCA&lt;/h2&gt;
&lt;p&gt;給定 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p,
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們可以造出其 EDM, 再用 classical MDS 投影到 $k$ 維.&lt;/li&gt;
&lt;li&gt;我們也可以直接用 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis&#34;&gt;PCA&lt;/a&gt; 投影到 $k$ 維.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有趣的是以上兩個做法得到完全相同的結果.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不管 PCA 或 classical MDS 最後都是考慮置中後的資料, 所以我們只需看 $Y$ 即可, $y_i = x_i-\mu$, 而 $\mu$ 是平均數.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;PCA 是對 $\Sigma = YY^T$ 做 spectral decomposition&lt;/li&gt;
&lt;li&gt;classical MDS 是對 $Y^TY$ 做 spectral decomposition&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;若將 $Y$ 做 SVD 得到 $Y = \hat{U}\hat{\Sigma}\hat{V}^T$, 則&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PCA 我們知道最後投影的結果為 $B = \hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;li&gt;classical MDS 我們也是投影到 $\hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以雖然出發點不同, 不過結果真的一模一樣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PCA = classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/Yt0o8ukIOKU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GeostatsGuy Lectures - Multidimensional Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;主成分分析, Principal component analysis, 簡稱 PCA, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;資料降維簡單來說, 假設我們有一些資料, 這資料中的每一筆維度都很高, 導致我們很難 &amp;ldquo;看出&amp;rdquo; 或 &amp;ldquo;分析&amp;rdquo; 這資料集的特性, 這時候我們就會想要在低維度空間裡(通常三維以下)建構一組點資料, 並且想辦法使新的這組點資料在某些程度上能表示出原本高維度的點資料, 或保有某種特性. 這種將高維度資料在低維度表現出來的方式就稱為資料降維. 而 PCA 就是其中一種線性降維的方式.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 PCA 的原理及做法.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p
$$
那我們想要做以下等價的兩件事:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到一組低維度的投影並且使得資料有 &lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;找到一組低維度的投影並且使得原始資料與投影後的資料之間的 &lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這裡所謂低維度的投影講精確一點, 事實上是在原本的 $R^p$ 空間中找到一個低維度的仿射子空間(affine subspace), 可以想像成一個不穿過原點的點或直線或平面, 找到之後再把原始資料點投影上去.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-0-維&#34;&gt;Example: 投影到 $0$ 維&lt;/h3&gt;
&lt;p&gt;先舉一個最簡單的例子, 假設我們想要投影到 $0$ 維, 也就是投影到一個點.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也就是說我們想要找一個點來代表整組資料.&lt;/p&gt;
&lt;p&gt;直覺上來想, 如果我要幫一組資料找一個最具代表性的點, 應該就會用 &amp;ldquo;平均數&amp;rdquo; 或 &amp;ldquo;中位數&amp;rdquo; 來當這個點.&lt;/p&gt;
&lt;p&gt;不過這邊我們需要講清楚的是究竟是以何種機制來做選擇的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PCA 的做法就是要找一個點 $\mu\in R^p$ 使得所有資料點到這個點的距離平方和最小, 也就是要讓下式有最小值
$$
\sum_{i=1}^n \|x_i - \mu\|^2.
$$
簡單的微分求極值我們可以得到其最佳解為
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i,
$$
也就是原始資料點的平均值.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果我們想要的是&amp;quot;距離&amp;quot;和最小而不是&amp;quot;距離平方&amp;quot;和, 也就是要使下式最小,
$$
\sum_{i=1}^n \|x_i - \mu\|,
$$
則最佳解為資料的中位數. 證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm#:~:text=111-,The%20Median%20Minimizes%20the%20Sum,Deviations%20%28The%20L1%20Norm%29&amp;amp;text=is%20minimal%20if%20x%20is%20equal%20to%20the%20median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Median minimizes sum of absolute deviations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-1-維&#34;&gt;Example: 投影到 $1$ 維&lt;/h3&gt;
&lt;p&gt;投影到 $0$ 維也許過於簡化, 接著我們來投影到 $1$ 維試試, 我們直接用圖形來說明:















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

藍色小圈就是原始資料點, 共10筆資料. 而 PCA 要做的事就是找到一個一維的 affine subspace, 就是中間那條斜線. 這樣我們就能把原始資料都投影到這個子空間去, 投影後的資料就是紅色點.&lt;/p&gt;
&lt;p&gt;而這 affine subspace 怎麼找到的呢, 事實上這個子空間就是使得所有虛線段(原始資料到投影點連線)距離平方加起來最小的那個.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我們來定義更一般的投影, 假設想要投影到 $k$ 維, $k\le p$, 也就是我們想要在 $R^p$ 空間中找一個 $k$ 維的 affine subspace, 使得說投影後資料與原始資料的距離平方和最小. 數學上來說就是要讓下式有最小值
$$
\sum_{i=1}^n \| x_i - (\mu + U\beta_i)\|^2,
$$
其中未知數有 $\mu\in R^p$, $U\in R^{p\times k}$, 以及 $\beta_i\in R^{k\times 1}$.&lt;/p&gt;
&lt;p&gt;稍微解釋一下以上這些變數:&lt;/p&gt;
&lt;p&gt;事實上這個 affine subspace 可以表示成
$$
\mu + V, \quad V = span\{u_1, \cdots, u_k\},
$$
其中 $\mu$ 是 $R^p$ 空間中的一個點, $\{u_1, u_2, \cdots, u_k\}$ 是這 affine subspace 的基底, 收集在一起構成 $U$, 也就是 $U=[u_1, u_2, \cdots, u_k]$. 而 $\mu + U\beta_i$ 就是 $x_i$ 這個資料點在這 affine subspace 上的投影, 所以 $\beta_i$ 可以想像是原始資料第 $i$ 筆投影到 affine subspace 之後的座標, 或是係數.&lt;/p&gt;
&lt;p&gt;此外, 我們可以特別要求這基底要正交, 也就是 $U^TU = I_k$, 其中 $I_k$ 表示 $k\times k$ 的 identity matrix.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;再稍微整理一下, 我們想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;第二個條件 $\sum_i \beta_i=\vec{0}$ 看起來似乎是憑空冒出來的, 不過這可以從兩方面來說.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們希望投影之後的解他們的平均是 $0$, 所以新資料點的中心就在新座標原點的位置.&lt;/li&gt;
&lt;li&gt;數學上來說, 其實 $\mu$ 並沒有唯一性. 也就是雖然 affine subspace 表示成 $\mu +V$, 但是這個 $\mu$ 是這子空間裡的任何一個點都可以. 為了讓解有唯一性我們加了這個條件.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;這問題可以被完全解出來, 也就是說給定 $x_i$, 我們可以決定出最佳的 $\mu$, $U$ 以及 $\beta_i$ 使得上式的值為最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推導過程我們在這先省略, 有興趣的請見 references, 或是 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其最佳解整理如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 就剛好是投影到 $0$ 維的解, 也就是資料點的平均:
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$ 是個 $p\times n$ 的矩陣, 而 $\Sigma$ 則是 $p\times p$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原資料投影到此 affine subspace 中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, (x_i-\mu)&amp;gt;
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般最基礎的 PCA 做法就是從 $x_i$, 得到 $\mu$. 將資料平移求出 $y_i$, 接著求共變異數矩陣 $\Sigma$. 接著對 $\Sigma$ 做 eigen-decomposition 求出其特徵值及特徵向量. 拿出最大的幾個就可以定出 affine subspace. 然後將 $y_i$ 投影下去就得到 $\beta_i$, 也就是投影之後的座標了.&lt;/p&gt;
&lt;p&gt;以上這做法的 &lt;code&gt;Matlab&lt;/code&gt; 程式如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$X$ 是 $p\times n$ 的 data matrix 含有 $p$ 個 features 以及 $n$ 個 samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;投影到 $k$ 維子空間, 投影後得到 $B$, 為 $k\times n$ 的 data matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均 mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    S = Y*Y&#39;;                       % 求出共變異數矩陣 S
    [U, D] = eig(S, &#39;vector&#39;);      % 求出特徵值 D 及特徵向量 U
    [D, ind] = sort(D, &#39;descend&#39;);  % 將特徵值由大到小排列
    U = U(:, ind);                  % 將特徵向量照樣排列
    B = U(:,1:k)&#39;*Y;                % 投影到前 k 個組成的空間中並求出係數 B
end
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這做法會造出一個 $p\times p$ 的共變異數矩陣 $\Sigma$, 並且算出全部的特徵值及特徵向量. 不過其實我們只需要前 $k$ 個. 所以多出來的部分會丟掉, 有點浪費.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們再來看一下這個共變異數矩陣 $\Sigma=YY^T$, 可以很輕易地看出來這是個對稱矩陣, 所以特徵值一定是實數, 也一定存在 orthonormal 的實數特徵向量組. 因此我們以上的要求(將特徵值按大小排列, 特徵向量要彼此正交)都一定做得到. 而分解之後可以得到
$$
\Sigma = U\Lambda U^T,
$$
其中 $U$ 是個 $p\times p$ 矩陣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;事實上由 $\Sigma=YY^T$ 可以知道 $\Sigma$ 一定是個半正定矩陣, 所以它的 eigenvalues 一定都非負.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們事實上可以將矩陣 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}$ 及 $\hat{V}$ 都是 orthogonal matrix, 也就是 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$.&lt;/p&gt;
&lt;p&gt;既然我們有 $\Sigma=YY^T$, 很容易可以看出來其實
$$
U = \hat{U}, \quad \Lambda = \hat{\Sigma}\hat{\Sigma}^T\in R^{p\times p}.
$$
也就是說將 $Y$ 的 singular values 平方就可以得到 $\Sigma$ 的 eigenvalues. 而 $Y$ 的 left singular vectors 事實上就是 $\Sigma$ 的 eigenvectors.&lt;/p&gt;
&lt;p&gt;而投影後的係數我們可以算一下:
$$
B=U^TY = \hat{U}^T\hat{U}\hat{\Sigma}\hat{V}^T = \hat{\Sigma}\hat{V}^T.
$$
所以如果我們只需要投影後的係數, 將 $Y$ 做 SVD 並且我們需要的是 $Y$ 的 right singular vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;以上這式子很有趣, 告訴我們投影之後的座標 $B$ 與投影之前的座標 $Y$ 的 SVD 之間的關係.
另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt; 在某些情況下會有一模一樣的關係. 這樣就把兩種方法 PCA 跟 MDS 連在一起了. 雖然出發點不一樣, 竟然(在某些情況下)結果是一樣的!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;利用以上關係我們可以將程式改寫如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis2(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均, mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    [~, S, V] = svds(Y, k);         % 將 Y 做奇異值分解並取前 k 個 eigenvectors
    B = S*V&#39;;                       % 求出投影後的係數
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;而事實上, matlab 已經內建 PCA 程式了, 所以其實完全不用自己寫. 只是要注意一下 matlab 裡 PCA 的輸入 sample points 是 $n\times p$, 由於我們以上都是將 $X$ 設成 $p\times n$ 的矩陣, 所以要轉置一下, 程式只有兩行, 如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
[U, B] = pca(X&#39;);
B = B(:,1:k)&#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;working-example&#34;&gt;Working example&lt;/h1&gt;
&lt;p&gt;這裡我們舉一個例子, 我們先構造 sample points, 是一個類似螺旋狀結構, 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
n = 1000;                                   % 取 n 個點
t = 3*pi*(1:n)/n;                           % 利用參數化來構造, 取 t\in[0, 3*pi]
X = [t.*cos(t); 5*rand(1,n); t.*sin(t)];    % 先利用 random 構造三圍中的一個面
M = makehgtform(&#39;axisrotate&#39;,[1 1 1], 30);  % 構造旋轉矩陣
X = M(1:3, 1:3)*X + 10*rand(3,1)*ones(1,n); % 將 sample 旋轉並且隨機平移
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scatter3(X(1,:), X(2,:), X(3,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;接著我們用 PCA 投影到二維, 圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[~, B] = pca(X&#39;);
B=B&#39;;
scatter(B(1,:), B(2,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;可以發現 PCA 找到一個正確的軸來做投影, 使得原本的螺旋線可以看得很清楚.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PCA 推導過程及證明: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>擲硬幣問題</title>
      <link>https://teshenglin.github.io/post/2020_coin_problem/</link>
      <pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_coin_problem/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;重複擲一枚硬幣, 當出現連續兩次為正時停止, 平均要扔幾次?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;第一種做法-窮舉法&#34;&gt;第一種做法: 窮舉法&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們以正負號 (+ -) 代表正面反面.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;觀察&#34;&gt;觀察&lt;/h3&gt;
&lt;p&gt;假設花 $n$ 次才擲到連續兩次為正, $n \geq 2$.&lt;/p&gt;
&lt;p&gt;我們發現, 可以從第 $n-1$ 次的結果進行窮舉法得到第 $n$ 次的結果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當 $n = 2$, 只會有 1 種情況 : &lt;code&gt;++&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;要窮舉 $n = 3$, 要知道三次中的尾巴兩次必須是 $n=2$ 的所有情況. 由於 $n=2$ 的第一個為正, 因此要在前加一項只能為負, 因此 $n=3$ 只有 &lt;code&gt;-++&lt;/code&gt; 這情況&lt;/li&gt;
&lt;li&gt;對 $n=4$, 尾巴三次必須是 $n=3$ 的所有情況. 由於 $n=3$ 的第一個為負, 因此要在前加正或負皆可, 因此 $n=4$ 有兩種情況: &lt;code&gt;+-++&lt;/code&gt;,  &lt;code&gt;--++&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;小結論&#34;&gt;小結論&lt;/h4&gt;
&lt;p&gt;對 $n=k+1$ 而言, 我們只需要考慮 $n=k$ 所有情況的第一個是正或負即可. 若為正則只能在前面再加個負, 若為負則可以在前面加正或負.&lt;/p&gt;
&lt;p&gt;也就是說, 假設 $n=k$ 中有 $x_k$ 個情況第一個為正, 有 $y_k$ 個情況第一個為負, 那 $n=k+1$ 的所有情況中必有 $y_k$ 個情況第一個為正, $x_k+y_k$ 個情況第一個為負.
寫成矩陣型式就是
$$
\begin{bmatrix} x_{k+1} \\ y_{k+1}\end{bmatrix}
= A
\begin{bmatrix} x_{k} \\ y_{k}\end{bmatrix}, \quad
A =
\begin{bmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 1 \end{bmatrix}
$$
則我們有
$$
\begin{bmatrix} x_{k} \\ y_{k}\end{bmatrix}
= A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}, \quad k\ge 2.
$$
並且我們知道花 $k$ 次才達到連續兩次為正共會有 $x_k+y_k$ 種情況, 意即
$$
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}.
$$
另外我們還知道丟硬幣 $k$ 次之任一情況的機率為 $\frac{1}{2^k}$.&lt;/p&gt;
&lt;h3 id=&#34;計算期望值&#34;&gt;計算期望值&lt;/h3&gt;
&lt;p&gt;利用以上結果我們可以知道期望值為
$$
E = \sum_{k=2}^\infty
\left(
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k-2}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
\right)
\cdot \frac{k}{2^{k}}
=\sum_{k=0}^\infty
\left(
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
\right)
\cdot \frac{k+2}{2^{k+2}}.
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;對角化
$$
A = \begin{bmatrix}0 &amp;amp; 1 \\ 1 &amp;amp; 1\end{bmatrix}
= \frac{1}{\sqrt{5}}
\begin{bmatrix}1 &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\frac{1+\sqrt{5}}{2} &amp;amp; 0 \\ 0 &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\frac{\sqrt{5}-1}{2} &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; -1\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可得
$$
A^k
= \frac{1}{\sqrt{5}}
\begin{bmatrix}1 &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; \frac{1-\sqrt{5}}{2}\end{bmatrix}
\begin{bmatrix}\left(\frac{1+\sqrt{5}}{2}\right)^k &amp;amp; 0 \\ 0 &amp;amp; \left(\frac{1-\sqrt{5}}{2}\right)^k\end{bmatrix}
\begin{bmatrix}\frac{\sqrt{5}-1}{2} &amp;amp; 1 \\ \frac{1+\sqrt{5}}{2} &amp;amp; -1\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接著我們有
$$
\begin{bmatrix} 1 &amp;amp; 1\end{bmatrix}
A^{k}
\begin{bmatrix} 1 \\ 0\end{bmatrix}
=\frac{1}{\sqrt{5}}\left[(\frac{1+\sqrt{5}}{2})^{k-1}\cdot\frac{3+\sqrt{5}}{2} - (\frac{1-\sqrt{5}}{2})^{k-1}\cdot\frac{3-\sqrt{5}}{2}\right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此
$$
E = \frac{3+\sqrt{5}}{16\sqrt{5}} \underbrace{\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1+\sqrt{5}}{4})^{k-1}}_{(a)} - \frac{3-\sqrt{5}}{16\sqrt{5}} \underbrace{\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1-\sqrt{5}}{4})^{k-1}}_{(b)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;recall-calculus&#34;&gt;Recall Calculus&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;要算 (a) 與 (b) 我們先 recall 微積分裡的一個結果&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$
\sum_{n=0}^{\infty} x^n = \frac{1}{1-x}, \quad |x| &amp;lt; 1.
$$
兩邊微分得到
$$
\sum_{n=0}^{\infty} n x^{n-1} = \frac{1}{(1-x)^2}, \quad |x| &amp;lt; 1.
$$&lt;/p&gt;
&lt;p&gt;利用這個我們可以算 (a)
$$
\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1+\sqrt{5}}{4})^{k-1} = 18 + 10\sqrt{5}
$$
也可以算 (b)
$$
\sum_{k=0}^{\infty}(k+2)\cdot(\frac{1-\sqrt{5}}{4})^{k-1} = 18 - 10\sqrt{5}
$$&lt;/p&gt;
&lt;p&gt;最後,
$$
E = \frac{3+\sqrt{5}}{16\sqrt{5}}\cdot (18+10\sqrt{5}) - \frac{3-\sqrt{5}}{16\sqrt{5}}\cdot (18-10\sqrt{5}) = 6
$$&lt;/p&gt;
&lt;h3 id=&#34;結論&#34;&gt;結論&lt;/h3&gt;
&lt;p&gt;重複擲一枚硬幣, 當出現連續兩次為正時停止, 平均要扔&lt;strong&gt;六&lt;/strong&gt;次.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;第二種做法&#34;&gt;第二種做法:&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們可以用 Markov chain 來想這問題&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;觀察-1&#34;&gt;觀察&lt;/h3&gt;
&lt;p&gt;假設目前連續投擲出 0 個正, 那下一投有 $0.5$ 機率是正以及 $0.5$ 機率是負.
也就是 $0.5$ 機率會是到 &lt;code&gt;1正&lt;/code&gt; 這狀態以及 $0.5$ 機率回到 &lt;code&gt;0正&lt;/code&gt; 這狀態.&lt;/p&gt;
&lt;p&gt;假設目前連續投擲出 1 個正, 那下一投則 $0.5$ 機率會是到 &lt;code&gt;2正&lt;/code&gt; 這狀態以及 $0.5$ 機率回到 &lt;code&gt;0正&lt;/code&gt; 這狀態.&lt;/p&gt;
&lt;p&gt;根據這些觀察我們可以做出以下的 diagram:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34; data-lang=&#34;mermaid&#34;&gt;graph LR;
A[0 正] -- 0.5 --&amp;gt;B(1 正)
  B -- 0.5 --&amp;gt; C(2 正)
  A -- 0.5 --&amp;gt; A
  B -- 0.5 --&amp;gt; A
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接著我們假設從 &lt;code&gt;0正&lt;/code&gt; 這狀態需要投 $k_0$ 次才會到 &lt;code&gt;2正&lt;/code&gt;, 而從 &lt;code&gt;1正&lt;/code&gt; 這狀態需要投 $k_1$ 次才會到 &lt;code&gt;2正&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;對 &lt;code&gt;0正&lt;/code&gt; 狀態而言, 投一次有 $0.5$ 機會到&lt;code&gt;1正&lt;/code&gt;狀態, 然後再需要 $k_1$ 次到&lt;code&gt;2正&lt;/code&gt;, 所以這樣總共投 $1+k_1$ 次.
而 &lt;code&gt;0正&lt;/code&gt; 投一次也有 $0.5$ 機會會回到自己原始 &lt;code&gt;0正&lt;/code&gt; 狀態, 然後再需要 $k_0$ 次到&lt;code&gt;2正&lt;/code&gt;, 所以這樣總共投 $1+k_0$ 次.
如此可以列式:
$$
k_0 = 0.5(1+k_1) + 0.5(1+k_0)
$$&lt;/p&gt;
&lt;p&gt;同理, 對 &lt;code&gt;1正&lt;/code&gt; 狀態而言我們有以下式子
$$
k_1 = 0.5(1) + 0.5(1+k_0)
$$&lt;/p&gt;
&lt;p&gt;這樣我們有兩個未知數兩個方程式, 解出來得到
$$
k_0=6, \quad k_1=4.
$$&lt;/p&gt;
&lt;p&gt;所以, 從 &lt;code&gt;0正&lt;/code&gt; 狀態到出現連續兩次為正平均要投 $6$ 次.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
