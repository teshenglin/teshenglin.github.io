<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computational Mathematics | Te-Sheng Lin</title>
    <link>https://teshenglin.github.io/categories/computational-mathematics/</link>
      <atom:link href="https://teshenglin.github.io/categories/computational-mathematics/index.xml" rel="self" type="application/rss+xml" />
    <description>Computational Mathematics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 07 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Computational Mathematics</title>
      <link>https://teshenglin.github.io/categories/computational-mathematics/</link>
    </image>
    
    <item>
      <title>主成分分析 - 2</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們補充一下主成分分析裡的證明部分.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p.
$$
假設想要投影到 $k$ 維, $k\le p$, 數學上來說就是想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;要求極值就是要找微分等於零的解, 由於這式子是 convex, 所以保證找到唯一解而且是最小的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;所以以下做法就是對每個變數做偏微分, 並求出偏微分等於零的解. 這樣就把最佳解找出來了!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;1-首先看-mu&#34;&gt;1. 首先看 $\mu$&lt;/h4&gt;
&lt;p&gt;對 $\mu$ 做偏微分並且利用 $\sum^n_{i=1} \beta_i=\vec{0}$ 我們可以得到
$$
\partial_{\mu} E = -2 \left(\sum_i x_i - n\mu\right)=0.
$$
因此, 最佳的 $\mu$ 是
$$
\mu = \frac{1}{n}\sum_{i=1}^n x_i.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-接著找-beta_i&#34;&gt;2. 接著找 $\beta_i$&lt;/h4&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 我們有
$$
E = \sum_{i=1}^n \|y_i - U\beta_i\|^2.
$$
接著對 $\beta_i$ 微分得到
$$
\partial_{\beta_i} E = 2\left(\beta_i - U^Ty_i\right)=0.
$$
因此可以得到
$$
\beta_i = U^T y_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;,
$$
其中 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-最後找-u&#34;&gt;3. 最後找 $U$&lt;/h4&gt;
&lt;p&gt;代入最佳的 $\beta_i$ 後我們又可將原本的 $E$ 改寫為
$$
E = \sum_{i=1}^n \|y_i - UU^Ty_i\|^2 = \|Y - UU^TY\|^2_F,
$$
其中 $Y=[y_1, \cdots, y_n]$, 而下標 $F$ 代表矩陣的 Frobenius norm.&lt;/p&gt;
&lt;p&gt;我們先定 $P = UU^T$, 則有 $P^T=P$, $P^2=P$. 接著我們改寫 $E$ 為
$$
E = \|Y - PY\|^2_F = trace\left[(Y-PY)^T(Y-PY)\right] = trace\left(Y^TY-Y^TPY\right).
$$
由於 $Y$ 不會變, 因此求 $E$ 的最小值變成求 $trace\left(-Y^TPY\right)$ 的最小值, 也就是求 $trace\left(Y^TPY\right)$ 的最大值, 換回來得到是求 $trace\left(Y^TUU^TY\right)$ 的最大值.&lt;/p&gt;
&lt;p&gt;接著我們用線性代數裡一個定理, $trace(AB)=trace(BA)$, 將原式轉換成求 $trace\left(U^TYY^TU\right)$ 的最大值, 最後得到
$$
\arg\min_U E = \arg\max_U trace\left(U^T\Sigma U\right),
$$
其中 $\Sigma=YY^T$ 也就是原資料的共變異數矩陣. 上式是個非常重要的式子, 它告訴我們以下兩件事是等價的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使得原始資料與投影後的資料之間的&lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;使得資料有&lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最後, 線性代數告訴我們等號右邊這問題的解就是 $\Sigma$ 最大的 $k$ 個 eigenvalues 其相對應的 eigenvectors, 收集起來得到 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以上使用的線性代數結果, 其證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/252272/is-trace-invariant-under-cyclic-permutation-with-rectangular-matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tr(AB)=tr(BA) proof&lt;/a&gt;, 以及 
&lt;a href=&#34;https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;maximize v^T Av&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;總結一下最佳解如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 是原始資料的平均
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將平移後的資料投影到子空間中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Multidimensional scaling</title>
      <link>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_multi_dimensional_scaling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Multidimensional scaling, 簡稱 MDS, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 MDS 的原理及做法, 更精確的說, 這裡講的是 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in \mathbb{R}^p
$$
那我們可以據此構造出一個距離平方矩陣 $D$, Euclidian distance matrix, 簡稱 EDM, 其中 $D_{ij} = \|x_i-x_j\|^2$, 也就是 $(x_i)$ 及 $x_j$ 這兩個點距離的平方.&lt;/p&gt;
&lt;hr&gt;
&lt;h5 id=&#34;舉例來說&#34;&gt;舉例來說&lt;/h5&gt;
&lt;p&gt;以下我們用 Matlab 隨機生出 $R^2$ 空間中的 $5$ 個點, 並求出他的 EDM:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
X = rand(2,5);                                  % R^2 中的 5 個點
D = squareform(pdist(X&#39;, &#39;squaredeuclidean&#39;));  % 求出其 EDM
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;D =
     0    0.3839    0.2333    0.2675    0.6373
0.3839         0    0.1571    0.1272    0.0470
0.2333    0.1571         0    0.0028    0.3725
0.2675    0.1272    0.0028         0    0.3222
0.6373    0.0470    0.3725    0.3222         0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可看出 EDM 的幾個性質&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一定是個對稱矩陣&lt;/li&gt;
&lt;li&gt;所有元素都非負&lt;/li&gt;
&lt;li&gt;對角線元素必為零&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;而 MDS 要做的事是以上的反問題:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設我們拿到一個 EDM 矩陣, 我想要把它原始生成的點 $x_i$ 找出來.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;這樣子的問題稱為 classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;classical MDS 處理這些距離用 Euclidean distance 來量出來的矩陣.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;要講 MDS 做法之前我們先定義一個矩陣稱為&lt;strong&gt;置中矩陣&lt;/strong&gt; $H$, centering matrix, 也就是把一組資料平移使得其中心為坐標原點:
$$
H = I_n - \frac{1}{n}{\bf 1}{\bf 1}^T,
$$
其中 $I_n$ 是 $n\times n$ 的單位矩陣, ${\bf 1}$ 則是元素全為 $1$ 的 $n\times 1$ 向量.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;可以輕易看出來 $H$ 是一個對稱矩陣, $H^T=H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;若我們將原始資料收集一起成一個 $p\times n$ 矩陣 $X = [x_1, \cdots, x_n]$, 也就是把原始資料每一筆當成一個 column 排隊排好, 則有
$$
X H = X (I_n - \frac{1}{n}{\bf 1}{\bf 1}^T) = X - \frac{1}{n}\left(\sum_i x_i\right) {\bf 1}^T = X - \mu {\bf 1}^T = Y,
$$
其中 $\mu=\frac{1}{n}\sum_i x_i \in \mathbb{R}^p$ 就是原始資料的平均,
而 $Y=[y_1, \cdots, y_n]$, $y_i = x_i-\mu$, 也就是把每筆資料減去平均之後記成 $y_i$ 再排排站好.
所以的確 $XH$ 就是把資料平移使得其中心為坐標原點.&lt;/p&gt;
&lt;p&gt;推導一下後也可以發現, 若將每筆資料當成一個 row 擺好變成 $\hat{X}$,
則要用 $H\hat{X}$ 將資料做平移使資料中心為原點.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們有以下這個定理&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;給定原始資料 $X = [x_1, \cdots, x_n]$, 且其相對應的 EDM 為 $D$, 則我們有
$$Y^T Y = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個定理證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;h3 id=&#34;sketch-not-complete-please-full-in-the-details-by-yourself&#34;&gt;(Sketch, not complete, please full-in the details by yourself)&lt;/h3&gt;
&lt;p&gt;我們先重新整理一下這個 EDM 矩陣:
$$
\begin{aligned}
D_{ij} &amp;amp;= \|x_i-x_j\|^2 = &amp;lt;x_i-x_j, x_i-x_j&amp;gt; \\ &amp;amp;= &amp;lt;x_i, x_i&amp;gt; + &amp;lt;x_j, x_j&amp;gt; - 2&amp;lt;x_i, x_j&amp;gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;接著我們定義一個 $n\times 1$ 向量 ${\bf k}$, 其中 ${\bf k}_i = &amp;lt;x_i, x_i&amp;gt;$, 則可以將 $D$ 改寫為
$$D = {\bf k}{\bf 1}^T + {\bf 1}{\bf k}^T - 2 X^TX.$$
接著兩邊乘上 $H$, 並利用 ${\bf 1}^T H = 0$ 以及 $H {\bf 1} = 0$ (分別將一個常數列向量以及常數行向量置中都會得到零向量), 我們可得
$$HDH = -2HX^TXH = -2(XH)^T(XH) = -2Y^TY.$$
故得證.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;這個定理告訴我們的是, 如果我們只知道一組資料的 EDM 矩陣 $D$, 那要怎樣把資料給還原回來.&lt;/p&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark&lt;/h4&gt;
&lt;p&gt;當然, 不可能把原始資料完全還原回來! 如同定理中所述我們所能算出的等號左邊是 $Y$, 也就是置中後的原始資料. 事實上我們也很容易想像, 若將一組資料平移或是旋轉後其 EDM 應該是完全不會變的. 也就是說我們若只知道 EDM, 則其原始資料應該有無限多解. 這裡所謂的還原回來是找到其中一組解, 其他所有可能的解則都可以將之&lt;strong&gt;平移&lt;/strong&gt;或&lt;strong&gt;旋轉&lt;/strong&gt;後得到.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;定理等號右邊由於是個對稱矩陣, 所以可以對角化為
$$
-\frac{1}{2}HDH = V\Lambda V^T = V\sqrt{\Lambda}\sqrt{\Lambda} V^T,
$$
其中 $\Lambda$ 是個對角矩陣包含所有特徵值, 而 $\sqrt{\Lambda}$ 則是將 $\Lambda$ 對角線元素都開根號.&lt;/p&gt;
&lt;p&gt;跟定理對照一下可以輕易地看出來, 平移後的原始資料點可以被還原出來: $Y = \sqrt{\Lambda}V^T$.&lt;/p&gt;
&lt;p&gt;這就是 classical MDS 的作法!! 非常簡單.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若原始資料 $x_i\in R^p$ (並假設排排站之後的 $X$ 其 rank 為 $p$), 則 $Y^TY$ 的 rank 為 $p$, 必有至少 $n-p$ 個為零的特徵值. 所以, 若我們將 $-\frac{1}{2}HDH$ 對角化後發現有 $m$ 個為零的特徵值, 表示原資料的 $p=n-m$, 那我們就把這些零特徵值都拿掉, 使 $\sqrt{\Lambda}$ 為一個 $p\times n$ 的矩陣, 這樣我們就有 $Y = \sqrt{\Lambda}V^T \in R^{p\times n}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若將 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$. 則 $Y^TY = \hat{V}\hat{\Sigma}^T\hat{\Sigma}\hat{V}^T$. 對照一下定理可以看出, 如果 $D$ 是個 EDM 矩陣, 那他的 eigenvalues 一定都是非負.&lt;/p&gt;
&lt;p&gt;依照上面兩個 remark 稍微整理一下 MDS 的做法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;將 EDM 矩陣 $D$ 做 double centering 並乘以 $-1/2$ 求出 $-\frac{1}{2}HDH$.&lt;/li&gt;
&lt;li&gt;做對角化得到 $\Lambda$ 以及 $V$&lt;/li&gt;
&lt;li&gt;拿掉所有零特徵值及其相對應的特徵向量, 求出 $Y = \sqrt{\Lambda}V^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般來說我們會將 MDS 當成一個降維的工具, 希望在低維度(二或三, 或 $k$)找到一組資料使得其 EDM 與原始資料的 EDM 最像. 而作法就是保留前 $k$ 個特徵值及其特徵向量.&lt;/p&gt;
&lt;p&gt;給定一個 $n\times n$ 的 EDM 以及欲投影的維度 $k$, MDS 簡單的程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function Y = multidimensional_scaling(D, k)

%   Input: D, n*n EDM 矩陣, 元素為距離平方
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: Y: k*n data matrix, k 個 features 以及 n 個 samples

    n = size(D, 1);                         % n 個 samples
    mu = sum(D, 1)/n; D = D - mu;           %
    mu = sum(D, 2)/n; B = D - mu;           %
    B = -0.5*B;                             % B = -0.5*H*D*H
    [V, D] = eig(B, &#39;vector&#39;);              % 求出特徵值及特徵向量
    [sqD, ind] = sort(sqrt(D), &#39;descend&#39;);  % 將特徵值按大小排列
    sqD = sqD(1:k);                         % 取前 k 個
    V = V(:, ind(1:k));                     % 取相對應的特徵向量
    Y = V&#39;.*(sqD*ones(1,n));                % 求出 k 維資料點
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;廣義一點的 MDS 可以想像是, 我們拿到的距離也許不是用歐式距離量的. 比如說手裡有許多照片, 照片與照片兩兩之間的距離就有非常多種測量的方式. 而不管是用什麼方式量的, 只要他們是 &lt;strong&gt;距離(metric)&lt;/strong&gt; , 我們就可以定出一個 $D$ 矩陣, 其中 $d_{ij}$ 就是 sample points $x_i$ 與 $x_j$ 之間的距離.&lt;/p&gt;
&lt;p&gt;接著我們可以定義一個 cost function, 稱之為 stress:
$$
Stress_D(x_1, \cdots, x_n) = \left(\sum_i\sum_j\left(d_{ij} - \|x_i-x_j\|\right)^2\right)^{1/2}.
$$
Metric Multidimensional scaling (mMDS) 要做的事就是要找到一組資料點 $\{x_1, \cdots, x_n\}$ 使得上式 stress 有最小值.&lt;/p&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;若 $D$ 是用廣義距離造出來的, 就無法保證 $-\frac{1}{2}HDH$ 這矩陣的特徵值都是正的. 不過我們依然可以用 classical MDS 的做法來做, 只是這時候我們會將所有負的特徵值全都丟掉.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-1&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;原始點資料若是在某個 weighted inner product space 裡, 內積定義為
$$
&amp;lt;x, y&amp;gt;_Q = x^TQy.
$$
則我們可以將原本的定理推廣如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem-1&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;$$Y^T QY = -\frac{1}{2}HDH.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension-classical-mds-vs-pca&#34;&gt;Extension: classical MDS v.s. PCA&lt;/h2&gt;
&lt;p&gt;給定 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p,
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們可以造出其 EDM, 再用 classical MDS 投影到 $k$ 維.&lt;/li&gt;
&lt;li&gt;我們也可以直接用 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis&#34;&gt;PCA&lt;/a&gt; 投影到 $k$ 維.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有趣的是以上兩個做法得到完全相同的結果.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不管 PCA 或 classical MDS 最後都是考慮置中後的資料, 所以我們只需看 $Y$ 即可, $y_i = x_i-\mu$, 而 $\mu$ 是平均數.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;PCA 是對 $\Sigma = YY^T$ 做 spectral decomposition&lt;/li&gt;
&lt;li&gt;classical MDS 是對 $Y^TY$ 做 spectral decomposition&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;若將 $Y$ 做 SVD 得到 $Y = \hat{U}\hat{\Sigma}\hat{V}^T$, 則&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PCA 我們知道最後投影的結果為 $B = \hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;li&gt;classical MDS 我們也是投影到 $\hat{\Sigma}\hat{V}^T$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以雖然出發點不同, 不過結果真的一模一樣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PCA = classical MDS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/Yt0o8ukIOKU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GeostatsGuy Lectures - Multidimensional Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;主成分分析, Principal component analysis, 簡稱 PCA, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;資料降維簡單來說, 假設我們有一些資料, 這資料中的每一筆維度都很高, 導致我們很難 &amp;ldquo;看出&amp;rdquo; 或 &amp;ldquo;分析&amp;rdquo; 這資料集的特性, 這時候我們就會想要在低維度空間裡(通常三維以下)建構一組點資料, 並且想辦法使新的這組點資料在某些程度上能表示出原本高維度的點資料, 或保有某種特性. 這種將高維度資料在低維度表現出來的方式就稱為資料降維. 而 PCA 就是其中一種線性降維的方式.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 PCA 的原理及做法.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p
$$
那我們想要做以下等價的兩件事:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到一組低維度的投影並且使得資料有 &lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;找到一組低維度的投影並且使得原始資料與投影後的資料之間的 &lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這裡所謂低維度的投影講精確一點, 事實上是在原本的 $R^p$ 空間中找到一個低維度的仿射子空間(affine subspace), 可以想像成一個不穿過原點的點或直線或平面, 找到之後再把原始資料點投影上去.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-0-維&#34;&gt;Example: 投影到 $0$ 維&lt;/h3&gt;
&lt;p&gt;先舉一個最簡單的例子, 假設我們想要投影到 $0$ 維, 也就是投影到一個點.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也就是說我們想要找一個點來代表整組資料.&lt;/p&gt;
&lt;p&gt;直覺上來想, 如果我要幫一組資料找一個最具代表性的點, 應該就會用 &amp;ldquo;平均數&amp;rdquo; 或 &amp;ldquo;中位數&amp;rdquo; 來當這個點.&lt;/p&gt;
&lt;p&gt;不過這邊我們需要講清楚的是究竟是以何種機制來做選擇的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PCA 的做法就是要找一個點 $\mu\in R^p$ 使得所有資料點到這個點的距離平方和最小, 也就是要讓下式有最小值
$$
\sum_{i=1}^n \|x_i - \mu\|^2.
$$
簡單的微分求極值我們可以得到其最佳解為
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i,
$$
也就是原始資料點的平均值.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果我們想要的是&amp;quot;距離&amp;quot;和最小而不是&amp;quot;距離平方&amp;quot;和, 也就是要使下式最小,
$$
\sum_{i=1}^n \|x_i - \mu\|,
$$
則最佳解為資料的中位數. 證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm#:~:text=111-,The%20Median%20Minimizes%20the%20Sum,Deviations%20%28The%20L1%20Norm%29&amp;amp;text=is%20minimal%20if%20x%20is%20equal%20to%20the%20median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Median minimizes sum of absolute deviations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-1-維&#34;&gt;Example: 投影到 $1$ 維&lt;/h3&gt;
&lt;p&gt;投影到 $0$ 維也許過於簡化, 接著我們來投影到 $1$ 維試試, 我們直接用圖形來說明:















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

藍色小圈就是原始資料點, 共10筆資料. 而 PCA 要做的事就是找到一個一維的 affine subspace, 就是中間那條斜線. 這樣我們就能把原始資料都投影到這個子空間去, 投影後的資料就是紅色點.&lt;/p&gt;
&lt;p&gt;而這 affine subspace 怎麼找到的呢, 事實上這個子空間就是使得所有虛線段(原始資料到投影點連線)距離平方加起來最小的那個.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我們來定義更一般的投影, 假設想要投影到 $k$ 維, $k\le p$, 也就是我們想要在 $R^p$ 空間中找一個 $k$ 維的 affine subspace, 使得說投影後資料與原始資料的距離平方和最小. 數學上來說就是要讓下式有最小值
$$
\sum_{i=1}^n \| x_i - (\mu + U\beta_i)\|^2,
$$
其中未知數有 $\mu\in R^p$, $U\in R^{p\times k}$, 以及 $\beta_i\in R^{k\times 1}$.&lt;/p&gt;
&lt;p&gt;稍微解釋一下以上這些變數:&lt;/p&gt;
&lt;p&gt;事實上這個 affine subspace 可以表示成
$$
\mu + V, \quad V = span\{u_1, \cdots, u_k\},
$$
其中 $\mu$ 是 $R^p$ 空間中的一個點, $\{u_1, u_2, \cdots, u_k\}$ 是這 affine subspace 的基底, 收集在一起構成 $U$, 也就是 $U=[u_1, u_2, \cdots, u_k]$. 而 $\mu + U\beta_i$ 就是 $x_i$ 這個資料點在這 affine subspace 上的投影, 所以 $\beta_i$ 可以想像是原始資料第 $i$ 筆投影到 affine subspace 之後的座標, 或是係數.&lt;/p&gt;
&lt;p&gt;此外, 我們可以特別要求這基底要正交, 也就是 $U^TU = I_k$, 其中 $I_k$ 表示 $k\times k$ 的 identity matrix.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;再稍微整理一下, 我們想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;第二個條件 $\sum_i \beta_i=\vec{0}$ 看起來似乎是憑空冒出來的, 不過這可以從兩方面來說.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們希望投影之後的解他們的平均是 $0$, 所以新資料點的中心就在新座標原點的位置.&lt;/li&gt;
&lt;li&gt;數學上來說, 其實 $\mu$ 並沒有唯一性. 也就是雖然 affine subspace 表示成 $\mu +V$, 但是這個 $\mu$ 是這子空間裡的任何一個點都可以. 為了讓解有唯一性我們加了這個條件.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;這問題可以被完全解出來, 也就是說給定 $x_i$, 我們可以決定出最佳的 $\mu$, $U$ 以及 $\beta_i$ 使得上式的值為最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推導過程我們在這先省略, 有興趣的請見 references, 或是 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其最佳解整理如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 就剛好是投影到 $0$ 維的解, 也就是資料點的平均:
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$ 是個 $p\times n$ 的矩陣, 而 $\Sigma$ 則是 $p\times p$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原資料投影到此 affine subspace 中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, (x_i-\mu)&amp;gt;
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般最基礎的 PCA 做法就是從 $x_i$, 得到 $\mu$. 將資料平移求出 $y_i$, 接著求共變異數矩陣 $\Sigma$. 接著對 $\Sigma$ 做 eigen-decomposition 求出其特徵值及特徵向量. 拿出最大的幾個就可以定出 affine subspace. 然後將 $y_i$ 投影下去就得到 $\beta_i$, 也就是投影之後的座標了.&lt;/p&gt;
&lt;p&gt;以上這做法的 &lt;code&gt;Matlab&lt;/code&gt; 程式如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$X$ 是 $p\times n$ 的 data matrix 含有 $p$ 個 features 以及 $n$ 個 samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;投影到 $k$ 維子空間, 投影後得到 $B$, 為 $k\times n$ 的 data matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均 mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    S = Y*Y&#39;;                       % 求出共變異數矩陣 S
    [U, D] = eig(S, &#39;vector&#39;);      % 求出特徵值 D 及特徵向量 U
    [D, ind] = sort(D, &#39;descend&#39;);  % 將特徵值由大到小排列
    U = U(:, ind);                  % 將特徵向量照樣排列
    B = U(:,1:k)&#39;*Y;                % 投影到前 k 個組成的空間中並求出係數 B
end
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這做法會造出一個 $p\times p$ 的共變異數矩陣 $\Sigma$, 並且算出全部的特徵值及特徵向量. 不過其實我們只需要前 $k$ 個. 所以多出來的部分會丟掉, 有點浪費.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們再來看一下這個共變異數矩陣 $\Sigma=YY^T$, 可以很輕易地看出來這是個對稱矩陣, 所以特徵值一定是實數, 也一定存在 orthonormal 的實數特徵向量組. 因此我們以上的要求(將特徵值按大小排列, 特徵向量要彼此正交)都一定做得到. 而分解之後可以得到
$$
\Sigma = U\Lambda U^T,
$$
其中 $U$ 是個 $p\times p$ 矩陣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;事實上由 $\Sigma=YY^T$ 可以知道 $\Sigma$ 一定是個半正定矩陣, 所以它的 eigenvalues 一定都非負.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們事實上可以將矩陣 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}$ 及 $\hat{V}$ 都是 orthogonal matrix, 也就是 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$.&lt;/p&gt;
&lt;p&gt;既然我們有 $\Sigma=YY^T$, 很容易可以看出來其實
$$
U = \hat{U}, \quad \Lambda = \hat{\Sigma}\hat{\Sigma}^T\in R^{p\times p}.
$$
也就是說將 $Y$ 的 singular values 平方就可以得到 $\Sigma$ 的 eigenvalues. 而 $Y$ 的 left singular vectors 事實上就是 $\Sigma$ 的 eigenvectors.&lt;/p&gt;
&lt;p&gt;而投影後的係數我們可以算一下:
$$
B=U^TY = \hat{U}^T\hat{U}\hat{\Sigma}\hat{V}^T = \hat{\Sigma}\hat{V}^T.
$$
所以如果我們只需要投影後的係數, 將 $Y$ 做 SVD 並且我們需要的是 $Y$ 的 right singular vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;以上這式子很有趣, 告訴我們投影之後的座標 $B$ 與投影之前的座標 $Y$ 的 SVD 之間的關係.
另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt; 在某些情況下會有一模一樣的關係. 這樣就把兩種方法 PCA 跟 MDS 連在一起了. 雖然出發點不一樣, 竟然(在某些情況下)結果是一樣的!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;利用以上關係我們可以將程式改寫如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis2(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均, mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    [~, S, V] = svds(Y, k);         % 將 Y 做奇異值分解並取前 k 個 eigenvectors
    B = S*V&#39;;                       % 求出投影後的係數
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;而事實上, matlab 已經內建 PCA 程式了, 所以其實完全不用自己寫. 只是要注意一下 matlab 裡 PCA 的輸入 sample points 是 $n\times p$, 由於我們以上都是將 $X$ 設成 $p\times n$ 的矩陣, 所以要轉置一下, 程式只有兩行, 如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
[U, B] = pca(X&#39;);
B = B(:,1:k)&#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;working-example&#34;&gt;Working example&lt;/h1&gt;
&lt;p&gt;這裡我們舉一個例子, 我們先構造 sample points, 是一個類似螺旋狀結構, 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
n = 1000;                                   % 取 n 個點
t = 3*pi*(1:n)/n;                           % 利用參數化來構造, 取 t\in[0, 3*pi]
X = [t.*cos(t); 5*rand(1,n); t.*sin(t)];    % 先利用 random 構造三圍中的一個面
M = makehgtform(&#39;axisrotate&#39;,[1 1 1], 30);  % 構造旋轉矩陣
X = M(1:3, 1:3)*X + 10*rand(3,1)*ones(1,n); % 將 sample 旋轉並且隨機平移
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scatter3(X(1,:), X(2,:), X(3,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;接著我們用 PCA 投影到二維, 圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[~, B] = pca(X&#39;);
B=B&#39;;
scatter(B(1,:), B(2,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;可以發現 PCA 找到一個正確的軸來做投影, 使得原本的螺旋線可以看得很清楚.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PCA 推導過程及證明: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Clenshaw–Curtis quadrature</title>
      <link>https://teshenglin.github.io/post/2020_numerical_integration_4/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_numerical_integration_4/</guid>
      <description>&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2019_numerical_integration&#34;&gt;數值積分初探&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_2&#34;&gt;以內插多項式來做數值積分&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_3&#34;&gt;高斯積分&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要利用 Chebyshev polynomial 來算積分&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-任意給定一可積分函數-fx-xin-1-1-我們想要算-int1_-1-fx-dx&#34;&gt;Goal: 任意給定一可積分函數 $f(x)$, $x\in[-1, 1]$, 我們想要算 $\int^1_{-1} f(x) dx$.&lt;/h4&gt;
&lt;hr&gt;
&lt;h4 id=&#34;idea&#34;&gt;Idea:&lt;/h4&gt;
&lt;p&gt;我們簡單做個變數變換 $x = \cos\theta$ 可以得到
$$
\int^1_{-1} f(x) dx = \int^{\pi}_0 f(\cos\theta)\sin\theta d\theta.
$$
所以如果我能將 $f(x)$ 這個函數以 second kind Chebyshev polynomial 來做展開, i.e.,
$$
f(x) = \sum^N_{k=0} a_k T_k(x) = \sum^N_{k=0} a_k \cos(k\cos^{-1}x) = \sum^N_{k=0} a_k \cos(k\theta)=f(\cos\theta).
$$
那代入後我們就有
$$
\int^1_{-1} f(x) dx = \int^{\pi}_0 \sum^N_{k=0} a_k \cos(k\theta)\sin\theta d\theta = \sum^{N/2}_{k=0}\frac{2 a_{2k}}{1-(2k)^2}.
$$&lt;/p&gt;
&lt;p&gt;因此, 若我們知道如何求一個函數的 Chebyshev coefficients, $a_k$, 那我們很容易就可以計算 $\int^1_{-1}f(x)dx$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;簡單的 Matlab 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% generate grid points
N = 25;
x = -sin((-N:2:N)&#39;*pi/(2*N));

% evaluate at grid points
f = exp(sin(x));

% extend the function and take scaled FFT
f_extend = [f; f(N: -1:2)];
f_hat = fft(f_extend)/(2*N);

% obtain the Chebyshev coefficients
a = zeros(N+1,1);
a(1) = real(f_hat(1));
a(2:N) = 2*real(f_hat(2:N));
a(N+1) = real(f_hat(N+1));

% Clenshaw–Curtis weights
w = zeros(1,N+1);
if(mod(N,2)==0)
    M = N/2;
else
    M = (N-1)/2;
end
w(1:2:end) = 2./(1-4*(0:M).^2);

% Clenshaw–Curtis quadrature
w*a
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;以上做法是先將函數取值, 算其係數, 再求其積分.
不過實際上我們應該能將積分權重直接算出來, 如此一來我們就能直接由函數值求積分了.
也就是說我們希望能寫成以下這式子:
$$
\int^1_{-1} f(x)dx \approx \sum^N_{k=0} w_k f(x_k), \quad x_k = \cos\left(\frac{k\pi}{N}\right).
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;積分權重&#34;&gt;積分權重:&lt;/h4&gt;
&lt;p&gt;要求得積分權重我們首先回顧一下上面的作法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先將點做 even extension&lt;/li&gt;
&lt;li&gt;做 FFT&lt;/li&gt;
&lt;li&gt;由 Fourier coefficients 求得 Chebyshev coefficients&lt;/li&gt;
&lt;li&gt;再將係數乘上積分權重即得到積分值&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上步驟可以寫成矩陣形式:
$$
q = [\pmb{w}][\pmb{I}]^T[\pmb{F}][\pmb{I}][\pmb{f}]
$$
其中&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$[\pmb{f}]$ 是函數值, 是個 $(N+1)\times 1$ 的矩陣&lt;/li&gt;
&lt;li&gt;$[\pmb{I}]$ 是 even extension 是個 $2N\times (N+1)$ 的矩陣&lt;/li&gt;
&lt;li&gt;$[\pmb{F}]$ 是 Fourier transform 是個 $2N\times 2N$ 的矩陣&lt;/li&gt;
&lt;li&gt;$[\pmb{w}]$ 是 Clenshaw–Curtis 對於係數的 weights 是個 $1\times (N+1)$ 的矩陣&lt;/li&gt;
&lt;li&gt;$q$ 是最後得到的數值積分值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以這樣看就很清楚, Clenshaw–Curtis 對於函數值的 weights 事實上就是 $[\pmb{w}][\pmb{I}]^T[\pmb{F}][\pmb{I}]$ 這串矩陣乘法.
也就是說如果我定義
$$
[CCW]=[\pmb{w}][\pmb{I}]^T[\pmb{F}][\pmb{I}],
$$
則我們有
$$
q = [CCW][\pmb{f}]
$$
而由於 $[\pmb{F}]$ 是對稱矩陣, 我們可以把這整個做矩陣轉置, 這樣就很清楚要怎樣求 weight 了
$$
[CCW]^T = [\pmb{I}]^T[\pmb{F}]^T[\pmb{I}][\pmb{w}]^T,
$$
其中 $[\pmb{F}]^T$ 這個矩陣乘法則是以 iFFT 來加速計算.&lt;/p&gt;
&lt;p&gt;簡單的 Matlab 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;N=25;

w = zeros(1,N+1);
if(mod(N,2)==0)
    M=N/2;
else
    M = (N-1)/2;
end
w(1:2:end) = 2./(1-4*(0:M).^2);

w2 = [w, w(N: -1:2)];
CCW = ifft(w2&#39;);
CCW = [CCW(1), 2*CCW(2:N)&#39;, CCW(N+1)];
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有了權重後即可由函數值直接求得積分值:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% generate grid points
N = 25;
x = -sin((-N:2:N)&#39;*pi/(2*N));

% evaluate at grid points
f = exp(sin(x));

% Clenshaw–Curtis quadrature
CCW*f
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier - 01</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;在微積分課程裡我們有學到如何利用 Lagrange multiplier 來解 constraint optimization 問題. 這邊要介紹課本裡沒教的 Lagrangian function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-我們想要解以下這個問題&#34;&gt;Goal: 我們想要解以下這個問題&lt;/h4&gt;
&lt;p&gt;$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=0.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;微積分課本告訴我們一個相對好懂的直觀, 就是這個解會發生在兩個等高線相切的地方, 因此在這個問題的解那個點的位置, 兩個函數 $f$ 與 $g$ 等高線的法向量會平行, 因此可以列出以下兩個式子
$$
\partial_x f + \lambda \partial_x g = 0, \quad  g(x)=0.
$$
這裡我們引進一個新的未知數 $\lambda$, 即稱為 Lagrange multiplier.
因為有兩個方程式, 因此我們可以解出兩個未知數 $x$ 以及 $\lambda$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;上列的這個方程組有另一種相對好記的方式, 就是引進所謂的 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x)
$$
這是一個雙變數方程式, 而原本問題的解會發生在這個方程的 critical point, 也就是會滿足 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;要注意的是由於 $L$ 是個雙變數函數, 所以 $\nabla = (\partial_x, \partial_{\lambda})$. 根據這樣的定義我們將 $\nabla L=0$ 方程式列出來會得到跟上面一模一樣的兩個方程組.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 1:&lt;/strong&gt;  $\partial_{x} L = \partial_x f + \lambda \partial_x g$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 2:&lt;/strong&gt;  $\partial_{\lambda} L = g(x)$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;引進 Lagrangian function 比較好記是因為假設我有更多的 constraints, 例如我想解以下這個問題:
$$
\min_{x} f(x), \quad \text{subject to } \quad  g_1(x)=0, \quad g_2(x)=0.
$$
要找一個函數最小值不過有兩個限制條件.&lt;/p&gt;
&lt;p&gt;這樣的話我們照之前的步驟, 先引進 Lagrangian function
$$
L(x, \lambda_1, \lambda_2) = f(x) + \lambda_1 g_1(x) + \lambda_2 g_2(x)
$$
然後原問題的最佳解會發生在 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;要注意的是這次的 $L$ 是個三變數函數, 所以 $\nabla L = (\partial_x, \partial_{\lambda_1}, \partial_{\lambda_2})$. 然後將 $\nabla L=0$ 寫下來就是
$$
\partial_x f + \lambda_1 \partial_x g_1 + \lambda_2 \partial_x g_2 = 0, \quad  g_1(x)=0, \quad g_2(x)=0.
$$
就得到我們需要解的方程組了!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接著我們試著將以上寫的更廣義一點, 考慮一個 $n$ 維度的最佳化問題有 $m$ 個限制式, 我們引進一些符號: $\pmb{x}\in \mathbb{R}^n$,  $\pmb{g}(\pmb{x}):\mathbb{R}^n\to\mathbb{R}^m$, 這個有限制的最佳化問題即可寫成
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x}), \quad \text{subject to } \quad  \pmb{g}(\pmb{x})=\pmb{0}.
$$&lt;/p&gt;
&lt;p&gt;我們接著引進 Lagrangian function
$$
L(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T \pmb{g}(\pmb{x}), \quad \pmb{\lambda}\in\mathbb{R}^m
$$&lt;/p&gt;
&lt;p&gt;原問題的最佳解會發生在 $\nabla L=0$, 也就是 $(\nabla_{\pmb{x}} L, \nabla_{\pmb{\lambda}} L) = (0, 0)$, 也就是
$$
\nabla_{\pmb{x}} f(\pmb{x}) + \pmb{\lambda}^T \nabla_{\pmb{x}}\pmb{g}(\pmb{x}) = 0, \quad \pmb{g}(\pmb{x})=\pmb{0}.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;所以為什麼要引進 Lagrange multiplier 或是 Lagrangian function?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrange multiplier 的好處是我們將一個&lt;strong&gt;最佳化問題&lt;/strong&gt;改寫成一個&lt;strong&gt;求根問題&lt;/strong&gt;. 理論上 $F(x)=0$ 這種問題不管線性或非線性我們都比較會解. 而最佳化問題就相對比較無從下手.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BUT&lt;/strong&gt;, 付出的代價是我們增加了維度! 原本 $n$ 維最小值問題變成 $n+m$ 維求根問題.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrangian function 最明顯的好處是&lt;strong&gt;比較好記&lt;/strong&gt;, 不管是&lt;code&gt;記&lt;/code&gt;在頭腦裡或是做筆&lt;code&gt;記&lt;/code&gt;寫下來. 整個問題很輕易的就記成 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;當然還有其他更重要的好處, 就是可以推導出所謂的 dual optimization problem: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_lagrange_multiplier_2&#34;&gt;Lagrange Multiplier - 02&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lagrange multiplier $\lambda$ 感覺起來很像一點用都沒有, 就是為了解出問題過程所引進的虛擬變數. 其實它還是有一點意義的, 可見: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_lagrange_multiplier_3&#34;&gt;Lagrange Multiplier - 03&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier - 02</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier_2/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們再多討論一點 Lagrangian function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們先看最簡單的一維問題, 求一個有限制式的函數最小值問題:
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=0.
$$&lt;/p&gt;
&lt;p&gt;我們引進 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x)
$$
並且知道原問題的解發生在 $\nabla L=0$ 的地方.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 從微積分我們知道 $\nabla L=0$ 是 $L(x, \lambda)$ 這個函數 critical point發生的地方, 那這個 critical point 究竟是 local max/min 還是 saddle 呢?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;要回答這問題其實很簡單, 我們看這個二維函數的判別式就知道. 這函數的變數有兩個分別是 $x$ 以及 $\lambda$, 所以判別式是
$$
L_{xx}L_{\lambda\lambda} - L_{x\lambda}^2 = -(g_x)^2 \le 0
$$
所以我們知道原最佳化問題的解其實是這個 Lagragian function 的 saddle point (至少不是 local max/min)!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Constraint optimization problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們事實上可以考慮更一般的問題
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x) \le 0.
$$
這樣的問題我們可以定義一樣的 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x).
$$&lt;/p&gt;
&lt;p&gt;我們有以下這個定理:&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem---saddle-point-sufficient-condition&#34;&gt;Theorem - Saddle point (Sufficient condition)&lt;/h2&gt;
&lt;p&gt;Let $P$ be a constraint optimization problem.
If $(x^+, \lambda^+)$ is a saddle point, that is,
$$\forall x\in\mathbb{R}, \forall\lambda\ge 0, \quad L(x^+, \lambda)\le L(x^+, \lambda^+)\le L(x, \lambda^+),$$
then it is a solution of $P$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;根據不等式第一個部分
$$\forall\lambda\ge 0, \quad L(x^+, \lambda)\le L(x^+, \lambda^+)$$
帶入 $L$ 我們得到 $\lambda g(x^+) \le \lambda^+ g(x^+)$.&lt;/p&gt;
&lt;p&gt;由於對所有 $\lambda\ge 0$ 都對, 我們讓 $\lambda\to\infty$ 得到 $g(x^+)\le 0$.
接著讓 $\lambda\to 0$ 我們得到 $0\le \lambda^+ g(x^+)\le 0$, 因此 $\lambda^+ g(x^+)=0$.&lt;/p&gt;
&lt;p&gt;有這件事後我們再看不等式第二部分, $L(x^+, \lambda^+)\le L(x, \lambda^+)$.
代入 $L$ 以及以上結果我們有 $f(x^+)\le f(x) + \lambda^+ g(x)$.&lt;/p&gt;
&lt;p&gt;因此, 若 $g(x)\le 0$, 我們必定有 $f(x^+)\le f(x)$.&lt;/p&gt;
&lt;p&gt;QED.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h4 id=&#34;可以看到在上面有個隱藏的假設是-lambdage-0-可以被很輕易地看出來所以這裡就不記下了&#34;&gt;可以看到在上面有個隱藏的假設是 $\lambda\ge 0$. 可以被很輕易地看出來所以這裡就不記下了.&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;更進一步, 我們可以引進這個 Lagrange dual function:
$$
F(\lambda) = \inf_{x} L(x, \lambda)
$$
也就是固定一個 $\lambda$ 我去找 $L$ 這函數的最大下界(有點像最小值不過不一定要碰到). 據此我們可以引進以下的 dual problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dual optimization problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\max_{\lambda} F(\lambda), \quad \lambda \ge 0.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;舉個例子&#34;&gt;舉個例子&lt;/h4&gt;
&lt;p&gt;我們考慮這個特別的 constraint optimization 問題
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x}), \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0},
$$
其中 $A$ 是個矩陣而 $\pmb{b}$ 是個向量. 也就是說限制式為 $g(\pmb{x}) = A\pmb{x}-\pmb{b}$.&lt;/p&gt;
&lt;p&gt;把 Lagrangian function 寫下來就是
$$
L(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T(A\pmb{x}-\pmb{b}).
$$
對於 dual problem 我們可以構造一個迭代法來解這問題.&lt;/p&gt;
&lt;p&gt;由 Dual optimization problem 我們知道對於 $F$ 這個函數我們要求其最大值. 所以我們試著朝 gradient 方向走來往上走, 意即
$$
\lambda^{k+1} = \lambda_k + \alpha^k\nabla_{\lambda} F(\lambda^k),
$$
其中 $\alpha^k$ 是步長. 而且我們知道 $\nabla_{\lambda} F(\lambda^k) = A\pmb{\tilde{x}}-\pmb{b},$ 其中 $\pmb{\tilde{x}} = argmin \ L(\pmb{x}, \pmb{\lambda}^k)$. 因此我們有以下這方法:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dual ascent iteration method:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步: 固定一個 $\pmb{\lambda}$ 我們解一個 optimization 問題
$$
\pmb{x}^{k+1} = argmin \ L(\pmb{x}, \pmb{\lambda}^k)
$$
&lt;strong&gt;Remark:&lt;/strong&gt; 這是一個沒有 constraint 的最小值問題, 可以用 gradient descent 或各式方法來解.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二步: 接著我們用 gradient ascent 來更新 $\pmb{\lambda}$
$$
\lambda^{k+1} = \lambda^k + \alpha^k(A\pmb{x}^{k+1}-\pmb{b})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;迭代下去我們可以得到原問題的解!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;augmented-lagrangian&#34;&gt;Augmented Lagrangian&lt;/h2&gt;
&lt;p&gt;一個很有趣的更進一步推廣是將 Lagrangian 引入一個新的 penalty term:
$$
L_{\rho}(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T(A\pmb{x}-\pmb{b}) + \frac{\rho}{2}\|A\pmb{x}-\pmb{b}\|^2_2,
$$
其中 $\rho$ 是個常數.&lt;/p&gt;
&lt;p&gt;我們可以反過來將這個 Lagrangian 所對應的限制最佳化問題寫下來:
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x})+\frac{\rho}{2}\|A\pmb{x}-\pmb{b}\|^2_2, \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0}.
$$
我們可以輕易發現, 加了這新的一項對於這整個問題的最佳解完全沒有影響! 也就是說 $L_{\rho}$ 所得到的解就是原本 $L$ 的解.&lt;/p&gt;
&lt;p&gt;接著我們一樣用 dual ascent iteration method 來解這問題（給定 $\pmb{x}^{k}$ 以及 $\pmb{\lambda}^{k}$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Method of multiplier:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步: 我們解一個 optimization 問題
$$
\pmb{x}^{k+1} = argmin \ L_{\rho}(\pmb{x}, \pmb{\lambda}^k)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二步: 接著我們用 gradient ascent 來更新 $\pmb{\lambda}$
$$
\pmb{\lambda}^{k+1} = \pmb{\lambda}^k + \rho(A\pmb{x}^{k+1}-\pmb{b})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 這裡有個有趣的事, 原本的方法中步長是不知道的需要自己決定, 不過在這裡我們卻直接設定步常為 $\rho$. 接下來我們就是要解釋這部分.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步中, 如果 $\pmb{x}^{k+1}$ 是個解那它要滿足 $\nabla_{\pmb{x}} L_{\rho}=0$, 其中整理一下發現
$$
\nabla_{\pmb{x}} L_{\rho}=\nabla_{\pmb{x}} f(\pmb{x}^{k+1}) + A^T\left(\pmb{\lambda}^k + \rho (A\pmb{x}^{k+1}-\pmb{b})\right)=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所以, 如果第二步中的步長我們設定為 $\rho$, 那上式就可以改寫為
$$
\nabla_{\pmb{x}} f(\pmb{x}^{k+1}) + A^T\pmb{\lambda}^{k+1}=0.
$$
有趣的是, 對原本的 Lagrangian 而言我們有
$$
\nabla_{\pmb{x}} L=\nabla_{\pmb{x}} f(\pmb{x}) + A^T\pmb{\lambda},
$$
所以我們這樣設定步長, 剛好使得我們每次算出的 $\pmb{x}^{k+1}$ 以及 $\pmb{\lambda}^{k+1}$ 滿足
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^{k+1})=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;再把以上兩個方法 summarize 一下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本問題是
$$
\min_{\pmb{x}} f(\pmb{x}), \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrangian 後變成我們要解以下方程
$$
\nabla_{\pmb{x}} L =\nabla_{\pmb{x}} f(\pmb{x}) + A^T\pmb{\lambda}=0, \quad \nabla_{\pmb{\lambda}} L=A\pmb{x}-\pmb{b}=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們使用迭代法來解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若用 &lt;strong&gt;dual ascent iteration method&lt;/strong&gt;, 則有
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^k)=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若用 &lt;strong&gt;method of multiplier&lt;/strong&gt;, 則有
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^{k+1})=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對於兩種方法, 原問題的 Constraint 都在 $k\to\infty$ 滿足
$$
\lim_{k\to\infty} A\pmb{x}^k-\pmb{b}=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier - 03</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier_3/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier_3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們討論一下 Lagrange multiplier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們知道, 如果想要解以下這個有限制式的最佳化問題
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=k,
$$
一個方式是引進 Lagrange multiplier, $\lambda$, 然後可以列出以下兩個式子
$$
\partial_x f + \lambda \partial_x g = 0, \quad  g(x)=k.
$$&lt;/p&gt;
&lt;h4 id=&#34;question&#34;&gt;Question:&lt;/h4&gt;
&lt;p&gt;解出聯立方程後我們得到 $x^+$, $\lambda^+$, 帶入原函數得到 $f^+=f(x^+)$.
我們知道 $f^+$ 是最佳值, 而 $x^+$ 是最佳解. 那 $\lambda^+$ 是做什麼用的?
看起來只是為了解出原問題所引進的虛擬變數. 它有什麼作用嗎?&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;先說結論, $\lambda^+ = -\frac{df^+}{dk}$. 也就是告訴我們如果稍微改變限制式中的 $k$ 值, 那原題的最佳值會有怎樣的變化.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;我們先引進 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda (g(x) - k)
$$
並且知道原問題的解發生在 $\nabla L=0$ 的地方, 意即, $\nabla L(x^+, \lambda^+)=0$.
然後我們有
$$
L^+ = L(x^+, \lambda^+) = f(x^+) + \lambda^+(g(x^+)-k) = f^+.
$$&lt;/p&gt;
&lt;p&gt;在這裡我們要稍微小心一點. 固定一個 $k$ 值, 我們就有一個最佳化問題, 然後就可以解出 $x^+$ 跟 $\lambda^+$.
而當 $k$ 值改變時, $x^+$, $\lambda^+$, 甚至 $f^+$ 以及 $L^+$ 也都會跟著改變, 所以我們想像這四個值都是 $k$ 的函數:
$$
x^+ = x^+(k), \ \lambda^+ = \lambda^+(k), \ f^+ = f^+(k), \ L^+ = L^+(k).
$$
再寫清楚一點就是
$$
L^+(k) = \left.L(x, \lambda,k)\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}
$$&lt;/p&gt;
&lt;p&gt;其中
$$
L(x, \lambda,k) = f(x) + \lambda (g(x) - k).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 讀到這邊也許會覺得有個奇怪的地方, 就是為什麼要把 $L$ 從雙變數變成三變數函數.
原因是當寫成 $L(x, \lambda)$ 時 $k$ 是個常數, 是不能變的. 想要改變 $k$ 值需要把它也當成一個變數比較合理.&lt;/p&gt;
&lt;p&gt;接著我們就可以微分
$$
\frac{d L^+}{dk} = \left.\frac{\partial L}{\partial x}\frac{d x}{dk} + \frac{\partial L}{\partial \lambda}\frac{d \lambda}{dk} + \frac{\partial L}{\partial k}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k} = -\lambda^+.
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;
$$
\left.\frac{\partial L}{\partial x}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}=
\left.\frac{\partial L}{\partial \lambda}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}=0.
$$&lt;/p&gt;
&lt;p&gt;由於 $f^+ =L^+$, 所以我們也可以得到 $\frac{d f^+}{dk} = -\lambda^+$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;舉個例子&#34;&gt;舉個例子&lt;/h4&gt;
&lt;p&gt;以下單位皆為&lt;code&gt;元&lt;/code&gt; or &lt;code&gt;萬元&lt;/code&gt; or &lt;code&gt;千萬元&lt;/code&gt;, 請自行依喜好帶入!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;假設我如果花費 $x$ 買進某一股票在一定時間後賣出可獲得 $2x$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假設我如果花費 $y$ 買進某一貴金屬在一定時間後賣出可獲得 $10y-y^2$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假設我總共可運用的財產只有 $10$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;那我應該花多少錢買股票花多少錢買貴金屬, 才能有最佳獲利呢?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們可以列出以下限制最佳化問題&lt;/p&gt;
&lt;p&gt;$$
\max_{x,y} f(x,y)=2x+(10y-y^2), \quad \text{subject to } \quad  x+y=10,
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;先引進 Lagrangian funtion
$$
L(x,y,\lambda) = 2x+(10y-y^2)+\lambda(x+y-10)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列出方程式 $\nabla L=0$:
$$
\frac{\partial L}{\partial x} = 2 + \lambda =0, \quad
\frac{\partial L}{\partial y} = 10-2y + \lambda =0, \quad
\frac{\partial L}{\partial \lambda} = x+y-10 =0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解出得到
$$
x^+ = 6, \quad y^+=4, \quad \lambda^+=-2.
$$
也就是我們若以 $6$ 單位買進股票, $4$ 單位買進貴金屬, 一定時間賣出後可獲利 $f^+ = 36$ 單位.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所以 $\lambda^+=-2$ 告訴我們 $\frac{d f^+}{dk}=-\lambda^+=2$, 意思就是如果我們增加一點 $k$ 值, 那獲利會是所增加數目的&lt;strong&gt;兩倍&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原題改成我目前可運用的財產有 $11$ (也就是多增加了 $1$), 那會發現最佳解是 $x^+=7$, $y^+=4$, $f^+=38$, 獲利真的增加 $2$ 單位了!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果這是真實情形, 那就告訴我們說如果去借 $1$ 單位的錢, 可以賺到 $2$ 單位. 如果借的錢加上利息會小於 $2$ 單位, 那就是個很好的投資, 應該去借.&lt;/li&gt;
&lt;li&gt;這例子比較特殊 $\lambda^+$ 恆等於 $2$, 所以才會那麼巧資金增加一單位獲利就增加兩單位. 一般而言這應該只是 linear appxoimation, 只有 $k$ 增加一點點時才會大約兩倍. 增加太多就不知道了.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;references&#34;&gt;References:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/m-G3K2GPmEQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube: Meaning of the Lagrange multiplier&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/b9B2FZ5cqbM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube: Proof for the meaning of Lagrange multipliers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>以內插多項式來做數值積分</title>
      <link>https://teshenglin.github.io/post/2020_numerical_integration_2/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_numerical_integration_2/</guid>
      <description>&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2019_numerical_integration&#34;&gt;數值積分初探&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;連續函數可以用多項式來逼近它, 因此直覺來講, 既然我們已經找到一個離給定函數&amp;quot;很近&amp;quot;的多項式了, 何不就以這個多項式的積分值來當成原函數積分值的一個逼近呢.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-任意給定一可積分函數-fx-xin-1-1-我們想要算-int1_-1-fx-dx&#34;&gt;Goal: 任意給定一可積分函數 $f(x)$, $x\in[-1, 1]$, 我們想要算 $\int^1_{-1} f(x) dx$.&lt;/h4&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;我們先講內插多項式&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;給定任意 $n+1$ 個不重複的點在 $[-1, 1]$ 區間, 我們把他們記為 $x_0, x_1, \cdots, x_n.$
接著我們計算函數 $f(x)$ 在這些點上的值, 記為 $f_i = f(x_i)$, 這樣我們就有平面上的 $n+1$ 個點座標 $(x_i, f_i)$.&lt;/p&gt;
&lt;p&gt;有這些點我們就一定能找到一個 $n$ 次多項式 $p_n(x)$ 使得說這個多項式會通過這些所有的點, 也就是 $p_n(x_i) = f_i$. 這就叫做內插多項式.&lt;/p&gt;
&lt;p&gt;內插多項式一個很簡潔的表示法是把它寫成 Lagrange polynomial 的樣子. 我們先引進 Lagrange basis function
$$
\ell_i(x) = \prod^n_{k=0, k\ne i} \frac{x-x_k}{x_i-x_k}.
$$
很明顯可以看出這是一個 $n$ 次多項式, 而且有很特殊的性質, 也就是 $\ell_i(x_j)=\delta_{ij}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\delta_{ij}$ 是所謂的 Kronecker delta function, 如果 $i=j$ 則 $\delta_{ij}=1$, 反之如果 $i\ne j$ 則 $\delta_{ij}=0$. 舉例來說, $\delta_{11}=1$, 而 $\delta_{12}=0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因此我們可以將內插多項式 $p_n(x)$ 改寫成
$$
p_n(x) = \sum^n_{i=0} f_i \ell_i(x).
$$
這就是內插多項式的 Lagrange 表示法.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;接著我們就可以來做積分了!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們直接以內插多項式來逼近函數並做積分,
$$
\int^1_{-1} f(x)dx \approx \int^1_{-1} p_n(x)dx = \int^1_{-1} \sum^n_{i=0} f_i \ell_i(x)dx = \sum^n_{i=0} f_i \left(\int^1_{-1} \ell_i(x)dx\right).
$$
理論上, 如果插值點 $x_i$ 是一開始就給定並且固定的, 那我們就可以把 Lagrange basis function $\ell_i(x)$ 算出來, 並且把他的積分也算出來. 我們把積分出來的值記為 $w_i$, 也就是
$$
w_i = \int^1_{-1} \ell_i(x)dx.
$$
如此我們就可以把積分改寫成
$$
\int^1_{-1} f(x)dx \approx \sum^n_{i=0} w_i f_i.
$$
這就是以內插多項式來做積分.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 如果 $f(x)$ 本身就是個 $n$ 次或以下的多項式, 那我們做出來的內插多項式就不是 approximation 而是等號了. 那這樣我們做出來的積分事實上應該就是等號! 也就是說
$$
\int^1_{-1} p(x)dx = \sum^n_{i=0} w_i f_i,
$$
where $p(x)$ is a polynomial with degree less than or equals to $n$.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;舉個簡單的例子來試試&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設我們只想要用兩個端點來估算積分, 那我們有 $x_0=-1, x_1=1$. 因此得到 Lagrange basis polynomials
$$
\ell_0(x) = \frac{x+1}{2}, \quad \ell_1(x)=\frac{x-1}{-2}.
$$
接著可以算出
$$
w_0 = \int^1_{-1} \ell_0(x)dx = 1, \quad w_1=\int^1_{-1}\ell_1(x)dx=1.
$$
也就是說
$$
\int^1_{-1} f(x)dx \approx f(-1) + f(1),
$$
得到所謂的梯形法!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;延伸閱讀: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_3&#34;&gt;高斯積分&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>高斯積分</title>
      <link>https://teshenglin.github.io/post/2020_numerical_integration_3/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_numerical_integration_3/</guid>
      <description>&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2019_numerical_integration&#34;&gt;數值積分初探&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_2&#34;&gt;以內插多項式來做數值積分&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要找到一些插值點使得以內插多項式來做數值積分會最準. 這就是高斯積分.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-任意給定一可積分函數-fx-xin-1-1-我們想要算-int1_-1-fx-dx&#34;&gt;Goal: 任意給定一可積分函數 $f(x)$, $x\in[-1, 1]$, 我們想要算 $\int^1_{-1} f(x) dx$.&lt;/h4&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;複習一下內插多項式的積分&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;給定任意 $n+1$ 個不重複的點在 $[-1, 1]$ 區間, 記為 $x_0, x_1, \cdots, x_n.$ 我們可以逼近函數 $f(x)$ 的積分
$$
\int^1_{-1} f(x)dx \approx \sum^n_{i=0} w_i f_i,
$$
其中 $f_i = f(x_i)$, 並且
$$
w_i = \int^1_{-1} \ell_i(x)dx = \int^1_{-1} \left(\prod^n_{k=0, k\ne i} \frac{x-x_k}{x_i-x_k}\right) dx.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一個重要的 remark: 如果 $p(x)$ 是個 $n$ 次或以下的多項式, 那這個積分是等號!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是說
$$
\int^1_{-1} p(x)dx = \sum^n_{i=0} w_i f_i,
$$
where $p(x)$ is a polynomial with degree less than or equals to $n$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;idea&#34;&gt;Idea:&lt;/h4&gt;
&lt;p&gt;高斯積分的想法就是, 我想要找到特別的點 $x_i$ 以及特別的權重 $w_i$ 使得我能算準的多項式次數最高.&lt;/p&gt;
&lt;p&gt;如果我們回想一下內插多項式, 給定點然後求 $n+1$ 個權重, 也就是有 $n+1$ 個未知數, 那顯然它所能滿足的方程式就是 $n+1$ 個. 如果要把多項式算準的話從常數($0$次)到 $n$次共可以列出剛好 $n+1$個方程式. 所以最多能把 $n$ 次多項式算準.&lt;/p&gt;
&lt;p&gt;現在假設點($x_i$)跟權重($w_i$)都是待決定的, 那我就有 $2(n+1)$ 個未知數, 能夠把多項式從常數($0$次)到 $2n+1$次都算準. 所以理論上, $n+1$ 個點的高斯積分可以將 $2n+1$次的多項式算準, 完全沒有誤差.&lt;/p&gt;
&lt;p&gt;那如果這這樣的想法直接列式的話就會得到以下 $2n+1$ 個方程式
$$
\int^1_{-1} x^m dx=\frac{1 - (-1)^{m+1}}{m+1} =  \sum^n_{i=0} w_i x^m_i, \quad 0\le m\le 2n+1.
$$
然後解以上的非線性聯立方程式就可以得到所有的點 $x_i$ 及權重 $w_i$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;看起來有點困難&lt;/strong&gt;, 不過好消息是其實不難解.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;我們需要引進正交多項式 (orthogonal polynomials)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在線性代數裡我們知道兩個向量如果正交則其內積等於零. 函數空間裡我們也可以做一樣的定義, 我們定義兩個函數的內積
$$
(f, g) = \int^1_{-1} f(x)g(x) dx.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;$f(x)$ and $g(x)$ are orthogonal if $(f,g)=0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們知道 $n$ 次多項式有一組最基本的基底 ${1, x, x^2, \cdots, x^n}$, 將這組基底以 Gram-Schimidt 做正交化就可以得到一組正交基底. 這組正交基底就是所謂的 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Legendre_polynomials&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Legendre polynomials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;我們將 Legendre polynomials 這組正交基底記為 $P_i(x)$, 其前三個如下:
$$
P_0(x) = 1, \quad P_1(x) = x, \quad P_2(x)=\frac{1}{2}(3x^2-1).
$$&lt;/p&gt;
&lt;p&gt;Legendre polynomials 有以下兩個性質:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P_i(x)$ 是一個 $i$次多項式&lt;/li&gt;
&lt;li&gt;$(P_i(x), P_j(x)) = \delta_{ij} = \begin{cases}
0, \quad i\ne j, \\&lt;br&gt;
1, \quad i=j.
\end{cases}$
&lt;ul&gt;
&lt;li&gt;對任何多項式 $Q(x)$ 如果其次數小於 $i$, 則有 $(P_i(x), Q(x))=0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;高斯積分的點與權重&#34;&gt;高斯積分的點與權重:&lt;/h4&gt;
&lt;p&gt;任意給定一個 $2n+1$ 次多項式 $p(x)$, 我們可以透過多項式的除法把它分解成以下這種樣子
$$
p(x) = P_{n+1}(x)Q(x) + R(x),
$$
其中 $Q(x)$ 以及 $R(x)$ 都是至多 $n$ 次的多項式. 透過這樣的分解我們發現
$$
\int^1_{-1} p(x)dx = \int^1_{-1} P_{n+1}(x)Q(x) + R(x)dx = \int^1_{-1} R(x)dx,
$$
其中 $P_{n+1}$ 與 $Q$ 相乘後的積分等於零是用到上面正交多項式的性質.&lt;/p&gt;
&lt;p&gt;以上這積分如果以數值積分來表示的話就是
$$
\int^1_{-1} p(x)dx \approx \sum^n_{i=0} w_i p(x_i) = \sum^n_{i=0} w_i \left(P_{n+1}(x_i)Q(x_i) + R(x_i)\right).
$$
我們希望數值積分也會有 &amp;ldquo;$P_{n+1}$ 與 $Q$ 相乘的積分等於零&amp;rdquo; 這件事. 那一個最簡單的取法就是我們取 $x_i$ 是 $P_{n+1}(x)$ 這個 $n+1$ 次多項式所有的根. 由於 $P_{n+1}$ 剛好有 $n+1$ 個不重複的根, 這樣我們就有 $x_i$ 這些點了, 而且 $P_{n+1}(x_i)=0$ 表示
$$
\sum^n_{i=0} w_i P_{n+1}(x_i)Q(x_i)=0,
$$
所以
$$
\int^1_{-1} p(x)dx \approx \sum^n_{i=0} w_i R(x_i),
$$
也就是說我們只需要把 $R(x)$ 這個 $n$ 次多項式算準就好. 那我們就可以用內插多項式算權重的方法把權重都寫下來
$$
w_i = \int^1_{-1} \left(\prod^n_{k=0, k\ne i} \frac{x-x_k}{x_i-x_k}\right) dx.
$$&lt;/p&gt;
&lt;p&gt;如此, 我們就把&lt;code&gt;點&lt;/code&gt;跟&lt;code&gt;權重&lt;/code&gt;都求出來了.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary:&lt;/h3&gt;
&lt;p&gt;最後總結一下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$n+1$ 個點的高斯積分可以將 $2n+1$ 次多項式算準無誤差&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高斯點是正交多項式 $P_{n+1}(x)$ 的 $n+1$ 個根&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;求出高斯點後我們以內插多項式的做法來求出權重&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;舉個簡單的例子來試試&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設我們只想要用兩個點來估算積分, 那 Legendre polynomial 的 $P_2(x)=\frac{1}{2}(3x^2-1)$.&lt;/p&gt;
&lt;p&gt;由此可算出兩個高斯點, 也就是 $P_2$ 的兩個根是
$$
x_0 = -\frac{1}{\sqrt{3}}, \quad x_1 = \frac{1}{\sqrt{3}}.
$$
接著權重也可以簡單的算出來, 我們得到.
$$
w_0 = w_1 = 1.
$$
因此, 兩個點的高斯積分公式是
$$
\int^1_{-1} f(x)dx = f\left(-\frac{1}{\sqrt{3}}\right) + f\left(\frac{1}{\sqrt{3}}\right),
$$
可以將 3次(含)以下的多項式算準無誤差.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;再延伸一下&#34;&gt;再延伸一下&lt;/h4&gt;
&lt;p&gt;對於更廣義有權重的積分
$$
\int^1_{-1} w(x)f(x)dx,
$$
我們只要找出相對應的正交多項式, 我們一樣可以求出其高斯積分點以及權重.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;舉例來說, $w(x)=\frac{1}{1+x^2}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其相對應的內積以及積分為
$$
(f,g) = \int^1_{-1} \frac{1}{1+x^2}f(x)g(x)dx, \quad \int^1_{-1} \frac{1}{1+x^2}f(x)dx.
$$
其正交多項式為 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Chebyshev_polynomials&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chebyshev polynomials&lt;/a&gt;, $T_i(x)$, 前三項為:
$$
T_0(x) = 1, \quad T_1(x) =x, \quad T_2(x) = 2x^2-1.
$$
比較特別的是其高斯點, 稱為 Gauss-Chebyshev points, 可以直接求出來:
$$
x_i = \cos\left(\frac{(i+\frac{1}{2})\pi}{n+1}\right), \quad 0\le i\le n.
$$
而且權重也可以直接求出來:
$$
w_i = \frac{\pi}{n+1}.
$$&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>數值積分初探</title>
      <link>https://teshenglin.github.io/post/2019_numerical_integration/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2019_numerical_integration/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;在微積分課程裡我們有學到積分的&#39;中點法&amp;rsquo;, &amp;lsquo;梯形法&#39;以及&#39;辛普森法&amp;rsquo;. 這裡我們簡介一些基本概念並且引進高斯積分法.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-任意給定一可積分函數-fx-xin-1-1-我們想要算-int1_-1-fx-dx&#34;&gt;Goal: 任意給定一可積分函數 $f(x)$, $x\in[-1, 1]$, 我們想要算 $\int^1_{-1} f(x) dx$.&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 對任一個積分式 $\int^b_{a} g(x),dx$ 我們皆可用變數變換來將此積分轉到 $[-1, 1]$ 這個區間. 所以我們只需考慮 $[-1, 1]$ 即可.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;首先我們討論一下哪種方法比較準. 一個最簡單的判斷方法是看積分法能不能將多項式算準.&lt;/p&gt;
&lt;p&gt;我們考慮三種積分法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Midpoint rule: $\int^1_{-1}f(x),dx\approx 2f(0)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trapezoidal rule: $\int^1_{-1}f(x),dx\approx f(-1)+f(1)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simpson&amp;rsquo;s rule: $\int^1_{-1}f(x),dx\approx \frac{1}{3}(f(-1)+4f(0)+f(1))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;我們先檢驗常數函數: $\int^1_{-1} 1,dx=2$. 很快我們就可以發現以上三種方法都可以得出完全正確的答案.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接著我們檢驗一次函數: $\int^1_{-1} x,dx=0$. 一樣, 三種方法都得出正確答案.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二次函數: $\int^1_{-1} x^2,dx=\frac{2}{3}$. 對於二次函數我們發現, 中點法以及梯形法都無法算出正確答案, 只有辛普森法能成功.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;三次函數: $\int^1_{-1} x^3,dx=0$. 對於三次函數我們只檢驗辛普森法, 發現三次函數辛普森法也能算準.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;四次函數: $\int^1_{-1} x^4,dx=\frac{2}{5}$. 對於四次函數, 辛普森法就算錯了.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;小結論&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中點跟梯形法準確率相同, 能準確的算出任意一次多項式的積分值.&lt;/li&gt;
&lt;li&gt;辛普森法比中點及梯形法更準確, 並且它能準確的算出任意三次多項式的積分值.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;接著我們稍微推廣一下, 並且介紹所謂的&#39;高斯積分&amp;rsquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 假設我們現在想要用函數在 $x=0$ 這個點的值來估算積分, 那係數要取多少會最好?&lt;/p&gt;
&lt;p&gt;也就是說我們假設
$$\int^1_{-1}f(x),dx\approx A_0f(0),$$
其中 $A_0$ 是個常數. 那 $A_0$ 要選哪個數字, 這個積分公式才會算的最準?&lt;/p&gt;
&lt;p&gt;依照我們之前的想法, 要讓積分式準就是要讓他把多項式算準. 所以我們希望他至少能把常數函數算準. 這樣的話很容易就看出 $A_0=2$, 也就是&#39;中點法&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 假設我們現在想要用函數在 $x=-1$ 以及 $x=1$ 這兩個點的值來估算積分, 那係數要取多少會最好?&lt;/p&gt;
&lt;p&gt;也就是說我們假設
$$\int^1_{-1}f(x),dx\approx \alpha f(-1)+\beta f(1),$$
其中 $\alpha, \beta$ 是常數.&lt;/p&gt;
&lt;p&gt;我們有兩個未知數, 所以我們希望他至少能把常數及一次函數算準. 這樣的話我們可以列出方程式:
$$
\alpha + \beta = 2, \quad -\alpha + \beta = 0.
$$
解方程式我們得到 $\alpha=\beta=1$, 也就是梯形法.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 假設我們現在只想用兩個點的值來估算積分, 那怎樣做會最好?&lt;/p&gt;
&lt;p&gt;也就是說我們假設
$$\int^1_{-1}f(x),dx\approx A_0 f(x_0)+A_1 f(x_1),$$
其中 $A_0, A_1, x_0, x_1$ 都待決定.&lt;/p&gt;
&lt;p&gt;我們有四個未知數, 所以理想狀況下應該能從常數到三次多項式都能算準, 依此可以列出四個方程式:
$$
\begin{aligned}
A_0 + A_1 &amp;amp;= 2 \\\&lt;br&gt;
A_0x_0 + A_1x_1 &amp;amp;= 0 \\\&lt;br&gt;
A_0x^2_0 + A_1x^2_1 &amp;amp;= \frac{2}{3} \\\&lt;br&gt;
A_0x^3_0 + A_1x^3_1 &amp;amp;= 0.
\end{aligned}
$$
解方程式我們得到 $A_0=A_1=1$, $x_0=-\frac{1}{\sqrt{3}}$, $x_1=\frac{1}{\sqrt{3}}$. 也就是說&lt;/p&gt;
&lt;p&gt;$$
\int^1_{-1} f(x),dx \approx f\left(-\frac{1}{\sqrt{3}}\right)+ f\left(\frac{1}{\sqrt{3}}\right).
$$
這就是所謂的兩點高斯積分公式 (two-point Gaussian quadrature formula).&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;最後我們看一下以數值積分來估算的例子.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們想計算 $\int^1_{-1} e^x,dx$, 當然我們知道答案是 $e^1-e^{-1}$, 不過我們想要知道若以上列幾種方法估算會有多準確.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 一般來說我們會先將所要積分的範圍分成 $n$ 等分, 接著在每一等分上使用上列積分法. 這就是微積分課本中所介紹的合成積分法 (composite integral rules). 實作上的細節我們就不在這裡討論.&lt;/p&gt;
&lt;p&gt;我們將 $[-1, 1]$ 區間均勻分成 $n$ 等分, 再用三種不同積分法來估算積分值. 下表列出不同積分法在不同區間數所得之值與實際值之間的絕對誤差.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;n\Method&lt;/th&gt;
&lt;th&gt;Midpoint&lt;/th&gt;
&lt;th&gt;Trapezoidal&lt;/th&gt;
&lt;th&gt;Simpson&amp;rsquo;s&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;3.91e-3&lt;/td&gt;
&lt;td&gt;7.83e-3&lt;/td&gt;
&lt;td&gt;2.08e-5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;3.92e-5&lt;/td&gt;
&lt;td&gt;7.83e-5&lt;/td&gt;
&lt;td&gt;2.09e-9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;3.92e-7&lt;/td&gt;
&lt;td&gt;7.83e-7&lt;/td&gt;
&lt;td&gt;2.10e-13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我們可以看到, 中點以及梯形法誤差皆是以 $O(n^{-2})$ 來下降, 也就是點數變十倍時誤差會降一百倍, 而辛普森法則是 $O(n^{-4})$.&lt;/p&gt;
&lt;p&gt;接著我們試試看用高斯積分來做, 這裡我們就不分小區間, 直接做整個 $[-1, 1]$ 區間. 多個點的高斯積分的公式可在 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_quadrature&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki&lt;/a&gt; 找到. 我們將 $m$-point 高斯積分所得之值與實際值之間的絕對誤差列於下表&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;m\Method&lt;/th&gt;
&lt;th&gt;Gaussian quadrature rule&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7.71e-3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6.55e-5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2.95e-7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8.25e-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1.56e-12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.66e-15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;4.44e-16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以看到我們用 8 個點就可以將這個積分準確估計到誤差接近機器的捨入誤差 (rounding error).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;延伸閱讀: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_2&#34;&gt;以內差多項式來做數值積分&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;延伸閱讀: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_numerical_integration_3&#34;&gt;高斯積分&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Fixed point iteration</title>
      <link>https://teshenglin.github.io/post/2019_fixed_point/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2019_fixed_point/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們介紹固定點迭代法 (Fixed point iteration)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先我們介紹什麼是固定點 (Fixed point)&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;definition-fixed-point&#34;&gt;Definition: Fixed point&lt;/h2&gt;
&lt;p&gt;A fixed point of a function $f(x)$ is a number $c$ in its domain such that $f(c)=c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以簡單來說, 把固定點這個數字丟進函數後會得到同樣的一個數字. 所以稱之為固定點.&lt;/p&gt;
&lt;p&gt;那固定點重要性其一是在數值計算上有一種迭代方式叫做固定點迭代(Fixed point iteration). 假設我們想要求某個函數的固定點, 也就是滿足 $c=f(c )$ 的這些 $c$, 那我們可以定義一個迭代式
$$
x_{n+1} = f(x_n).
$$&lt;/p&gt;
&lt;p&gt;如果夠幸運的, $\{x_{n}\}$ 這串數字收斂了, 那把它收斂到的數字稱為 $\bar{c}$ 我們就有 $\bar{c}=f(\bar{c})$, 也就是固定點.&lt;/p&gt;
&lt;p&gt;舉個例子來說, 假設我們想要解 $x=\cos(x)$ 這個方程式, 那我們可以定義一個固定點迭代為
$$
x_{n+1} = \cos(x_n).
$$&lt;/p&gt;
&lt;p&gt;這樣的話如果數列收斂, $\{x_{n}\}\to \bar{x}$, 那我們就有 $\bar{x}=\cos(\bar{x})$, 那就解出來了!!&lt;/p&gt;
&lt;p&gt;不過這裡有兩個問題.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;為什麼這個數列會收斂?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原方程式的固定點迭代其實有無窮多種改寫方式, 例如也可寫為 $x_{n+1} = \cos^{-1}(x_n)$. 如果收斂的話一樣會是原方程式的解. 那, 哪種改寫方式最好呢?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我們有以下這個定理&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem&#34;&gt;Theorem&lt;/h2&gt;
&lt;p&gt;If $f:[a, b]\to [a,b]$ is a differentiable function such that
$$ |f&amp;rsquo;(x)|\leq \alpha&amp;lt;1, \quad \forall x\in[a, b],$$
then $f$ has exactly one fixed point $c$ and the fixed point iteration converges to $c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;h3 id=&#34;sketch-not-complete-please-full-in-the-details-by-yourself&#34;&gt;(Sketch, not complete, please full-in the details by yourself)&lt;/h3&gt;
&lt;h4 id=&#34;existence&#34;&gt;existence&lt;/h4&gt;
&lt;p&gt;Since the domain and the range of $f$ are both $[a, b]$, by Intermediate Value Theorem, there exists $c$ such that $c=f(c ).$&lt;/p&gt;
&lt;h4 id=&#34;uniqueness&#34;&gt;uniqueness&lt;/h4&gt;
&lt;p&gt;If there exits another fixed point $\bar{c}$, $\bar{c}\ne c$, such that $\bar{c}=f(\bar{c} )$, then according to Mean Value Theorem(MVT), there exits $z$ between $c$ and $\bar{c}$ such that $$f&amp;rsquo;(z) = \frac{f(c ) - f(\bar{c})}{c-\bar{c}}=\frac{c - \bar{c}}{c-\bar{c}} = 1,$$ which violate the assumption that $|f&#39;|\leq \alpha&amp;lt;1$. So the fixed point is unique.&lt;/p&gt;
&lt;h4 id=&#34;convergence-of-fixed-point-iteration&#34;&gt;convergence of fixed point iteration&lt;/h4&gt;
&lt;p&gt;Based on MVT we have
$$|x_{n+1} - c| = |f(x_n) - f(c )| = |f&amp;rsquo;(c_i)(x_n-c)|\leq\alpha |x_n-c|\leq\alpha^n|x_1-c|\to 0.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以如果在函數固定點 $c$ 的微分小於 $1$, 那就存在一個包含 $c$ 的小區間使得函數的微分都在這區間內小於 $1$, 那根據這定理固定點迭代就會收斂.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;固定點迭代是求根問題(root finding problems)中很重要的一種迭代方式. 舉個例子來說, 假設我們想要找 $g(x)$ 這個函數的根, 那我們可以定義
$$
f(x) = x + g(x).
$$
這樣的話 $f$ 的固定點就會是 $g$ 的根了. 不過這種最簡單的改寫方式完全不保證會收斂.&lt;/p&gt;
&lt;p&gt;那要如何改寫才會比較好呢? 其中一個最有名的就是牛頓法 (Newton&amp;rsquo;s method):&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;newtons-iteration&#34;&gt;Newton&amp;rsquo;s iteration&lt;/h2&gt;
&lt;p&gt;$$x_{n+1} = x_n - \frac{g(x_n)}{g&amp;rsquo;(x_n)}.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們可以定義 $f(x) = x - \frac{g(x)}{g&amp;rsquo;(x)}$, 這樣上面這個式子就是個固定點迭代. 接著我們可以發現, 如果 $c$ 是 $g$ 函數的根, 也就是 $g(c )=0$, 那 $f&amp;rsquo;(c ) = 0$. 所以根據上面的定理就存在某個包含 $c$ 的小區間使得迭代會收斂.&lt;/p&gt;
&lt;p&gt;更進一步我們可以利用泰勒展開式(Taylor&amp;rsquo;s series expansion) 來證明牛頓法事實上有二次收斂,
$$
|x_{n+1} - c| \approx \beta |x_n-c|^2.
$$
這個證明我們這邊就先略過不寫. 不過接著我們來看一下牛頓法究竟有多快.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;依之前的例子我們想要解 $x=\cos(x)$, $x\in[0, \pi]$. 最簡單的固定點迭代為
$$
x_{n+1} = \cos(x_n),
$$
也就是求 $g(x) = \cos(x)-x$ 的根. 我們以 $x_0=1$ 當初始值, 發現需要 $80$ 個迭代才能使誤差在 $10^{-14}$ 之下.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    x_new= 0.5403023058681398		error= 0.31725090997825367
    x_new= 0.8575532158463934		error= 0.2032634253486143
    x_new= 0.6542897904977791		error= 0.13919056824478648
    x_new= 0.7934803587425656		error= 0.0921115851198091
    x_new= 0.7013687736227565		error= 0.06259090927789768
    ⋮
    x_new= 0.7390851332151851		error= 4.0967229608668276e-14
    x_new= 0.7390851332151441		error= 2.7644553313166398e-14
    x_new= 0.7390851332151718		error= 1.865174681370263e-14
    x_new= 0.7390851332151531		error= 1.2545520178264269e-14
    x_new= 0.7390851332151657		error= 8.43769498715119e-15
    Total number of iterations=80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;若是使用牛頓法迭代則迭代式為
$$
x_{n+1} = x_n + \frac{cos(x_n)- x_n}{\sin(x_n)+1}.
$$
一樣以 $x_0=1$ 當初始值, 發現只需要 $4$ 個迭代就能使誤差在 $10^{-14}$ 之下. 比上個例子快上許多.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    x_new= 0.7503638678402439		error= 0.018923073822117442
    x_new= 0.7391128909113617		error= 4.6455898990771516e-5
    x_new= 0.739085133385284		error= 2.847205804457076e-10
    x_new= 0.7390851332151607		error= 0.0
    Total number of iterations=4
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary:&lt;/h3&gt;
&lt;p&gt;最後總結一下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;固定點迭代要收斂, 至少在固定點的微分值必須比 $1$ 小.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要取迭代函數, 如果知道如何對函數微分, 以牛頓法 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Newton&amp;rsquo;s method&lt;/a&gt; 來取通常會有不錯的效果.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若無法得知微分函數, 可以用數值微分來逼近真實微分, 這樣會得到割線法 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Secant_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;secant method&lt;/a&gt;, 收斂速度比牛頓法慢一點點.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;固定點定理保證在區間裡任意取點當初使迭代都會收斂, 不過要滿足定理的條件很強, 實務上不容易做到.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;例如牛頓法, 我們能證明一定存在某個區間滿足固定點定理, 不過實際上這個區間有多大並不知道. 因此一般在討論牛頓法時都會要求初始值要離實際要求的固定點&amp;quot;夠近&amp;rdquo;. 至於&amp;quot;夠近&amp;quot;什麼意思就只能用嘗試的.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由於牛頓法不保證收斂, 因此實務上要求根時會與一些保證收斂的方法, 如二分逼進法 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bisection_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(bisection method)&lt;/a&gt; 來合作. 如 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Brent%27s_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brent-Dekker method&lt;/a&gt;. 這樣既能保證收斂, 又兼有收斂快速的優點.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>用電腦算微分</title>
      <link>https://teshenglin.github.io/post/2019_derivate_evaluate/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2019_derivate_evaluate/</guid>
      <description>&lt;p&gt;前情提要: 
&lt;a href=&#34;https://teshenglin.github.io/post/2019_limit_evaluate&#34;&gt;用電腦算極限&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要講的是用數值計算來算函數的微分值.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;已知一個函數 $f(x)$ 在某個點 $a$ 的微分值定義是
$$
f&amp;rsquo;(a) = \lim_{h\to 0} \frac{f(a+h)-f(a)}{h}.
$$&lt;/p&gt;
&lt;p&gt;我們用一個簡單的例子試試看. 假設我們想求 $f(x)=x^2$ 在 $x=\pi$ 的微分. 根據定義我們有&lt;/p&gt;
&lt;p&gt;$$
f&amp;rsquo;(\pi) = \lim_{h\to 0} \frac{(\pi+h)^2-\pi^2}{h}.
$$&lt;/p&gt;
&lt;p&gt;接著我們將 $h$ 取靠近 $0$ 的 $11$ 的點並帶入上列這個式子試著來算其極限值. &lt;code&gt;Julia&lt;/code&gt; 程式如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;); h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; h[&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;];
fp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((pi&lt;span style=&#34;color:#f92672&#34;&gt;.+&lt;/span&gt;h)&lt;span style=&#34;color:#f92672&#34;&gt;.^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;.-&lt;/span&gt; pi&lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;./&lt;/span&gt;h;
hcat(h, fp, fp&lt;span style=&#34;color:#f92672&#34;&gt;.-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;結果如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$h$&lt;/th&gt;
&lt;th&gt;$f&#39;$&lt;/th&gt;
&lt;th&gt;$f&amp;rsquo;-2\pi$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;6.29319&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.009&lt;/td&gt;
&lt;td&gt;6.29219&lt;/td&gt;
&lt;td&gt;0.009&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;td&gt;6.29119&lt;/td&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.007&lt;/td&gt;
&lt;td&gt;6.29019&lt;/td&gt;
&lt;td&gt;0.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;6.28919&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;6.28819&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;6.28719&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;td&gt;6.28619&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;6.28519&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;6.28419&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;上列數字最左邊是 $h$ 值, 中間為估計的微分值. 我們發現的確這個數字會越來越接近真實的解, 也就是 $2\pi$, 約等於 $6.283185307179586$. 最右邊為這個估算值與真實值 $2\pi$ 之間的差. 的確, 當 $h$ 越接近零, 這個估計出來的微分值離 $2\pi$ 的距離越來越小.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 用數值計算微分能有多精確? 這個誤差能不能一直遞減下去?&lt;/p&gt;
&lt;p&gt;接著我們取更多靠近 $0$ 的點來計算微分的極限值, 我們列出其與真實值 $2\pi$ 之間的差, 並且把它畫出來.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;); h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.^&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;h[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;]);
fp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((pi&lt;span style=&#34;color:#f92672&#34;&gt;.+&lt;/span&gt;h)&lt;span style=&#34;color:#f92672&#34;&gt;.^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;.-&lt;/span&gt; pi&lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;./&lt;/span&gt;h;
hcat(h, fp, abs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(fp&lt;span style=&#34;color:#f92672&#34;&gt;.-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;結果如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$h$&lt;/th&gt;
&lt;th&gt;$f&#39;$&lt;/th&gt;
&lt;th&gt;$abs(f&amp;rsquo;-2\pi)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;6.38319&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;6.29319&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;6.28419&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;6.28329&lt;/td&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-5&lt;/td&gt;
&lt;td&gt;6.2832&lt;/td&gt;
&lt;td&gt;9.99998e-6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-6&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;td&gt;1.00006e-6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-7&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;td&gt;9.5898e-8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-8&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;td&gt;4.26073e-8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-9&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;td&gt;2.20243e-7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-10&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;td&gt;1.9966e-6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-11&lt;/td&gt;
&lt;td&gt;6.28315&lt;/td&gt;
&lt;td&gt;3.35305e-5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-12&lt;/td&gt;
&lt;td&gt;6.28297&lt;/td&gt;
&lt;td&gt;0.000211166&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-13&lt;/td&gt;
&lt;td&gt;6.27054&lt;/td&gt;
&lt;td&gt;0.0126457&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-14&lt;/td&gt;
&lt;td&gt;6.39488&lt;/td&gt;
&lt;td&gt;0.111699&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-15&lt;/td&gt;
&lt;td&gt;5.32907&lt;/td&gt;
&lt;td&gt;0.954115&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0e-16&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;6.28319&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;觀察最後一下發現, 當 $h$ 很小時微分竟然與真實質差更多, 更不準了!! 比如說當 $h=10^{-14}$ 時, 誤差竟然大到約是 $10^{-1}$.&lt;/p&gt;
&lt;p&gt;我們把它畫出來看看, &lt;code&gt;Julia&lt;/code&gt; code 如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Plots
plot(log10&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(h), log10&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(abs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(fp&lt;span style=&#34;color:#f92672&#34;&gt;.-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi)),label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error&amp;#34;&lt;/span&gt;)
xlabel!(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log10(h)&amp;#34;&lt;/span&gt;)
ylabel!(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log10(Error)&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;發現雖然誤差在 $h$ 大的時候遞減, 不過當 $h$ 接近零的時候卻又遞增上去了.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2019_derivative_evaulate_01.svg&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2019_derivative_evaulate_01.svg&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;那誤差最小值出現在什麼時候呢?&lt;/p&gt;
&lt;p&gt;我們發現當 當 $h=1.0*10^{-8}$ 時, 其估計出來的微分值離真實值誤差最小, 其誤差為 $4.26 *10^{-8}$.&lt;/p&gt;
&lt;p&gt;不過, WHY?? 為什麼誤差不會一直往下遞減? 其實這也是因為 &lt;strong&gt;捨入誤差(rounding-error)&lt;/strong&gt; 的關係.&lt;/p&gt;
&lt;p&gt;觀察一下我們的式子
$$
\frac{f(a+h)-f(a)}{h}
$$
當我們在用數值計算這個式子的時候其實並不完全是這樣子, 在分子應該會有捨入誤差在, 也就是說, 其實我們看到的數字應該是以下這個式子算出來的
$$
\frac{f(a+h)-f(a) + \epsilon}{h}
$$
其中的 $\epsilon$ 就是捨入誤差. 所以, 我們計算的時候會多出了 $\frac{\epsilon}{h}$ 這麼多.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;泰勒展開式&#34;&gt;泰勒展開式&lt;/h4&gt;
&lt;p&gt;再深一點來說, 我們可以利用泰勒展開式知道
$$
\frac{f(a+h)-f(a)}{h} = f&amp;rsquo;(a) + \frac{h}{2}f&amp;rsquo;&#39;(\xi), \quad a\leq \xi \leq a+h.
$$
這個式子告訴我們, 用這方式算微分誤差應該是 $\frac{h}{2}f&amp;rsquo;&#39;(\xi) = O(h)$, 誤差會隨著 $h$ 減少而線性變小.&lt;/p&gt;
&lt;p&gt;數學上我們有以上這個等式, 而數值計算上則是有以下這個等式
$$
\frac{f(a+h)-f(a) + \epsilon}{h} = f&amp;rsquo;(a) + \frac{h}{2}f&amp;rsquo;&#39;(\xi) + \frac{\epsilon}{h}, \quad a\leq \xi \leq a+h.
$$
也就是說, 真正的誤差公式應該是
$$
\frac{h}{2}f&amp;rsquo;&#39;(\xi) + \frac{\epsilon}{h},
$$
當 $h$ 非常小的時候 $\frac{\epsilon}{h}$ 這項就會變很大.&lt;/p&gt;
&lt;p&gt;比如說, 依我們之前所算的 $\epsilon\approx 10^{-16}$, 那當 $h=10^{-8}$ 時, 算出來的數字會多了大約 $\frac{10^{-16}}{10^{-8}} = 10^{-8}$.&lt;/p&gt;
&lt;p&gt;而當 $h=10^{-14}$ 時, 算出來的數字會多了大約 $\frac{10^{-16}}{10^{-14}} = 10^{-2}$. 跟我們之前所發現的完全吻合!! 而這也就是為什麼當 $h$ 很靠近零的時候誤差會上升的原因.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;optimal-h&#34;&gt;Optimal $h$&lt;/h4&gt;
&lt;p&gt;那給定一個微分公式, 要怎麼知道 $h$ 能小到什麼程度呢? 一個簡單的感覺是這樣的, 由於誤差的第一項  $\frac{h}{2}f&amp;rsquo;&#39;(\xi)$ 會隨著 $h$ 變小而變小, 第二項 $\frac{\epsilon}{h}$ 則會變大, 因此整體最小值約會發生在兩項交叉時, 也就是當
$$
h \sim \frac{\epsilon}{h},
$$
(由於我們不知道 $f&amp;rsquo;&#39;(\xi)/2$ 是多少, 簡單起見設成 1). 上式稍微計算一下發現誤差最小值約發生在 $h=10^{-8}$, 誤差最小值則約為 $10^{-8}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary:&lt;/h3&gt;
&lt;p&gt;最後總結一下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;我們可以用數值計算來估計一個函數在某點的微分值
$$
f&amp;rsquo;(a) \approx \frac{f(a+h)-f(a)}{h}.
$$
這樣的做法稱為&lt;strong&gt;有限差分法 (finite difference method)&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不過計算時 $h$ 值不能無限取小, 需考慮到捨入誤差的影響.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>用電腦算極限</title>
      <link>https://teshenglin.github.io/post/2019_limit_evaluate/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2019_limit_evaluate/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們要介紹如何用電腦算極限, 以及我們來看一下當我們真的這樣做的時候有可能會發生什麼問題. 我們以 $sinc$ 函數為例來做說明.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sinc-function&#34;&gt;sinc function&lt;/h3&gt;
&lt;p&gt;首先我們要介紹一個特別的函數, $sinc(x)$, 定義如下:
$$
sinc(x) = \frac{\sin(x)}{x}, \quad x\ne 0.
$$
很明顯可以看出來當 $x=0$ 的時候分母會等於零, 是一件壞事, 所以把 $x=0$ 這個點先拿掉.&lt;/p&gt;
&lt;p&gt;比較有趣的是我們可以把這個函數畫出來. 首先我們在 $[-20, 20]$ 這個區間取 $1000$ 個點, 然後帶入上面 $sinc$ 函數的定義, 再把所有點連起來看看.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 有件事需要先說明一下, 由於我們是在 $[-20, 20]$ 這個區間均勻的取偶數個點, 所以會有 $500$ 個正數以及 $500$ 個負數, 重點是保證不會取到 $x=0$ 這個點, 所以沒有問題. 相反的, 如果取奇數個點就一定會取到 $x=0$, 那就會有函數無定義的問題了.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/output_3_0.svg&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/output_3_0.svg&#34; alt=&#34;&#34; width=&#34;400px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;我們很輕易可以看出來, 連起來的線還蠻&amp;quot;光滑&amp;quot;的. 函數值在 $x=0$ 附近似乎不會趨近正無窮大或負無窮大, 也沒有跳躍的現象. 接著我們試著在 $x=0$ 附近放大一點看看, 我們在 $[-0.1, 0.1]$ 這個區間取 $1000$ 個點, 然後帶入 $sinc$ 函數的定義再把它畫出來:&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/output_5_0.svg&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/output_5_0.svg&#34; alt=&#34;&#34; width=&#34;400px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;看起來真的很光滑!! 而且似乎當 $x$ 很靠近 $0$ 時, $sinc(x)$ 的值很靠近 $1$.&lt;/p&gt;
&lt;p&gt;接著我們取一個會越來越靠近 $0$ 的數列, 然後看一下當把 $sinc$ 函數在這個數列的點上取值時, 其值會不會越來越靠近 $1$.  我們取以下數列:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 1.670170079024566e-5
 6.14421235332821e-6
 2.2603294069810542e-6
 8.315287191035679e-7
 3.059023205018258e-7
 1.1253517471925912e-7
 4.139937718785167e-8
 1.522997974471263e-8
 5.602796437537268e-9
 2.061153622438558e-9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;算一下 $sinc$ 函數在這些點上面的值, 並觀察他的趨勢:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 0.9999999999535089
 0.9999999999937081
 0.9999999999991485
 0.9999999999998848
 0.9999999999999845
 0.9999999999999979
 0.9999999999999997
 1.0
 1.0
 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;赫然發現算到後來就等於 $1$ 了!! 所以我們發現&lt;/p&gt;
&lt;p&gt;$$
\lim_{x\to 0} sinc(x) = \lim_{x\to 0} \frac{\sin(x)}{x} = 1.
$$&lt;/p&gt;
&lt;p&gt;不過有一點點詭異的是, 在剛剛的計算裡我們最多也只是取到離 $0$ 很近的點而已, 但是算出來的結果卻是 $1$. 難道不只是 $sinc(0)=1$, 我們也有 $sinc(2.061153622438558 *10^{-9})=1$ 嗎?&lt;/p&gt;
&lt;p&gt;事實上並不是這樣. 電腦有所謂的捨入誤差(rounding error). 這是因為電腦需要用有限位數來表達無窮小數, 所以一定要捨棄後面的位數. 我們把算出來的數字減去 $1$ 看看:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; -4.649114426769074e-11
 -6.291855925155687e-12
 -8.515410598874951e-13
 -1.1524114995609125e-13
 -1.554312234475219e-14
 -2.1094237467877974e-15
 -3.3306690738754696e-16
  0.0
  0.0
  0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我們可以發現這個數字最小可以到大約 $10^{-16}$, 之後就變成 $0$ 了. 也就是說我們目前用個這個程式語言其捨入誤差大約就是 $10^{-16}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;machine-epsilon&#34;&gt;machine epsilon&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;一般我們會用 machine epsilon 這個數字來量化在電腦裡浮點運算的捨入誤差.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;machine epsilon 的定義是, 考慮正數$\epsilon$, 使得 $ 1+ \epsilon \ge 1$ 中最小的那個稱之為 machine epsilon. 當然以數學來看這個 machine epsilon 必須等於零. 不過在電腦裡並不是這樣.&lt;/p&gt;
&lt;p&gt;為了方便我們稍微改一下定義, 我們考慮 $\epsilon = 2^{-k}$ 這種形式, 然後 machine epsilon 一樣是使得 $ 1+ \epsilon \ge 1$ 中最小的那個. &lt;code&gt;julia&lt;/code&gt; 程式如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;s&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
    s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; s&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;s)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;
        s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;s
        println(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k=&amp;#34;&lt;/span&gt;, k&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,  eps=&amp;#34;&lt;/span&gt;, s)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在我的電腦上我發現 $\epsilon = 2^{-52} = 2.220446049250313e-16$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k=52,  eps=2.220446049250313e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以的確捨入誤差大約是 $10^{-16}$ 這個等級.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary:&lt;/h3&gt;
&lt;p&gt;稍微總結一下目前我們看到的:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;我們用程式跑數值發現 $sinc(x\to 0)=1$, 所以我們可以定義 sinc 函數為
$$
sinc(x) =
\begin{cases}
\frac{\sin(x)}{x}, \quad x\ne 0, \\\&lt;br&gt;
1, \quad x=0.
\end{cases}
$$
在這樣的定義之下 $sinc$ 函數是個&lt;code&gt;連續函數&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更多關於 $sinc$ 函數的性質可以參考 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Sinc_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki&lt;/a&gt; 上的介紹.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在數值計算上有所謂的捨入誤差, 這是用有限位元來表達無限位數一定會有的差異.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以用 machines epsilon 來量化捨入誤差.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
