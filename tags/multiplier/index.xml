<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>multiplier | Te-Sheng Lin</title>
    <link>https://teshenglin.github.io/tags/multiplier/</link>
      <atom:link href="https://teshenglin.github.io/tags/multiplier/index.xml" rel="self" type="application/rss+xml" />
    <description>multiplier</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 12 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>multiplier</title>
      <link>https://teshenglin.github.io/tags/multiplier/</link>
    </image>
    
    <item>
      <title>Lagrange Multiplier - 01</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;在微積分課程裡我們有學到如何利用 Lagrange multiplier 來解 constraint optimization 問題. 這邊要介紹課本裡沒教的 Lagrangian function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;goal-我們想要解以下這個問題&#34;&gt;Goal: 我們想要解以下這個問題&lt;/h4&gt;
&lt;p&gt;$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=0.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Observation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;微積分課本告訴我們一個相對好懂的直觀, 就是這個解會發生在兩個等高線相切的地方, 因此在這個問題的解那個點的位置, 兩個函數 $f$ 與 $g$ 等高線的法向量會平行, 因此可以列出以下兩個式子
$$
\partial_x f + \lambda \partial_x g = 0, \quad  g(x)=0.
$$
這裡我們引進一個新的未知數 $\lambda$, 即稱為 Lagrange multiplier.
因為有兩個方程式, 因此我們可以解出兩個未知數 $x$ 以及 $\lambda$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;上列的這個方程組有另一種相對好記的方式, 就是引進所謂的 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x)
$$
這是一個雙變數方程式, 而原本問題的解會發生在這個方程的 critical point, 也就是會滿足 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;要注意的是由於 $L$ 是個雙變數函數, 所以 $\nabla = (\partial_x, \partial_{\lambda})$. 根據這樣的定義我們將 $\nabla L=0$ 方程式列出來會得到跟上面一模一樣的兩個方程組.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 1:&lt;/strong&gt;  $\partial_{x} L = \partial_x f + \lambda \partial_x g$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 2:&lt;/strong&gt;  $\partial_{\lambda} L = g(x)$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;引進 Lagrangian function 比較好記是因為假設我有更多的 constraints, 例如我想解以下這個問題:
$$
\min_{x} f(x), \quad \text{subject to } \quad  g_1(x)=0, \quad g_2(x)=0.
$$
要找一個函數最小值不過有兩個限制條件.&lt;/p&gt;
&lt;p&gt;這樣的話我們照之前的步驟, 先引進 Lagrangian function
$$
L(x, \lambda_1, \lambda_2) = f(x) + \lambda_1 g_1(x) + \lambda_2 g_2(x)
$$
然後原問題的最佳解會發生在 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;要注意的是這次的 $L$ 是個三變數函數, 所以 $\nabla L = (\partial_x, \partial_{\lambda_1}, \partial_{\lambda_2})$. 然後將 $\nabla L=0$ 寫下來就是
$$
\partial_x f + \lambda_1 \partial_x g_1 + \lambda_2 \partial_x g_2 = 0, \quad  g_1(x)=0, \quad g_2(x)=0.
$$
就得到我們需要解的方程組了!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接著我們試著將以上寫的更廣義一點, 考慮一個 $n$ 維度的最佳化問題有 $m$ 個限制式, 我們引進一些符號: $\pmb{x}\in \mathbb{R}^n$,  $\pmb{g}(\pmb{x}):\mathbb{R}^n\to\mathbb{R}^m$, 這個有限制的最佳化問題即可寫成
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x}), \quad \text{subject to } \quad  \pmb{g}(\pmb{x})=\pmb{0}.
$$&lt;/p&gt;
&lt;p&gt;我們接著引進 Lagrangian function
$$
L(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T \pmb{g}(\pmb{x}), \quad \pmb{\lambda}\in\mathbb{R}^m
$$&lt;/p&gt;
&lt;p&gt;原問題的最佳解會發生在 $\nabla L=0$, 也就是 $(\nabla_{\pmb{x}} L, \nabla_{\pmb{\lambda}} L) = (0, 0)$, 也就是
$$
\nabla_{\pmb{x}} f(\pmb{x}) + \pmb{\lambda}^T \nabla_{\pmb{x}}\pmb{g}(\pmb{x}) = 0, \quad \pmb{g}(\pmb{x})=\pmb{0}.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;所以為什麼要引進 Lagrange multiplier 或是 Lagrangian function?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrange multiplier 的好處是我們將一個&lt;strong&gt;最佳化問題&lt;/strong&gt;改寫成一個&lt;strong&gt;求根問題&lt;/strong&gt;. 理論上 $F(x)=0$ 這種問題不管線性或非線性我們都比較會解. 而最佳化問題就相對比較無從下手.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BUT&lt;/strong&gt;, 付出的代價是我們增加了維度! 原本 $n$ 維最小值問題變成 $n+m$ 維求根問題.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrangian function 最明顯的好處是&lt;strong&gt;比較好記&lt;/strong&gt;, 不管是&lt;code&gt;記&lt;/code&gt;在頭腦裡或是做筆&lt;code&gt;記&lt;/code&gt;寫下來. 整個問題很輕易的就記成 $\nabla L=0$.&lt;/p&gt;
&lt;p&gt;當然還有其他更重要的好處, 就是可以推導出所謂的 dual optimization problem: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_lagrange_multiplier_2&#34;&gt;Lagrange Multiplier - 02&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lagrange multiplier $\lambda$ 感覺起來很像一點用都沒有, 就是為了解出問題過程所引進的虛擬變數. 其實它還是有一點意義的, 可見: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_lagrange_multiplier_3&#34;&gt;Lagrange Multiplier - 03&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier - 02</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier_2/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們再多討論一點 Lagrangian function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們先看最簡單的一維問題, 求一個有限制式的函數最小值問題:
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=0.
$$&lt;/p&gt;
&lt;p&gt;我們引進 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x)
$$
並且知道原問題的解發生在 $\nabla L=0$ 的地方.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; 從微積分我們知道 $\nabla L=0$ 是 $L(x, \lambda)$ 這個函數 critical point發生的地方, 那這個 critical point 究竟是 local max/min 還是 saddle 呢?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;要回答這問題其實很簡單, 我們看這個二維函數的判別式就知道. 這函數的變數有兩個分別是 $x$ 以及 $\lambda$, 所以判別式是
$$
L_{xx}L_{\lambda\lambda} - L_{x\lambda}^2 = -(g_x)^2 \le 0
$$
所以我們知道原最佳化問題的解其實是這個 Lagragian function 的 saddle point (至少不是 local max/min)!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Constraint optimization problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們事實上可以考慮更一般的問題
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x) \le 0.
$$
這樣的問題我們可以定義一樣的 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda g(x).
$$&lt;/p&gt;
&lt;p&gt;我們有以下這個定理:&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;theorem---saddle-point-sufficient-condition&#34;&gt;Theorem - Saddle point (Sufficient condition)&lt;/h2&gt;
&lt;p&gt;Let $P$ be a constraint optimization problem.
If $(x^+, \lambda^+)$ is a saddle point, that is,
$$\forall x\in\mathbb{R}, \forall\lambda\ge 0, \quad L(x^+, \lambda)\le L(x^+, \lambda^+)\le L(x, \lambda^+),$$
then it is a solution of $P$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個證明很簡單.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;根據不等式第一個部分
$$\forall\lambda\ge 0, \quad L(x^+, \lambda)\le L(x^+, \lambda^+)$$
帶入 $L$ 我們得到 $\lambda g(x^+) \le \lambda^+ g(x^+)$.&lt;/p&gt;
&lt;p&gt;由於對所有 $\lambda\ge 0$ 都對, 我們讓 $\lambda\to\infty$ 得到 $g(x^+)\le 0$.
接著讓 $\lambda\to 0$ 我們得到 $0\le \lambda^+ g(x^+)\le 0$, 因此 $\lambda^+ g(x^+)=0$.&lt;/p&gt;
&lt;p&gt;有這件事後我們再看不等式第二部分, $L(x^+, \lambda^+)\le L(x, \lambda^+)$.
代入 $L$ 以及以上結果我們有 $f(x^+)\le f(x) + \lambda^+ g(x)$.&lt;/p&gt;
&lt;p&gt;因此, 若 $g(x)\le 0$, 我們必定有 $f(x^+)\le f(x)$.&lt;/p&gt;
&lt;p&gt;QED.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h4 id=&#34;可以看到在上面有個隱藏的假設是-lambdage-0-可以被很輕易地看出來所以這裡就不記下了&#34;&gt;可以看到在上面有個隱藏的假設是 $\lambda\ge 0$. 可以被很輕易地看出來所以這裡就不記下了.&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;更進一步, 我們可以引進這個 Lagrange dual function:
$$
F(\lambda) = \inf_{x} L(x, \lambda)
$$
也就是固定一個 $\lambda$ 我去找 $L$ 這函數的最大下界(有點像最小值不過不一定要碰到). 據此我們可以引進以下的 dual problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dual optimization problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\max_{\lambda} F(\lambda), \quad \lambda \ge 0.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;舉個例子&#34;&gt;舉個例子&lt;/h4&gt;
&lt;p&gt;我們考慮這個特別的 constraint optimization 問題
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x}), \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0},
$$
其中 $A$ 是個矩陣而 $\pmb{b}$ 是個向量. 也就是說限制式為 $g(\pmb{x}) = A\pmb{x}-\pmb{b}$.&lt;/p&gt;
&lt;p&gt;把 Lagrangian function 寫下來就是
$$
L(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T(A\pmb{x}-\pmb{b}).
$$
對於 dual problem 我們可以構造一個迭代法來解這問題.&lt;/p&gt;
&lt;p&gt;由 Dual optimization problem 我們知道對於 $F$ 這個函數我們要求其最大值. 所以我們試著朝 gradient 方向走來往上走, 意即
$$
\lambda^{k+1} = \lambda_k + \alpha^k\nabla_{\lambda} F(\lambda^k),
$$
其中 $\alpha^k$ 是步長. 而且我們知道 $\nabla_{\lambda} F(\lambda^k) = A\pmb{\tilde{x}}-\pmb{b},$ 其中 $\pmb{\tilde{x}} = argmin \ L(\pmb{x}, \pmb{\lambda}^k)$. 因此我們有以下這方法:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dual ascent iteration method:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步: 固定一個 $\pmb{\lambda}$ 我們解一個 optimization 問題
$$
\pmb{x}^{k+1} = argmin \ L(\pmb{x}, \pmb{\lambda}^k)
$$
&lt;strong&gt;Remark:&lt;/strong&gt; 這是一個沒有 constraint 的最小值問題, 可以用 gradient descent 或各式方法來解.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二步: 接著我們用 gradient ascent 來更新 $\pmb{\lambda}$
$$
\lambda^{k+1} = \lambda^k + \alpha^k(A\pmb{x}^{k+1}-\pmb{b})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;迭代下去我們可以得到原問題的解!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;augmented-lagrangian&#34;&gt;Augmented Lagrangian&lt;/h2&gt;
&lt;p&gt;一個很有趣的更進一步推廣是將 Lagrangian 引入一個新的 penalty term:
$$
L_{\rho}(\pmb{x}, \pmb{\lambda}) = f(\pmb{x}) + \pmb{\lambda}^T(A\pmb{x}-\pmb{b}) + \frac{\rho}{2}\|A\pmb{x}-\pmb{b}\|^2_2,
$$
其中 $\rho$ 是個常數.&lt;/p&gt;
&lt;p&gt;我們可以反過來將這個 Lagrangian 所對應的限制最佳化問題寫下來:
$$
\min_{\pmb{x}\in\mathbb{R}^n}f(\pmb{x})+\frac{\rho}{2}\|A\pmb{x}-\pmb{b}\|^2_2, \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0}.
$$
我們可以輕易發現, 加了這新的一項對於這整個問題的最佳解完全沒有影響! 也就是說 $L_{\rho}$ 所得到的解就是原本 $L$ 的解.&lt;/p&gt;
&lt;p&gt;接著我們一樣用 dual ascent iteration method 來解這問題（給定 $\pmb{x}^{k}$ 以及 $\pmb{\lambda}^{k}$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Method of multiplier:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步: 我們解一個 optimization 問題
$$
\pmb{x}^{k+1} = argmin \ L_{\rho}(\pmb{x}, \pmb{\lambda}^k)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二步: 接著我們用 gradient ascent 來更新 $\pmb{\lambda}$
$$
\pmb{\lambda}^{k+1} = \pmb{\lambda}^k + \rho(A\pmb{x}^{k+1}-\pmb{b})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 這裡有個有趣的事, 原本的方法中步長是不知道的需要自己決定, 不過在這裡我們卻直接設定步常為 $\rho$. 接下來我們就是要解釋這部分.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一步中, 如果 $\pmb{x}^{k+1}$ 是個解那它要滿足 $\nabla_{\pmb{x}} L_{\rho}=0$, 其中整理一下發現
$$
\nabla_{\pmb{x}} L_{\rho}=\nabla_{\pmb{x}} f(\pmb{x}^{k+1}) + A^T\left(\pmb{\lambda}^k + \rho (A\pmb{x}^{k+1}-\pmb{b})\right)=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所以, 如果第二步中的步長我們設定為 $\rho$, 那上式就可以改寫為
$$
\nabla_{\pmb{x}} f(\pmb{x}^{k+1}) + A^T\pmb{\lambda}^{k+1}=0.
$$
有趣的是, 對原本的 Lagrangian 而言我們有
$$
\nabla_{\pmb{x}} L=\nabla_{\pmb{x}} f(\pmb{x}) + A^T\pmb{\lambda},
$$
所以我們這樣設定步長, 剛好使得我們每次算出的 $\pmb{x}^{k+1}$ 以及 $\pmb{\lambda}^{k+1}$ 滿足
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^{k+1})=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;再把以上兩個方法 summarize 一下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本問題是
$$
\min_{\pmb{x}} f(\pmb{x}), \quad \text{subject to } \quad  A\pmb{x}-\pmb{b}=\pmb{0}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;引進 Lagrangian 後變成我們要解以下方程
$$
\nabla_{\pmb{x}} L =\nabla_{\pmb{x}} f(\pmb{x}) + A^T\pmb{\lambda}=0, \quad \nabla_{\pmb{\lambda}} L=A\pmb{x}-\pmb{b}=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們使用迭代法來解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若用 &lt;strong&gt;dual ascent iteration method&lt;/strong&gt;, 則有
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^k)=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若用 &lt;strong&gt;method of multiplier&lt;/strong&gt;, 則有
$$
\nabla_{\pmb{x}} L(\pmb{x}^{k+1}, \pmb{\lambda}^{k+1})=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對於兩種方法, 原問題的 Constraint 都在 $k\to\infty$ 滿足
$$
\lim_{k\to\infty} A\pmb{x}^k-\pmb{b}=0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange Multiplier - 03</title>
      <link>https://teshenglin.github.io/post/2020_lagrange_multiplier_3/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_lagrange_multiplier_3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們討論一下 Lagrange multiplier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們知道, 如果想要解以下這個有限制式的最佳化問題
$$
\min_{x} f(x), \quad \text{subject to } \quad  g(x)=k,
$$
一個方式是引進 Lagrange multiplier, $\lambda$, 然後可以列出以下兩個式子
$$
\partial_x f + \lambda \partial_x g = 0, \quad  g(x)=k.
$$&lt;/p&gt;
&lt;h4 id=&#34;question&#34;&gt;Question:&lt;/h4&gt;
&lt;p&gt;解出聯立方程後我們得到 $x^+$, $\lambda^+$, 帶入原函數得到 $f^+=f(x^+)$.
我們知道 $f^+$ 是最佳值, 而 $x^+$ 是最佳解. 那 $\lambda^+$ 是做什麼用的?
看起來只是為了解出原問題所引進的虛擬變數. 它有什麼作用嗎?&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;先說結論, $\lambda^+ = -\frac{df^+}{dk}$. 也就是告訴我們如果稍微改變限制式中的 $k$ 值, 那原題的最佳值會有怎樣的變化.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;我們先引進 Lagrangian function
$$
L(x, \lambda) = f(x) + \lambda (g(x) - k)
$$
並且知道原問題的解發生在 $\nabla L=0$ 的地方, 意即, $\nabla L(x^+, \lambda^+)=0$.
然後我們有
$$
L^+ = L(x^+, \lambda^+) = f(x^+) + \lambda^+(g(x^+)-k) = f^+.
$$&lt;/p&gt;
&lt;p&gt;在這裡我們要稍微小心一點. 固定一個 $k$ 值, 我們就有一個最佳化問題, 然後就可以解出 $x^+$ 跟 $\lambda^+$.
而當 $k$ 值改變時, $x^+$, $\lambda^+$, 甚至 $f^+$ 以及 $L^+$ 也都會跟著改變, 所以我們想像這四個值都是 $k$ 的函數:
$$
x^+ = x^+(k), \ \lambda^+ = \lambda^+(k), \ f^+ = f^+(k), \ L^+ = L^+(k).
$$
再寫清楚一點就是
$$
L^+(k) = \left.L(x, \lambda,k)\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}
$$&lt;/p&gt;
&lt;p&gt;其中
$$
L(x, \lambda,k) = f(x) + \lambda (g(x) - k).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; 讀到這邊也許會覺得有個奇怪的地方, 就是為什麼要把 $L$ 從雙變數變成三變數函數.
原因是當寫成 $L(x, \lambda)$ 時 $k$ 是個常數, 是不能變的. 想要改變 $k$ 值需要把它也當成一個變數比較合理.&lt;/p&gt;
&lt;p&gt;接著我們就可以微分
$$
\frac{d L^+}{dk} = \left.\frac{\partial L}{\partial x}\frac{d x}{dk} + \frac{\partial L}{\partial \lambda}\frac{d \lambda}{dk} + \frac{\partial L}{\partial k}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k} = -\lambda^+.
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;
$$
\left.\frac{\partial L}{\partial x}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}=
\left.\frac{\partial L}{\partial \lambda}\right|_{x=x^+(k), \lambda=\lambda^+(k),k=k}=0.
$$&lt;/p&gt;
&lt;p&gt;由於 $f^+ =L^+$, 所以我們也可以得到 $\frac{d f^+}{dk} = -\lambda^+$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;舉個例子&#34;&gt;舉個例子&lt;/h4&gt;
&lt;p&gt;以下單位皆為&lt;code&gt;元&lt;/code&gt; or &lt;code&gt;萬元&lt;/code&gt; or &lt;code&gt;千萬元&lt;/code&gt;, 請自行依喜好帶入!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;假設我如果花費 $x$ 買進某一股票在一定時間後賣出可獲得 $2x$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假設我如果花費 $y$ 買進某一貴金屬在一定時間後賣出可獲得 $10y-y^2$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假設我總共可運用的財產只有 $10$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;那我應該花多少錢買股票花多少錢買貴金屬, 才能有最佳獲利呢?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們可以列出以下限制最佳化問題&lt;/p&gt;
&lt;p&gt;$$
\max_{x,y} f(x,y)=2x+(10y-y^2), \quad \text{subject to } \quad  x+y=10,
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;先引進 Lagrangian funtion
$$
L(x,y,\lambda) = 2x+(10y-y^2)+\lambda(x+y-10)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列出方程式 $\nabla L=0$:
$$
\frac{\partial L}{\partial x} = 2 + \lambda =0, \quad
\frac{\partial L}{\partial y} = 10-2y + \lambda =0, \quad
\frac{\partial L}{\partial \lambda} = x+y-10 =0.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解出得到
$$
x^+ = 6, \quad y^+=4, \quad \lambda^+=-2.
$$
也就是我們若以 $6$ 單位買進股票, $4$ 單位買進貴金屬, 一定時間賣出後可獲利 $f^+ = 36$ 單位.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所以 $\lambda^+=-2$ 告訴我們 $\frac{d f^+}{dk}=-\lambda^+=2$, 意思就是如果我們增加一點 $k$ 值, 那獲利會是所增加數目的&lt;strong&gt;兩倍&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原題改成我目前可運用的財產有 $11$ (也就是多增加了 $1$), 那會發現最佳解是 $x^+=7$, $y^+=4$, $f^+=38$, 獲利真的增加 $2$ 單位了!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果這是真實情形, 那就告訴我們說如果去借 $1$ 單位的錢, 可以賺到 $2$ 單位. 如果借的錢加上利息會小於 $2$ 單位, 那就是個很好的投資, 應該去借.&lt;/li&gt;
&lt;li&gt;這例子比較特殊 $\lambda^+$ 恆等於 $2$, 所以才會那麼巧資金增加一單位獲利就增加兩單位. 一般而言這應該只是 linear appxoimation, 只有 $k$ 增加一點點時才會大約兩倍. 增加太多就不知道了.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;references&#34;&gt;References:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/m-G3K2GPmEQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube: Meaning of the Lagrange multiplier&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/b9B2FZ5cqbM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube: Proof for the meaning of Lagrange multipliers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
