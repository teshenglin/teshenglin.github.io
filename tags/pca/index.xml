<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PCA | Te-Sheng Lin</title>
    <link>https://teshenglin.github.io/tags/pca/</link>
      <atom:link href="https://teshenglin.github.io/tags/pca/index.xml" rel="self" type="application/rss+xml" />
    <description>PCA</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 18 Dec 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>PCA</title>
      <link>https://teshenglin.github.io/tags/pca/</link>
    </image>
    
    <item>
      <title>主成分分析 - 0</title>
      <link>https://teshenglin.github.io/post/2023_principal_component_analysis_0/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_principal_component_analysis_0/</guid>
      <description>&lt;h1 id=&#34;principle-component-analysis---0&#34;&gt;Principle component analysis - 0&lt;/h1&gt;
&lt;h2 id=&#34;1-一維資料的統計學&#34;&gt;1. 一維資料的統計學&lt;/h2&gt;
&lt;p&gt;假設我們有 $n$ 筆資料, 每筆資料都是一個數字 (例如 $n$ 個學生的成績). 這 $n$ 筆資料我們設為 $x_1, \cdots, x_n$, 並且定義一個矩陣&lt;/p&gt;
&lt;p&gt;$$
\tag{1}
A =
\begin{bmatrix}
x_1 &amp;amp; \cdots &amp;amp; x_n
\end{bmatrix}\in M_{1\times n}.
$$&lt;/p&gt;
&lt;p&gt;那這些資料的平均數為&lt;/p&gt;
&lt;p&gt;$$
\tag{2}
\mu = \frac{1}{n}(x_1+\cdots+x_n) = \frac{1}{n}\begin{bmatrix}
1 &amp;amp; \cdots &amp;amp; 1
\end{bmatrix}\begin{bmatrix}
x_1\\
\vdots\\&lt;br&gt;
x_n
\end{bmatrix} = \frac{1}{n}\mathbb{1}^TA^T,
$$&lt;/p&gt;
&lt;p&gt;其中 $\mathbb{1}^T = [1, \cdots, 1]$ 是個全為 $1$ 的向量.&lt;/p&gt;
&lt;p&gt;資料的變異數 (variance) 則定義為&lt;/p&gt;
&lt;p&gt;$$
\tag{3}
\text{Var}(A) = \sigma^2 = \frac{1}{n}\sum^n_{k=1} (x_i - \mu)^2,
$$&lt;/p&gt;
&lt;p&gt;而 $\sigma$ 則是標準差 (standard deviation, std).&lt;/p&gt;
&lt;p&gt;要將之寫成矩陣形式首先我們定義置中矩陣 (centering matrix)&lt;/p&gt;
&lt;p&gt;$$
\tag{4}
H = I - \frac{1}{n}\mathbb{1}\mathbb{1}^T.
$$&lt;/p&gt;
&lt;p&gt;計算一下可以發現&lt;/p&gt;
&lt;p&gt;$$
\tag{5}
HA^T = (I - \frac{1}{n}\mathbb{1}\mathbb{1}^T)A^T = A^T - \mu\mathbb{1} = \begin{bmatrix}
x_1 -\mu\\&lt;br&gt;
\vdots\\&lt;br&gt;
x_n -\mu
\end{bmatrix} = Y^T,
$$&lt;/p&gt;
&lt;p&gt;其中我們將這些置中後的資料存為 $Y$ 矩陣. 接著我們就可以知道&lt;/p&gt;
&lt;p&gt;$$
\tag{6}
\sigma^2 = \frac{1}{n}YY^T.
$$&lt;/p&gt;
&lt;h2 id=&#34;2-二維資料的統計學&#34;&gt;2. 二維資料的統計學&lt;/h2&gt;
&lt;p&gt;假設我們有 $n$ 筆資料, 每筆資料都是 $2$ 個數字 (例如 $n$ 個學生在 $2$ 次考試的成績), 這兩個數字我們稱之為這筆資料的 features. 這 $n$ 筆資料我們設為 $(x_1, y_1), \cdots, (x_n, y_n)$, 並且定義一個矩陣&lt;/p&gt;
&lt;p&gt;$$
\tag{7}
A =
\begin{bmatrix}
x_1 &amp;amp; \cdots &amp;amp; x_n\\&lt;br&gt;
y_1 &amp;amp; \cdots &amp;amp; y_n
\end{bmatrix}\in M_{2\times n}.
$$&lt;/p&gt;
&lt;p&gt;我們把每種資料都平移, 使其平均為 $0$, 並令平移後的資料為 $Y$. 簡單計算可以發現我們一樣可以用置中矩陣來做, $HA^T = Y^T$,&lt;/p&gt;
&lt;p&gt;$$
\tag{8}
Y^T = \begin{bmatrix}
x_1 -\mu_x &amp;amp; y_1 -\mu_y\\
\vdots &amp;amp; \vdots\\&lt;br&gt;
x_n -\mu_x &amp;amp; y_n -\mu_y
\end{bmatrix}, \quad
\mu_x = \frac{1}{n}\sum^n_{k=1} x_k, \quad
\mu_y = \frac{1}{n}\sum^n_{k=1} y_k.
$$&lt;/p&gt;
&lt;p&gt;接著我們可以定義兩個變數的共變異數 (covariance),&lt;/p&gt;
&lt;p&gt;$$
\tag{9}
\text{cov}(x, y) = \frac{1}{n}\sum^n_{k=1}(x_k - \mu_x)(y_k-\mu_y).
$$&lt;/p&gt;
&lt;p&gt;接著計算一下就可以發現&lt;/p&gt;
&lt;p&gt;$$
\tag{10}
\frac{1}{n}YY^T = \begin{bmatrix}
\text{cov}(x,x) &amp;amp; \text{cov}(x,y)\\
\text{cov}(x,y) &amp;amp; \text{cov}(y,y)
\end{bmatrix},
$$&lt;/p&gt;
&lt;p&gt;也就是所謂的共變異數矩陣. 這個矩陣的對角線元素代表每個 feature 自己的變異數, 而非對角線則是共變異數, 代表兩個 features 的相關程度.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;/em&gt;: 不過要真正算相關程度會更近一步的去計算相關係數 (correlation coefficients), 這邊就不再深入探討.&lt;/p&gt;
&lt;h2 id=&#34;3-pca-maximize-variance&#34;&gt;3. PCA: maximize variance&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要找到一個方向, 使得資料投影上去之後, 新資料的變異數會最大.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設這個方向為 $v\in\mathbb{R}^2$, 並且 $\|v\|=1$, 那資料投影到 $v$ 所得的新資料就是 $v^TY \in M_{1\times n}$. 接著我們就來算這筆新資料的變異數.&lt;/p&gt;
&lt;p&gt;第一步一樣先置中. 也就是計算 $HY^Tv$. 不過由於 $Y$ 是置中過的資料, 因此 $HY^T = Y^T$, 所以 $HY^Tv = Y^Tv$, 也就是說, 新資料 $v^TY$ 的平均一定是 $0$.&lt;/p&gt;
&lt;p&gt;接著, 新資料的變異數就會是&lt;/p&gt;
&lt;p&gt;$$
\tag{11}
\sigma^2 =\frac{1}{n}(v^TY)(v^TY)^T =\frac{1}{n}v^TYY^Tv.
$$&lt;/p&gt;
&lt;p&gt;所以統整一下, 若我們將資料投影到 $v$ 上, 那新資料的變異數就是 (11). 而PCA 所要找的方向就是使變異數最大的方向, 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{12}
\hat{v} = \arg\max_{v\in\mathbb{R}^2, \|v\|=1} \left(v^TYY^Tv\right).
$$&lt;/p&gt;
&lt;p&gt;最後, 我們知道這個解可以由 $Y^T$ 這個矩陣的 SVD 得出.&lt;/p&gt;
&lt;p&gt;假設 $Y^T = U\Sigma V^T$, 那  $\hat{v} = v_1$, 也就是第一個 singular vector.&lt;/p&gt;
&lt;h2 id=&#34;4-pca-minimize-square-distance&#34;&gt;4. PCA: minimize square distance&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我們想要找到一個方向, 使得資料投影上去之後, 新舊資料的距離平方和最小.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設這個方向為 $v\in\mathbb{R}^2$, 並且 $\|v\|=1$, 那資料投影到 $v$ 的投影點就是 $vv^TY \in M_{2\times n}$. 接著我們就來算新舊資料的距離平方和:&lt;/p&gt;
&lt;p&gt;$$
\tag{13}
\begin{align}
\sum^n_{k=1} d_k^2 &amp;amp;= \sum^n_{k=1}\|Y_k - vv^TY_k\|^2  \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}&amp;lt;Y_k-vv^TY_k, Y_k-vv^TY_k&amp;gt; \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}Y_k^TY_k -Y_k^Tvv^TY_k \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}Y_k^TY_k -v^TY_kY_k^Tv \\&lt;br&gt;
&amp;amp;= \sum^n_{k=1}(Y_k^TY_k) -v^TYY^Tv,
\end{align}
$$&lt;/p&gt;
&lt;p&gt;其中我們用到了 $v^TY_k$ 是個數字以及 $\sum Y_kY_k^T = YY^T$ 這兩件事.&lt;/p&gt;
&lt;p&gt;我們希望找到一個方向使得距離平方和最小, 也就是&lt;/p&gt;
&lt;p&gt;$$
\tag{14}
\hat{v}=\arg\min_{v\in\mathbb{R}^2, \|v\|=1} \left(\sum^n_{k=1}(Y_k^TY_k) -v^TYY^Tv\right) = \arg\max_{v\in\mathbb{R}^2, \|v\|=1} \left(v^TYY^Tv\right).
$$&lt;/p&gt;
&lt;p&gt;觀察 (12) 與 (14) 發現他們一模一樣. 也就是這兩個問題的解會是一樣的, 最佳的方向都是第一個 singular vector.&lt;/p&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PCA 想做的事就是找到一個仿射子空間 (affine subspace), 使得
&lt;ul&gt;
&lt;li&gt;投影下去之後的資料有最大的變異數&lt;/li&gt;
&lt;li&gt;投影前後的資料距離平方合最小.
&lt;blockquote&gt;
&lt;p&gt;而這兩件事情是等價的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PCA 也是一種資料降維的工具, 而將資料投影到一維所出來的新資料就是 $v^TY$.&lt;/li&gt;
&lt;li&gt;以上雖然是以二維資料為例, 不過若有 $m$ 維資料整個推導是一樣的.&lt;/li&gt;
&lt;li&gt;以上是以投影到一維為例, 若投影到更高維度就是依序找第二, 三, 等等的 singular vectors. 不過推導會利用到矩陣 trace 的一些性質, 一些細節這裡就先跳過.&lt;/li&gt;
&lt;li&gt;PCA 要找的是個仿射子空間, $V = \mu + \text{span}\{v\}$,  這裏我們都直接說 $\mu$ 就是資料的平均. 不過其實這是可以算出來的. 假設我們想要找一個點 $\mu$ 使得所有資料到這個點的距離和為最小, 也就是
$$
\tag{15}
\mu = \arg\min \sum^n_{k=1}(x_k - \mu)^2.
$$
我們先定義 $f(\mu) = \sum^n_{k=1}(x_k - \mu)^2$. 這是個單變數函數, 而且其實就是個 $\mu$ 的二次多項式, 首項係數等於 $1$, 有一個最小值. 接著微分求極值得到
$$
\tag{16}
\frac{d}{d\mu} f =  \sum^n_{k=1}(-2)(x_k - \mu) = (-2)\left[\sum^n_{k=1}x_k - n\mu\right]
$$
因此極值發生在 $\frac{d}{d\mu} f=0$, 也就是
$$
\tag{17}
\mu = \frac{1}{n}\sum^n_{k=1}x_k.
$$
所以到所有資料點距離和最小的就是平均數.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析 - code</title>
      <link>https://teshenglin.github.io/post/2023_principal_component_analysis_01/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2023_principal_component_analysis_01/</guid>
      <description>&lt;h3&gt;&lt;a href=&#34;https://teshenglin.github.io/pybooks/pca.html&#34;&gt;Click here to view this notebook in full screen&lt;/a&gt;&lt;/h3&gt;
 &lt;iframe src=&#34;https://teshenglin.github.io/pybooks/pca.html&#34;
        style=&#34;height:2750px;width:100%;border:none;overflow:hidden;&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Diffusion maps</title>
      <link>https://teshenglin.github.io/post/2021_diffusion_maps/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2021_diffusion_maps/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;擴散映射, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Diffusion_map&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion maps&lt;/a&gt; (以下簡稱 DM), 是個資料分析, 流型學習或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要介紹以 &lt;code&gt;julia&lt;/code&gt; 來做 diffusion maps 降維.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;algorithm---diffusion-maps-embeding&#34;&gt;Algorithm - diffusion maps embeding&lt;/h2&gt;
&lt;p&gt;先簡單介紹一下作法.&lt;/p&gt;
&lt;p&gt;假設我們有 $n$ 筆 $d$ 維的資料,
$$
\{x_1, x_2, \cdots, x_n\} \in R^d.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;1-affinity-matrix-k&#34;&gt;1. Affinity matrix $K$&lt;/h4&gt;
&lt;p&gt;我們需要先定一個 $K$ 矩陣, $K_{ij}=k(x_{i},x_{j})$, 一般而言我們使用 Guassian kernel
$$
k(x,y) = e^{-\frac{\lVert x-y \rVert^2}{\sigma^2}},
$$
其中 $\sigma$ 是個常數. 這樣造出來的 $K$ 矩陣會是個 $n\times n$ 對稱半正定的矩陣.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-normalized-affinity-matrix-q&#34;&gt;2. Normalized affinity matrix $Q$&lt;/h4&gt;
&lt;p&gt;接著我們要定 diffusion matrix $P$, 其定義為
$$
P=D^{-1}K,
$$
其中 $D$ 是個只有對角線有值的矩陣, 其元素為相對應 $K$ 矩陣的 rowsum, $D_{ii} = \sum^n_{j=1} K_{ij}$.
因此可以知道 $P$ 矩陣其實就是將 $K$ 矩陣的每個 row 做 normalize 的動作, 使其 rowsum 等於 $1$.&lt;/p&gt;
&lt;p&gt;$P$ 這矩陣可看成是個機率矩陣, 其第 $i$ 個 row 表示從 $x_{i}$ 這個點跳到其他點的機率分佈.&lt;/p&gt;
&lt;p&gt;接著我們考慮 $Q$ 矩陣, 定義為
$$
Q=D^{-\frac{1}{2}}KD^{-\frac{1}{2}}.
$$
我們可以很輕易發現 $P$ 以及 $Q$ 兩個矩陣有完全相同的 eigenvalues, 而 $P$ 的 eigenvectors 是
$$
v = D^{-1/2}v_Q,
$$
其中 $v_Q$ 是 $Q$ 的 eigenvector.&lt;/p&gt;
&lt;p&gt;另一個重要觀察是 $Q$ 矩陣跟 $K$ 一樣是對稱半正定的矩陣, 因此它的 eigenvalues 都非負, 而且他的 eigenvalues 跟 singular values 會完全一模一樣.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-求出-p-矩陣的-eigenvalues-跟-eigenvectors&#34;&gt;3. 求出 $P$ 矩陣的 eigenvalues 跟 eigenvectors&lt;/h4&gt;
&lt;p&gt;先將 $Q$ 的 eigenvalues 跟 eigenvectors 都找出來, 接著 $P$ 的 eigenvalues 跟 eigenvectors 就依照上面的公式可以輕易得到.&lt;/p&gt;
&lt;p&gt;我們令 $P$ 的 eigenvalues 跟 eigenvectors 分別為 $\lambda_i$ 跟 $\psi_i$, $1\le i\le n$.&lt;/p&gt;
&lt;p&gt;需要注意的是, $\lambda_1=1$ 並且 $\psi_1$ 是一個常數向量, 因為我們要拿的是從第二個開始.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-define-diffusion-map-y&#34;&gt;4. Define diffusion map $Y$&lt;/h4&gt;
&lt;p&gt;接著我們定義 $Y$ 矩陣為
$$
Y = \left[\lambda_2\psi_2, \lambda_3\psi_3, \cdots, \lambda_n\psi_n\right],
$$
這是個 $n\times (n-1)$ 的矩陣.&lt;/p&gt;
&lt;p&gt;如果我們想要將原始資料投射到 $k$ 維, $k \le (n-1)$, 那我們就只要到第 $k+1$ 個 eigenvector 就好. 比如說要投影到三維, 我們只需要取
$$
Y = \left[\lambda_2\psi_2, \lambda_3\psi_3, \lambda_4\psi_4\right].
$$
而 diffusion maps embedding 的每個點就是這個 $Y$ 矩陣的 row, 也就是 $y_i = Y(i,:)$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in &lt;code&gt;julia&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;接著我們就可以來看要怎樣以 &lt;code&gt;julia&lt;/code&gt; 來做 diffusion maps.&lt;/p&gt;
&lt;p&gt;我們需要以下幾個 packages:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Plots
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Distances
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; LinearAlgebra
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;以下例子為一個 spiral curve, 我們想要看經過 diffusion maps 的投射到二或三維後會長什麼樣子.&lt;/p&gt;
&lt;p&gt;我們先把 spiral curve 上面的點造出來 (取 $300$ 個點)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# generating the data - a spiral curve&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# n: number of sampling&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;;

theta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,stop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi,length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n);
r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,stop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n);
&lt;span style=&#34;color:#75715e&#34;&gt;# x and y-coordinates&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;cos&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(theta);
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;sin&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(theta);
&lt;span style=&#34;color:#75715e&#34;&gt;# X is a n-by-2 data matrix&lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [x y];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;畫出來看看原始 data 長怎樣. 我們照順序將點標為藍色, 紅色以及綠色.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;plot(reshape(X[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(X[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), aspect_ratio&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:equal&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_01.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h4 id=&#34;1-affinity-matrix-k-1&#34;&gt;1. Affinity matrix $K$&lt;/h4&gt;
&lt;p&gt;先來造出 $K$ 矩陣:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;E &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pairwise(Euclidean(), X, dims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
sigma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;;
K &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; exp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(E&lt;span style=&#34;color:#f92672&#34;&gt;.^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;./&lt;/span&gt;(sigma&lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h4 id=&#34;2-normalized-affinity-matrix-q-1&#34;&gt;2. Normalized affinity matrix $Q$&lt;/h4&gt;
&lt;p&gt;接著造出 $Q$ 矩陣, 並求出其 eigenvalues 跟 eigenvectors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q = zeros(n,n);
d_sq = zeros(n);
for ii = 1:n
    d_sq[ii] = sqrt(sum(K[ii,:]));
end
for ii = 1:n
    for jj = 1:n
        Q[ii,jj] = K[ii,jj]/(d_sq[ii]*d_sq[jj]);
    end
end
U,S,V = svd(Q);
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h4 id=&#34;3-求出-p-矩陣的-eigenvalues-跟-eigenvectors-1&#34;&gt;3. 求出 $P$ 矩陣的 eigenvalues 跟 eigenvectors&lt;/h4&gt;
&lt;p&gt;$P$ 矩陣的 eigenvalues 跟 $Q$ 一樣, eigenvectors 也可以依公式得到:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ii &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; n
    V[ii,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V[ii,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;d_sq[ii];
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;將 eigenvalues 畫出來看看. 左圖是第 $2$ 到第 $11$ 個, 右圖則是以 semilogy 畫出全部的 eigenvalues. 可以看出 eigenvalues 遞減的非常快, 所以其實 embedding 不需要取到全部 $(n-1)$ 維.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;p1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; scatter(S[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;], title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;eigenvalues 2:11&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false);
p2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(log&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(S), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;eigenvalues in log&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false);
plot(p1, p2, layout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:png&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_02.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h4 id=&#34;4-define-diffusion-map-y-1&#34;&gt;4. Define diffusion map $Y$&lt;/h4&gt;
&lt;p&gt;最後我們定義 embedding $Y$:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;Y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zeros(n,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;);
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ii &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
    Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,ii] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,ii&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.*&lt;/span&gt;S[ii&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我們知道 $Y$ 的每個 row 就是這個 embedding 的座標.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們將 embedding 畫出來看看. 左圖是 embed 到二維, 右圖則是 embed 到三維.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;p1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2D&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
p2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plot(reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), reshape(Y[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3D&amp;#34;&lt;/span&gt;, leg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false)
plot(p1, p2, layout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:png&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我們一樣照順序將點標為藍色, 紅色以及綠色. 有趣的是, 我們發現這個 embedding 將整個 spiral curve &lt;strong&gt;打開了&lt;/strong&gt;. 因此, 經由 diffusion maps 投射之後我們比較榮以可以看出點資料真正在整個曲線上彼此間距離的遠近.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/DM_spiral_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/DM_spiral_03.png&#34; alt=&#34;&#34; width=&#34;500px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;p&gt;Diffusion maps 也常被拿來搭配 
&lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k-means&lt;/a&gt; 做成分群演算法, 算是 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Spectral_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spectral clustering&lt;/a&gt; 的其中一種.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diffusion maps 的 &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt; 以及 &lt;code&gt;matlab&lt;/code&gt; 程式都可以在這裡找到: 
&lt;a href=&#34;https://github.com/teshenglin/diffusion_maps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github - teshenglin/diffusion_maps&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析 - 2</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis_2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;這裡我們補充一下主成分分析裡的證明部分.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p.
$$
假設想要投影到 $k$ 維, $k\le p$, 數學上來說就是想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;要求極值就是要找微分等於零的解, 由於這式子是 convex, 所以保證找到唯一解而且是最小的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;所以以下做法就是對每個變數做偏微分, 並求出偏微分等於零的解. 這樣就把最佳解找出來了!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;1-首先看-mu&#34;&gt;1. 首先看 $\mu$&lt;/h4&gt;
&lt;p&gt;對 $\mu$ 做偏微分並且利用 $\sum^n_{i=1} \beta_i=\vec{0}$ 我們可以得到
$$
\partial_{\mu} E = -2 \left(\sum_i x_i - n\mu\right)=0.
$$
因此, 最佳的 $\mu$ 是
$$
\mu = \frac{1}{n}\sum_{i=1}^n x_i.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-接著找-beta_i&#34;&gt;2. 接著找 $\beta_i$&lt;/h4&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 我們有
$$
E = \sum_{i=1}^n \|y_i - U\beta_i\|^2.
$$
接著對 $\beta_i$ 微分得到
$$
\partial_{\beta_i} E = 2\left(\beta_i - U^Ty_i\right)=0.
$$
因此可以得到
$$
\beta_i = U^T y_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;,
$$
其中 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-最後找-u&#34;&gt;3. 最後找 $U$&lt;/h4&gt;
&lt;p&gt;代入最佳的 $\beta_i$ 後我們又可將原本的 $E$ 改寫為
$$
E = \sum_{i=1}^n \|y_i - UU^Ty_i\|^2 = \|Y - UU^TY\|^2_F,
$$
其中 $Y=[y_1, \cdots, y_n]$, 而下標 $F$ 代表矩陣的 Frobenius norm.&lt;/p&gt;
&lt;p&gt;我們先定 $P = UU^T$, 則有 $P^T=P$, $P^2=P$. 接著我們改寫 $E$ 為
$$
E = \|Y - PY\|^2_F = trace\left[(Y-PY)^T(Y-PY)\right] = trace\left(Y^TY-Y^TPY\right).
$$
由於 $Y$ 不會變, 因此求 $E$ 的最小值變成求 $trace\left(-Y^TPY\right)$ 的最小值, 也就是求 $trace\left(Y^TPY\right)$ 的最大值, 換回來得到是求 $trace\left(Y^TUU^TY\right)$ 的最大值.&lt;/p&gt;
&lt;p&gt;接著我們用線性代數裡一個定理, $trace(AB)=trace(BA)$, 將原式轉換成求 $trace\left(U^TYY^TU\right)$ 的最大值, 最後得到
$$
\arg\min_U E = \arg\max_U trace\left(U^T\Sigma U\right),
$$
其中 $\Sigma=YY^T$ 也就是原資料的共變異數矩陣. 上式是個非常重要的式子, 它告訴我們以下兩件事是等價的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使得原始資料與投影後的資料之間的&lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;使得資料有&lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最後, 線性代數告訴我們等號右邊這問題的解就是 $\Sigma$ 最大的 $k$ 個 eigenvalues 其相對應的 eigenvectors, 收集起來得到 $U=[u_1, \cdots, u_k]$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以上使用的線性代數結果, 其證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/252272/is-trace-invariant-under-cyclic-permutation-with-rectangular-matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tr(AB)=tr(BA) proof&lt;/a&gt;, 以及 
&lt;a href=&#34;https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;maximize v^T Av&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;總結一下最佳解如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 是原始資料的平均
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將平移後的資料投影到子空間中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, y_i&amp;gt;.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析</title>
      <link>https://teshenglin.github.io/post/2020_principal_component_analysis/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://teshenglin.github.io/post/2020_principal_component_analysis/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;主成分分析, Principal component analysis, 簡稱 PCA, 是個資料分析或是資料降維的工具.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;資料降維簡單來說, 假設我們有一些資料, 這資料中的每一筆維度都很高, 導致我們很難 &amp;ldquo;看出&amp;rdquo; 或 &amp;ldquo;分析&amp;rdquo; 這資料集的特性, 這時候我們就會想要在低維度空間裡(通常三維以下)建構一組點資料, 並且想辦法使新的這組點資料在某些程度上能表示出原本高維度的點資料, 或保有某種特性. 這種將高維度資料在低維度表現出來的方式就稱為資料降維. 而 PCA 就是其中一種線性降維的方式.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;這裡我們要談一下從數學角度來說 PCA 的原理及做法.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;假設我們有 $n$ 筆 $p$ 維的資料, 記成
$$
\{x_1, x_2, \cdots, x_n\} \in R^p
$$
那我們想要做以下等價的兩件事:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到一組低維度的投影並且使得資料有 &lt;em&gt;&lt;strong&gt;最大的變異性&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;找到一組低維度的投影並且使得原始資料與投影後的資料之間的 &lt;em&gt;&lt;strong&gt;距離平方和最小&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;remark&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這裡所謂低維度的投影講精確一點, 事實上是在原本的 $R^p$ 空間中找到一個低維度的仿射子空間(affine subspace), 可以想像成一個不穿過原點的點或直線或平面, 找到之後再把原始資料點投影上去.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-0-維&#34;&gt;Example: 投影到 $0$ 維&lt;/h3&gt;
&lt;p&gt;先舉一個最簡單的例子, 假設我們想要投影到 $0$ 維, 也就是投影到一個點.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也就是說我們想要找一個點來代表整組資料.&lt;/p&gt;
&lt;p&gt;直覺上來想, 如果我要幫一組資料找一個最具代表性的點, 應該就會用 &amp;ldquo;平均數&amp;rdquo; 或 &amp;ldquo;中位數&amp;rdquo; 來當這個點.&lt;/p&gt;
&lt;p&gt;不過這邊我們需要講清楚的是究竟是以何種機制來做選擇的.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PCA 的做法就是要找一個點 $\mu\in R^p$ 使得所有資料點到這個點的距離平方和最小, 也就是要讓下式有最小值
$$
\sum_{i=1}^n \|x_i - \mu\|^2.
$$
簡單的微分求極值我們可以得到其最佳解為
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i,
$$
也就是原始資料點的平均值.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果我們想要的是&amp;quot;距離&amp;quot;和最小而不是&amp;quot;距離平方&amp;quot;和, 也就是要使下式最小,
$$
\sum_{i=1}^n \|x_i - \mu\|,
$$
則最佳解為資料的中位數. 證明可見 
&lt;a href=&#34;https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm#:~:text=111-,The%20Median%20Minimizes%20the%20Sum,Deviations%20%28The%20L1%20Norm%29&amp;amp;text=is%20minimal%20if%20x%20is%20equal%20to%20the%20median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Median minimizes sum of absolute deviations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example-投影到-1-維&#34;&gt;Example: 投影到 $1$ 維&lt;/h3&gt;
&lt;p&gt;投影到 $0$ 維也許過於簡化, 接著我們來投影到 $1$ 維試試, 我們直接用圖形來說明:















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_01.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

藍色小圈就是原始資料點, 共10筆資料. 而 PCA 要做的事就是找到一個一維的 affine subspace, 就是中間那條斜線. 這樣我們就能把原始資料都投影到這個子空間去, 投影後的資料就是紅色點.&lt;/p&gt;
&lt;p&gt;而這 affine subspace 怎麼找到的呢, 事實上這個子空間就是使得所有虛線段(原始資料到投影點連線)距離平方加起來最小的那個.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我們來定義更一般的投影, 假設想要投影到 $k$ 維, $k\le p$, 也就是我們想要在 $R^p$ 空間中找一個 $k$ 維的 affine subspace, 使得說投影後資料與原始資料的距離平方和最小. 數學上來說就是要讓下式有最小值
$$
\sum_{i=1}^n \| x_i - (\mu + U\beta_i)\|^2,
$$
其中未知數有 $\mu\in R^p$, $U\in R^{p\times k}$, 以及 $\beta_i\in R^{k\times 1}$.&lt;/p&gt;
&lt;p&gt;稍微解釋一下以上這些變數:&lt;/p&gt;
&lt;p&gt;事實上這個 affine subspace 可以表示成
$$
\mu + V, \quad V = span\{u_1, \cdots, u_k\},
$$
其中 $\mu$ 是 $R^p$ 空間中的一個點, $\{u_1, u_2, \cdots, u_k\}$ 是這 affine subspace 的基底, 收集在一起構成 $U$, 也就是 $U=[u_1, u_2, \cdots, u_k]$. 而 $\mu + U\beta_i$ 就是 $x_i$ 這個資料點在這 affine subspace 上的投影, 所以 $\beta_i$ 可以想像是原始資料第 $i$ 筆投影到 affine subspace 之後的座標, 或是係數.&lt;/p&gt;
&lt;p&gt;此外, 我們可以特別要求這基底要正交, 也就是 $U^TU = I_k$, 其中 $I_k$ 表示 $k\times k$ 的 identity matrix.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;再稍微整理一下, 我們想要找到 $\mu$, $U$ 以及 $\beta_i$ 使得下式 $E$ 有最小值
$$
E = \sum_{i=1}^n \|x_i - (\mu + U\beta_i)\|^2,
$$
其中有兩個條件, $U^TU=I_k$, 以及 $\sum^n_{i=1} \beta_i=\vec{0}$.&lt;/p&gt;
&lt;h4 id=&#34;remark-1&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;第二個條件 $\sum_i \beta_i=\vec{0}$ 看起來似乎是憑空冒出來的, 不過這可以從兩方面來說.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我們希望投影之後的解他們的平均是 $0$, 所以新資料點的中心就在新座標原點的位置.&lt;/li&gt;
&lt;li&gt;數學上來說, 其實 $\mu$ 並沒有唯一性. 也就是雖然 affine subspace 表示成 $\mu +V$, 但是這個 $\mu$ 是這子空間裡的任何一個點都可以. 為了讓解有唯一性我們加了這個條件.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;這問題可以被完全解出來, 也就是說給定 $x_i$, 我們可以決定出最佳的 $\mu$, $U$ 以及 $\beta_i$ 使得上式的值為最小.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推導過程我們在這先省略, 有興趣的請見 references, 或是 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其最佳解整理如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\mu$ 就剛好是投影到 $0$ 維的解, 也就是資料點的平均:
$$
\mu = \frac{1}{n}\sum^n_{i=1} x_i
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到平均後我們將所有資料做平移使得中心為原點, 定 $y_i = x_i-\mu$, 求出這組新資料的共變異數矩陣 $\Sigma=YY^T$, 其中 $Y=[y_1, \cdots, y_n]$ 是個 $p\times n$ 的矩陣, 而 $\Sigma$ 則是 $p\times p$.&lt;/p&gt;
&lt;p&gt;對 $\Sigma$ 做譜分解(spectral decomposition), 找到其最大的 $k$ 個 eigenvalues 以及相對應的 eigenvectors, 將 eigenvectors 收集起來就得到 $U=[u_1, \cdots, u_k]$, 也就是 affine subspace 的 basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;將原資料投影到此 affine subspace 中得到 $\beta_i$, 也就是
$$
\beta_i = \sum^k_{j=1}&amp;lt;u_j, (x_i-\mu)&amp;gt;
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;一般最基礎的 PCA 做法就是從 $x_i$, 得到 $\mu$. 將資料平移求出 $y_i$, 接著求共變異數矩陣 $\Sigma$. 接著對 $\Sigma$ 做 eigen-decomposition 求出其特徵值及特徵向量. 拿出最大的幾個就可以定出 affine subspace. 然後將 $y_i$ 投影下去就得到 $\beta_i$, 也就是投影之後的座標了.&lt;/p&gt;
&lt;p&gt;以上這做法的 &lt;code&gt;Matlab&lt;/code&gt; 程式如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$X$ 是 $p\times n$ 的 data matrix 含有 $p$ 個 features 以及 $n$ 個 samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;投影到 $k$ 維子空間, 投影後得到 $B$, 為 $k\times n$ 的 data matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均 mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    S = Y*Y&#39;;                       % 求出共變異數矩陣 S
    [U, D] = eig(S, &#39;vector&#39;);      % 求出特徵值 D 及特徵向量 U
    [D, ind] = sort(D, &#39;descend&#39;);  % 將特徵值由大到小排列
    U = U(:, ind);                  % 將特徵向量照樣排列
    B = U(:,1:k)&#39;*Y;                % 投影到前 k 個組成的空間中並求出係數 B
end
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;remark-2&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;這做法會造出一個 $p\times p$ 的共變異數矩陣 $\Sigma$, 並且算出全部的特徵值及特徵向量. 不過其實我們只需要前 $k$ 個. 所以多出來的部分會丟掉, 有點浪費.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們再來看一下這個共變異數矩陣 $\Sigma=YY^T$, 可以很輕易地看出來這是個對稱矩陣, 所以特徵值一定是實數, 也一定存在 orthonormal 的實數特徵向量組. 因此我們以上的要求(將特徵值按大小排列, 特徵向量要彼此正交)都一定做得到. 而分解之後可以得到
$$
\Sigma = U\Lambda U^T,
$$
其中 $U$ 是個 $p\times p$ 矩陣.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;事實上由 $\Sigma=YY^T$ 可以知道 $\Sigma$ 一定是個半正定矩陣, 所以它的 eigenvalues 一定都非負.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們事實上可以將矩陣 $Y$ 做奇異值分解 (Singular Value Decomposition, SVD), 得到
$$
Y = \hat{U}\hat{\Sigma}\hat{V}^T, \quad \hat{U}\in R^{p\times p}, \quad \hat{\Sigma}\in R^{p\times n}, \quad \hat{V}\in R^{n\times n},
$$
並且 $\hat{U}$ 及 $\hat{V}$ 都是 orthogonal matrix, 也就是 $\hat{U}^T\hat{U}=I_p$ 以及 $\hat{V}^T\hat{V}=I_n$.&lt;/p&gt;
&lt;p&gt;既然我們有 $\Sigma=YY^T$, 很容易可以看出來其實
$$
U = \hat{U}, \quad \Lambda = \hat{\Sigma}\hat{\Sigma}^T\in R^{p\times p}.
$$
也就是說將 $Y$ 的 singular values 平方就可以得到 $\Sigma$ 的 eigenvalues. 而 $Y$ 的 left singular vectors 事實上就是 $\Sigma$ 的 eigenvectors.&lt;/p&gt;
&lt;p&gt;而投影後的係數我們可以算一下:
$$
B=U^TY = \hat{U}^T\hat{U}\hat{\Sigma}\hat{V}^T = \hat{\Sigma}\hat{V}^T.
$$
所以如果我們只需要投影後的係數, 將 $Y$ 做 SVD 並且我們需要的是 $Y$ 的 right singular vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;remark-3&#34;&gt;Remark:&lt;/h4&gt;
&lt;p&gt;以上這式子很有趣, 告訴我們投影之後的座標 $B$ 與投影之前的座標 $Y$ 的 SVD 之間的關係.
另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt; 在某些情況下會有一模一樣的關係. 這樣就把兩種方法 PCA 跟 MDS 連在一起了. 雖然出發點不一樣, 竟然(在某些情況下)結果是一樣的!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;利用以上關係我們可以將程式改寫如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
function B = principal_component_analysis2(X, k)

%   Input: X, p*n data matrix, p 個 features 以及 n 個 samples
%          k, 要降到的維度, k 為正整數, k&amp;lt;=p
%   Output: B: k*n data matrix, k 個 features 以及 n 個 samples

    [p, n] = size(X);               % p 個 features 以及 n 個 samples
    mu = sum(X, 2)/n;               % 計算 sample 的平均, mu
    Y = X - mu*ones(1,n);           % 資料平移得到 Y
    [~, S, V] = svds(Y, k);         % 將 Y 做奇異值分解並取前 k 個 eigenvectors
    B = S*V&#39;;                       % 求出投影後的係數
end
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;而事實上, matlab 已經內建 PCA 程式了, 所以其實完全不用自己寫. 只是要注意一下 matlab 裡 PCA 的輸入 sample points 是 $n\times p$, 由於我們以上都是將 $X$ 設成 $p\times n$ 的矩陣, 所以要轉置一下, 程式只有兩行, 如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
[U, B] = pca(X&#39;);
B = B(:,1:k)&#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;working-example&#34;&gt;Working example&lt;/h1&gt;
&lt;p&gt;這裡我們舉一個例子, 我們先構造 sample points, 是一個類似螺旋狀結構, 程式如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% Matlab program
n = 1000;                                   % 取 n 個點
t = 3*pi*(1:n)/n;                           % 利用參數化來構造, 取 t\in[0, 3*pi]
X = [t.*cos(t); 5*rand(1,n); t.*sin(t)];    % 先利用 random 構造三圍中的一個面
M = makehgtform(&#39;axisrotate&#39;,[1 1 1], 30);  % 構造旋轉矩陣
X = M(1:3, 1:3)*X + 10*rand(3,1)*ones(1,n); % 將 sample 旋轉並且隨機平移
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scatter3(X(1,:), X(2,:), X(3,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_02.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;接著我們用 PCA 投影到二維, 圖形長這樣:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[~, B] = pca(X&#39;);
B=B&#39;;
scatter(B(1,:), B(2,:), [], (1:n)/n, &#39;fill&#39;)
&lt;/code&gt;&lt;/pre&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; &gt;


  &lt;img src=&#34;https://teshenglin.github.io/post/figs/2020_pca_03.png&#34; alt=&#34;&#34; width=&#34;600px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;可以發現 PCA 找到一個正確的軸來做投影, 使得原本的螺旋線可以看得很清楚.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;extension&#34;&gt;Extension&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;另一種降維方法: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_multi_dimensional_scaling&#34;&gt;Multidimensional scaling (MDS)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PCA 推導過程及證明: 
&lt;a href=&#34;https://teshenglin.github.io/post/2020_principal_component_analysis_2&#34;&gt;主成分分析 - 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/manifoldlearning2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCTS mini-course on manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is principal component analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
