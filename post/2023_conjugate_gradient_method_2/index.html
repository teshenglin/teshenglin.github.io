<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Te-Sheng Lin (林得勝)">

  
  
  
    
  
  <meta name="description" content="共軛梯度法 (CG method, conjugate gradient method) 目錄: CG method - Direct mehtod 直接法 CG method - iterate mehtod 迭代法 For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix. Assumptions: $A\in M_{n\times n}$ is a symmetric positive definite matrix. Definition: A-orthogonal (A-conjugate) 假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \neq">

  
  <link rel="alternate" hreflang="en-us" href="https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-77TT93X4N1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-77TT93X4N1');
</script>
  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Te-Sheng Lin">
  <meta property="og:url" content="https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/">
  <meta property="og:title" content="Conjugate gradient method - iterative method | Te-Sheng Lin">
  <meta property="og:description" content="共軛梯度法 (CG method, conjugate gradient method) 目錄: CG method - Direct mehtod 直接法 CG method - iterate mehtod 迭代法 For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix. Assumptions: $A\in M_{n\times n}$ is a symmetric positive definite matrix. Definition: A-orthogonal (A-conjugate) 假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \neq"><meta property="og:image" content="https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2023-04-27T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2023-04-29T12:37:53&#43;08:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/"
  },
  "headline": "Conjugate gradient method - iterative method",
  
  "datePublished": "2023-04-27T00:00:00Z",
  "dateModified": "2023-04-29T12:37:53+08:00",
  
  "author": {
    "@type": "Person",
    "name": "Te-Sheng Lin (林得勝)"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Te-Sheng Lin",
    "logo": {
      "@type": "ImageObject",
      "url": "https://teshenglin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "共軛梯度法 (CG method, conjugate gradient method) 目錄: CG method - Direct mehtod 直接法 CG method - iterate mehtod 迭代法 For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix. Assumptions: $A\\in M_{n\\times n}$ is a symmetric positive definite matrix. Definition: A-orthogonal (A-conjugate) 假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \\neq"
}
</script>

  

  


  


  





  <title>Conjugate gradient method - iterative method | Te-Sheng Lin</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Te-Sheng Lin</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Te-Sheng Lin</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Resume</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#experience"><span>Appointments </span></a>
            
              <a class="dropdown-item" href="/#publications"><span>Publications</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Posts</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#posts"><span>All posts</span></a>
            
              <a class="dropdown-item" href="/categories/calculus/"><span>Calculus 微積分</span></a>
            
              <a class="dropdown-item" href="/categories/computational-mathematics/"><span>Computational Mathematics 計算數學</span></a>
            
              <a class="dropdown-item" href="/categories/linear-algebra/"><span>Linear Algebra 線性代數</span></a>
            
              <a class="dropdown-item" href="/categories/notes/"><span>程式語言學習筆記</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Teaching</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/courses/taught"><span>Courses taught</span></a>
            
              <a class="dropdown-item" href="/courses/students"><span>Students supervised</span></a>
            
              <a class="dropdown-item" href="/courses/gsw"><span>Graduate student workshops</span></a>
            
              <a class="dropdown-item" href="/courses/summer_research"><span>Summer research internship</span></a>
            
          </div>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Conjugate gradient method - iterative method</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Te-Sheng Lin (林得勝)</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Apr 29, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/notes/">notes</a>, <a href="/categories/linear-algebra/">Linear algebra</a>, <a href="/categories/computational-mathematics/">Computational Mathematics</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>共軛梯度法 (CG method,  conjugate gradient method) 目錄:</p>
<ol>
<li>
<a href="/post/2023_conjugate_gradient_method_1">CG method - Direct mehtod</a>
<blockquote>
<p>直接法</p>
</blockquote>
</li>
<li>
<a href="/post/2023_conjugate_gradient_method_2">CG method - iterate mehtod</a>
<blockquote>
<p>迭代法</p>
</blockquote>
</li>
</ol>
<hr>
<blockquote>
<p>For solving $Ax=b$, where $A$ is a square symmetric positive definite matrix.</p>
</blockquote>
<h2 id="assumptions">Assumptions:</h2>
<p>$A\in M_{n\times n}$ is a symmetric positive definite matrix.</p>
<h2 id="definition">Definition:</h2>
<ul>
<li>A-orthogonal (A-conjugate)
假設有兩個向量 $u_1$ 跟 $u_2$ 皆非 $0$ 且 $u_1 \neq u_2$，若這兩個向量滿足
$$
\langle{u_1},A{u_2}\rangle = {u_1}^TA{u_2} = 0,
$$
則稱之為 A-orthogonal (或 A-conjugate).</li>
</ul>
<p><strong>Note :</strong> We can define
$$
\langle{u_1}, {u_2}\rangle_A = \langle{u_1},A{u_2}\rangle= \langle A{u_1},{u_2}\rangle,
$$
then $\langle \cdot\rangle_A$ is an inner product.</p>
<hr>
<h2 id="cg-as-an-iterative-method-first-attempt">CG as an iterative method: First attempt</h2>
<h3 id="theorem-1">Theorem 1:</h3>
<p>假設對一個矩陣 A，我們可以找到一組 A-orthogonal set ${u_0, \ldots, u_{n-1}}$. 則給定一個初始值 $x_0$, 我們可定義一個迭代法: For $0\le i\le n-1$,
$$
\tag{1} x_{i+1}=x_i+t_{i}u_{i}, \quad
t_{i}=\frac{\langle b-Ax_i,u_{i}\rangle}{\langle u_i,Au_{i}\rangle}.
$$
並且我們可以證明以下兩件事</p>
<ol>
<li>定義第 $k$ 步的 residual, $r_k = b-Ax_k$, 我們有 $\langle r_k, u_{j}\rangle=0$, for $0\le j&lt; k\le n$.</li>
<li>$Ax_n=b$, 也就是第 $n$ 次迭代保證得到解.</li>
</ol>
<h4 id="pf-of-1">pf of 1:</h4>
<p>首先我們可以推 residual 的迭代式
$$
\tag{2} \begin{align}
r_{k+1} &amp;= b - Ax_{k+1} \\
&amp;= b - A(x_k + t_ku_k) \\<br>
&amp;= (b-Ax_k)  - t_kAu_k \\<br>
&amp;= r_k  - t_kAu_k.
\end{align}
$$</p>
<p>接著, for $0\le j&lt;k-1$, 由於 $u_{k-1}$ 與 $u_j$ 是 A-orthogonal, 因此
$$
\begin{align}
\langle r_{k}, u_{j}\rangle
&amp;= \langle r_{k-1}, u_{j}\rangle-t_{k-1}\langle Au_{k-1}, u_{j}\rangle \\<br>
&amp;= \langle r_{k-1}, u_{j}\rangle.
\end{align}
$$
同樣的理由可以一直使用得到
$$
\langle r_{k}, u_{j}\rangle
= \langle r_{k-1}, u_{j}\rangle
=\cdots
= \langle r_{j+1}, u_{j}\rangle.
$$</p>
<p>此外, 若 $j=k-1$, 則我們直接有 $\langle r_{k}, u_{j}\rangle  = \langle r_{j+1}, u_j\rangle$.</p>
<p>接著再用 (2) 一次, 並且用 (1) 裡 $t_j$ 的定義我們得到</p>
<p>$$
\begin{align}
\langle r_{j+1}, u_j\rangle
&amp;= \langle r_j, u_j\rangle-t_j\langle Au_j, u_j\rangle \\
&amp;= \langle r_j, u_j\rangle-\frac{\langle r_j,u_j\rangle}{\langle u_j,Au_j\rangle}\langle Au_j, u_j\rangle\\<br>
&amp;=0.
\end{align}
$$</p>
<p>因此, for $0\le j&lt;k$, 我們有
$$
\langle r_{k}, u_{j}\rangle =0.
$$</p>
<h4 id="pf-of-2">pf of 2:</h4>
<p>根據 1 我們有 $\langle r_n, u_{j}\rangle=0$ for $0\le j&lt; n-1$, 也就是
$$
\langle b - Ax_n, u_{j}\rangle=0, \quad 0\le j&lt; n-1.
$$
因為 $\{u_i\}^{n-1}_{i=0}$ span $\mathbb{R}^n$, 所以 $b - Ax_n$ 必為一個零向量, 也就是說 $Ax_n=b$.</p>
<hr>
<h2 id="gram-schimidt-process-to-find-a-orthogonal-set">Gram-Schimidt process to find A-orthogonal set</h2>
<p>不過這方法建立在一個重要假設之下, 就是一開始我們可以找到一組 A-orthogonal set $\{u_0, \ldots, u_{n-1}\}$. 不過在實際解問題是這顯然是不容易做到的, 因此我們會需要一步一步地將 $u_i$ 求出來.</p>
<p>這裡我們可以利用兩件事</p>
<ol>
<li>The residual vector $r_k = b - Ax_k$.
<ul>
<li>每次迭代都會產生出一組新的 residual vector, 並且我們知道 residual 是下降最快的方向, 因此我們可以將下個搜尋方向設為 $r_k$, 只不過這並不滿足 A-orthogonal 的條件.</li>
</ul>
</li>
<li>Gram-Schimidt process
<ul>
<li>線性代數裡告訴我們, 可以用 Gram-Schimidt process 將一組向量正交化. 因此我們就利用這方法來造出一組 A-orthogonal 的 basis.</li>
<li>若已經有 $\{u_0, \cdots, u_k\}$, 並且我們得到 $r_{k+1}$, 則 Gram-Schimidt process 告訴我們可以定義 $u_{k+1}$ 為
$$
u_{k+1} = r_{k+1} - \frac{\langle r_{k+1}, Au_0\rangle}{\langle u_0, Au_0\rangle}u_0 - \cdots - \frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle}u_k,
$$
並且 $u_{k+1}$ 會跟 $u_0, \cdots, u_k$ 是 A-orthogonal.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="cg-as-iterative-method-second-attempt">CG as iterative method: Second attempt</h2>
<p>給一個初始猜測 $x_0$, 我們可以算 $r_0 = b-Ax_0$, 並且我們定義第一個方向為 $u_0 = r_0$.</p>
<p>接著我們可以往下做, for $k\ge 0$,
$$
\begin{align}
t_{k} &amp;=\frac{\langle b-Ax_k,u_{k}\rangle}{\langle u_k,Au_{k}\rangle}, \\<br>
x_{k+1} &amp;= x_k + t_k u_k, \\<br>
r_{k+1} &amp;= r_k - t_kAu_k, \\<br>
u_{k+1} &amp;= r_{k+1} - \frac{\langle r_{k+1}, Au_0\rangle}{\langle u_0, Au_0\rangle}u_0 - \cdots - \frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle}u_k.
\end{align}
$$
這樣保證在第 $n$ 步得到解.</p>
<p>不過一般而言這是個迭代法, 我們在過程中會看 residual 的大小, $\|r_{k+1}\|^2_2$, 如果 residual 夠小就會停止整個迭代, 不需要真的做到第 $n$ 步.</p>
<p><strong>Remark :</strong>
這樣子做有個很明顯的缺點, 就是為了求出 A-orthogonal set 我們需要把所有方向 $\{u_i\}$ 都一直記著. 這樣當問題維度非常大時會需要耗費大量記憶體在存這些方向.</p>
<hr>
<p>不過我們可以再觀察一下, 若使用 Gram-Schimidt process, 則這些 $\{u\}$ 都是從 $\{r\}$ 得來的, 因此這兩個所組出來的空間應該是一樣的, 也就是
$$
\text{span}\{u_0,\cdots, u_k\} = \text{span}\{r_0,\cdots, r_k\}.
$$
我們若定義 $V_k = \text{span}\{u_0,\cdots, u_k\}$, 則由 Theorem 1 我們知道,
$$
\langle r_{k+1}, u_{j}\rangle=0, \quad 0\le j\le k,
$$</p>
<p>也就是 $r_{k+1}$ 這個向量會垂直於 $V_k$ 這個空間, 因此我們也有</p>
<p>$$
\tag{3} \langle r_{k+1}, r_{j}\rangle=0, \quad 0\le j\le k.
$$</p>
<hr>
<p>事實上, 實作 second attempt 後會發現, 要算 $u_{k+1}$ 其實只需要他的前一項, $u_{k}$, 即可, 也就是
$$
\tag{4} u_{k+1} = r_{k+1}  + \beta_k u_k, \quad \beta_k =  -\frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle},
$$
因為其他項都自動為零了! 接著我們就來證明這件事.</p>
<hr>
<h3 id="lemma-"><em><strong>Lemma :</strong></em></h3>
<p>$$
\langle r_{k+1}, Au_j\rangle = 0, \quad 0\le j\le k-1.
$$</p>
<h4 id="pf">pf:</h4>
<p>根據 (2), 我們可以得到 $Au_j = \frac{r_j - r_{j+1}}{t_j}$. 因此
$$
\tag{5} \langle r_{k+1}, Au_j\rangle = \frac{1}{t_j}\langle r_{k+1}, r_j - r_{j+1}\rangle= \frac{\langle r_{k+1}, r_j \rangle - \langle r_{k+1}, r_{j+1} \rangle}{t_j}.
$$</p>
<p>接著我們利用 (3), 代入 (5) 我們就得到 $\langle r_{k+1}, Au_j\rangle=0$ for $0\le j\le k-1$.</p>
<p><strong>Remark :</strong></p>
<p>將 $j=k$ 代入 (5) 得到
$$
\tag{6}\langle r_{k+1}, Au_k\rangle =  \frac{\langle r_{k+1}, r_k \rangle - \langle r_{k+1}, r_{k+1} \rangle}{t_k} = \frac{ - \langle r_{k+1}, r_{k+1} \rangle}{t_k}.
$$</p>
<p>利用 (1) 與 (4) $t_k$ 及 $u_k$ 的定義我們可以推得
$$
\tag{7}
\begin{align}
t_k &amp;= \frac{\langle r_{k}, u_{k} \rangle}{\langle u_{k}, Au_{k} \rangle} = \frac{\langle r_{k}, r_{k} + \beta_{k-1}u_{k-1} \rangle}{\langle u_{k}, Au_{k} \rangle} \\<br>
&amp;= \frac{\langle r_{k}, r_{k}  \rangle}{\langle u_{k}, Au_{k} \rangle} + \frac{\langle r_{k}, \beta_{k-1}u_{k-1} \rangle}{\langle u_{k}, Au_{k} \rangle} \\<br>
&amp;= \frac{\langle r_{k}, r_{k}  \rangle}{\langle u_{k}, Au_{k} \rangle}.
\end{align}
$$
最後一個等號我們用到了 Theorem 1 的結果. 因此 $t_k$ 可改為以 (7) 來做.</p>
<p>將 (7) 改寫一下我們得到
$$
\tag{8}\langle u_{k}, Au_{k} \rangle = \frac{\langle r_{k}, r_{k}  \rangle}{t_k}.
$$</p>
<p>最後, 我們可以將 $\beta_k$ 重新計算一下, 利用 (6) 跟 (8) 得到
$$
\tag{9} \beta_k = -\frac{\langle r_{k+1}, Au_k\rangle}{\langle u_k, Au_k\rangle} = \frac{\langle r_{k+1}, r_{k+1}\rangle}{\langle r_{k}, r_{k}\rangle}.
$$</p>
<hr>
<h2 id="cg-as-iterative-method-the-algorithm">CG as iterative method: The algorithm</h2>
<p>給一個初始猜測 $x_0$, 我們可以算 $r_b = b-Ax_0$, 並且我們定義第一個方向為 $u_0 = r_0$.</p>
<p>接著我們可以往下做, for $k\ge 0$,
$$
\begin{align}
t_{k} &amp;=\frac{\langle r_k,r_{k}\rangle}{\langle u_k,Au_{k}\rangle},\\<br>
x_{k+1} &amp;= x_k + t_k u_k, \\<br>
r_{k+1} &amp;= r_k  - t_kAu_k, \\<br>
\beta_k &amp;= \frac{\langle r_{k+1}, r_{k+1}\rangle}{\langle r_{k}, r_{k}\rangle},\\<br>
u_{k+1} &amp;= r_{k+1} + \beta_k u_k.
\end{align}
$$</p>
<p>這樣保證在第 $n$ 步得到解, 而且每次迭代都只需要 <em><strong>一個</strong></em> 矩陣向量乘法.</p>
<p><strong>Remark:</strong></p>
<ul>
<li>不過一般而言這是個迭代法, 我們在過程中會看 residual 的大小, $\|r_{k+1}\|^2_2=\langle r_{k+1}, r_{k+1}\rangle$, 如果 residual 夠小就會停止整個迭代, 不需要真的做到第 $n$ 步. 而且這個量是在過程中會被計算出來的, 所以不需花費額外力氣.</li>
<li>Residual $r_{k+1}=r_k-t_kAu_k$ 這個式子可能會在迭代的過程由於誤差進來而越來越不準, 所以每隔一段時間要以原本公式重新更新一次, $r_{k+1} = b - Ax_{k+1}$.</li>
</ul>
<hr>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/conjugate-gradient-method/">conjugate gradient method</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/&amp;text=Conjugate%20gradient%20method%20-%20iterative%20method" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/&amp;t=Conjugate%20gradient%20method%20-%20iterative%20method" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Conjugate%20gradient%20method%20-%20iterative%20method&amp;body=https://teshenglin.github.io/post/2023_conjugate_gradient_method_2/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hua0d4d4b24f4da3a102e4452ac738b95f_109236_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://teshenglin.github.io/">Te-Sheng Lin (林得勝)</a></h5>
      <h6 class="card-subtitle">Associate Professor</h6>
      <p class="card-text">The focus of my research is concerned with the development of analytical and computational tools, and further to communicate with scientists from other disciplines to solve engineering problems in practice.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:teshenglin@nycu.edu.tw" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Te-Sheng_Lin" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/teshenglin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/cv/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/2023_conjugate_gradient_method_1/">Conjugate gradient method - direct method</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.4/mermaid.min.js" integrity="sha256-JEqEejGt4tR35L0a1zodzsV0/PJ6GIf7J4yDtywdrH8=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/julia.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/fortran.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/matlab.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
